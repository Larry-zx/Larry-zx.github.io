<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>OmniArt: Multi-task Deep Learning for Artistic Data Analysis | 扁同学不发言的个人博客</title><meta name="keywords" content="cs.CV,深度学习,属性识别"><meta name="author" content="扁同学不发言"><meta name="copyright" content="扁同学不发言"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="OmniArt: Multi-task Deep Learning for Artistic Data Analysis ABSTRACT 大量的艺术数据分散在网上，来自博物馆和艺术应用程序 收集、处理和研究所有相关属性是一个昂贵的过程 为了加快和提高艺术领域分类分析的质量，在本文中，我们提出了一种有效且准确的多任务学习方法，该方法具有应用于艺术领域的共享表示。 我们继续展示我们方法的不同多任务配">
<meta property="og:type" content="article">
<meta property="og:title" content="OmniArt: Multi-task Deep Learning for Artistic Data Analysis">
<meta property="og:url" content="http://www.larryai.com/2022/07/19/OmniArt1/index.html">
<meta property="og:site_name" content="扁同学不发言的个人博客">
<meta property="og:description" content="OmniArt: Multi-task Deep Learning for Artistic Data Analysis ABSTRACT 大量的艺术数据分散在网上，来自博物馆和艺术应用程序 收集、处理和研究所有相关属性是一个昂贵的过程 为了加快和提高艺术领域分类分析的质量，在本文中，我们提出了一种有效且准确的多任务学习方法，该方法具有应用于艺术领域的共享表示。 我们继续展示我们方法的不同多任务配">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.larryai.com/2022/07/19/OmniArt1/1.png">
<meta property="article:published_time" content="2022-07-19T13:52:50.000Z">
<meta property="article:modified_time" content="2022-07-19T13:55:40.048Z">
<meta property="article:author" content="扁同学不发言">
<meta property="article:tag" content="cs.CV">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="属性识别">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.larryai.com/2022/07/19/OmniArt1/1.png"><link rel="shortcut icon" href="/img/bian_logo.png"><link rel="canonical" href="http://www.larryai.com/2022/07/19/OmniArt1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'OmniArt: Multi-task Deep Learning for Artistic Data Analysis',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-19 21:55:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="扁同学不发言的个人博客" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/%E6%96%B0%E4%B8%80.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 专栏</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/picture/"><i class="fa-fw fas fa-image"></i><span> 图库</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2022/07/19/OmniArt1/1.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">扁同学不发言的个人博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 专栏</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/picture/"><i class="fa-fw fas fa-image"></i><span> 图库</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">OmniArt: Multi-task Deep Learning for Artistic Data Analysis</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-19T13:52:50.000Z" title="发表于 2022-07-19 21:52:50">2022-07-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-19T13:55:40.048Z" title="更新于 2022-07-19 21:55:40">2022-07-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="OmniArt: Multi-task Deep Learning for Artistic Data Analysis"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1
id="omniart-multi-task-deep-learning-for-artistic-data-analysis">OmniArt:
Multi-task Deep Learning for Artistic Data Analysis</h1>
<h2 id="abstract">ABSTRACT</h2>
<p>大量的艺术数据分散在网上，来自博物馆和艺术应用程序</p>
<p>收集、处理和研究所有相关属性是一个昂贵的过程</p>
<p>为了加快和提高艺术领域分类分析的质量，在本文中，我们提出了一种有效且准确的多任务学习方法，该方法具有应用于艺术领域的共享表示。</p>
<p>我们继续展示我们方法的<strong>不同多任务</strong>配置如何在艺术数据上表现，并<strong>优于手工特征方法以及卷积神经网络</strong>。</p>
<p>除了方法和分析之外，我们还对包含近 50
万个样本和结构化元数据的新聚合数据集提出了挑战，以鼓励进一步的研究和社会参与。</p>
<h2 id="introduction">1 INTRODUCTION</h2>
<p>诸如 <em>WikiArt 1、Europeana 2、ArtUk 3、WGA 4 和 Google Art Project
5</em>
等<strong>艺术应用程序</strong>将各种艺术收藏品与基本元数据聚合在一起，并通过网络将其公开。</p>
<p>另一方面，<strong>博物馆</strong>基于以博物馆为中心的观点，公开了大部分元数据和结构。</p>
<p>通常，这些集合包含比在线艺术应用程序中发现的更丰富、专业设计的元数据。</p>
<p>以博物馆为中心的数据将准确的元数据与收藏品的高质量照片复制相结合，是分析的理想选择，因为它们允许从多个角度进行更深入的艺术探索。</p>
<p>在此类以博物馆为中心的藏品中，可用信息与注释中包含的数据之间存在差距。</p>
<p>通常，可用于特定艺术作品的语义元数据在不同的用例场景中以不同的方式传递。</p>
<p><strong>这给这些集合的搜索和索引带来了问题</strong>。</p>
<p>施莱伯等人，在<strong>以标准化方式将元数据归因于艺术品</strong>方面，迈出了重要的一步
。</p>
<p>他们从各种收藏中收集了超过 200,000
个艺术样本，并在这些样本之上创建了一个用于<strong>描述艺术作品的词汇</strong>。</p>
<p>使用这个词汇表，<strong>他们将具有 RDF
关系的现有元数据映射到链接开放数据云中的其他实体</strong>。</p>
<p>这样做有效地提供了标准化的注释，并极大地扩展了语义上下文可用的元数据</p>
<figure>
<img src="OmniArt1/1.png" alt="1" />
<figcaption aria-hidden="true">1</figcaption>
</figure>
<p>然而，即使没有关联数据的扩展，文化遗产通常也是一个知识丰富的领域。</p>
<p>例如，在艺术绘画中，大多数艺术品都有着名的<strong>艺术家、风格、创作年份、材料、地理来源</strong>，甚至关于其内容的非常详细的文字描述。</p>
<p>通过材料科学的特定方法，<strong>可以将颜色的化学成分与绘画中的画布线图案一起提取</strong>。</p>
<p>这些数据集的信息在更精细的范围内可用。</p>
<p>使用高分辨率摄影和 X 射线
，我们能够看到虚幻的细节并产生比以往更多的洞察力。</p>
<p><strong>每个提到的信息块都对科学家提出了不同的挑战</strong>。</p>
<p>随着这种以博物馆为中心、内容丰富的数据的数量不断增长，对探索和分析艺术收藏品的有效工具的需求变得越来越迫切。</p>
<p>对于这项工作，我们专注于该数据池的一个子域，即<strong>艺术品的原始照片复制品与博物馆提供的文本元数据相结合</strong>。</p>
<p>我们在元数据中的重点领域包含我们假设在语义上相关的属性。</p>
<p><strong>具有多种类型的属性，为每种类型创建了单独的任务的可能性，从而使这项研究具有多任务性质。</strong></p>
<p>处理多个任务最省时的方法是同时进行，这就是为什么在论文中我们提出了一种多任务深度学习方法</p>
<p>在图 1 中描绘的任务之间具有<strong>共享数据表示</strong>。</p>
<p><strong>创建共享表示允许我们利用
不同任务之间的语义纠缠</strong>。</p>
<p>考虑到在缩小上述信息鸿沟方面鼓励科学进步，我们对论文中解决的任务提出了挑战。</p>
<p><strong>挑战一再被证明是激励社区为某项事业做出贡献的良好催化剂</strong></p>
<p>自 2009
年以来，多媒体大挑战就是多媒体挑战成功的一个光辉例子，它一直将商业工业需求引起科学研究人员的注意。</p>
<p>在计算机视觉领域，ImageNet 大规模视觉识别竞赛 (ILSVRC) 让位于
一些最大的突破。</p>
<p>Mensink 等人于 2014
年推出了一项这样的艺术挑战，即结合丰富的艺术数据信息，同时提供以博物馆为中心的视角，作为国立博物馆挑战。</p>
<p>国立博物馆挑战赛由四个独立的任务组成</p>
<blockquote>
<p>即：<strong>艺术家</strong>归属、预测<strong>艺术类型</strong>、预测使用的<strong>材料</strong>和预测收藏中特定艺术品的<strong>创作年份</strong>。</p>
</blockquote>
<p>2014 年，该数据集包含近 120,000
条国家博物馆在线收藏的艺术品的各种摄影复制品条目。</p>
<p>挑战中的单个艺术品由许多属性描述，例如艺术家、创作时期、使用的材料、艺术品类型等。</p>
<p>该数据集中的艺术作品可以追溯到远古时代到 19 世纪后期，并归属于 6000
多名艺术家。</p>
<p>尽管艺术数据提供了多种分析方法，但<strong>大多数当前的艺术数据分析方法分别解决了每项任务</strong>。</p>
<p>在<strong>单个端到端系统中解决这个多任务问题会更有效</strong>，因为这样可以显着减少多个模型所需的训练时间。</p>
<p>在某些情况下，如果每<strong>个任务中的类别之间存在相关性，多任务学习环境甚至可以提高分类性能</strong></p>
<p>Kokkinos
等人所做的工作成功地展示了适用于一次解决多个任务的深度模型的好处。</p>
<p><strong>他们的架构试图通过模型一次传播输入数据来执行多项计算机视觉任务</strong>，这在一定程度上启发了我们的工作。</p>
<p>使用我们提出的方法，称为 OmniArt，我们报告了 2014
年国立博物馆挑战赛的最先进结果，并提出了一个新的挑战，扩展了一个更好的结构化数据集。</p>
<p>接受后，我们将通过数据集、训练好的模型和评估引擎公开挑战，以刺激进一步的发展。</p>
<p>在这项工作中，我们详细阐述了以下贡献：</p>
<ul>
<li>我们提出了<strong>一种高效且准确的多任务端到端方法</strong>，用于学习关于所有任务的输入数据的共享表示</li>
<li>我们提供了一个以博物馆为中心的数据集，其中包含超过 430,000
个样本和更新的元数据，名为 The
OmniArt挑战刺激参与，鼓励新的研究和最大化社会影响。</li>
<li>我们报告了 2014 年国立博物馆挑战赛和 OmniArt
挑战赛的最新结果，训练时间显着缩短</li>
</ul>
<p>本文的其余部分结构如下：</p>
<ul>
<li>第 2 部分包含从多任务学习角度和一般艺术数据分析的相关工作。</li>
<li>第 3 节介绍了所提出的方法及其背后的逻辑。</li>
<li>第 4 节是关于实验设置、用于训练和测试的数据集以及实验结果。</li>
<li>在最后一节中，我们给出了结论性意见以及我们进行的分析的定性发现。</li>
</ul>
<h2 id="related-work">2 RELATED WORK</h2>
<p>该领域的相关工作可以分为<strong>艺术数据分析</strong>和<strong>多任务学习</strong>两个部分。</p>
<h3 id="艺术数据分析">2.1 艺术数据分析</h3>
<p>早在 1979 年，J. Rush 就得出结论</p>
<blockquote>
<p>对特定艺术家的个别艺术实例的体验 可以导致 识别同一艺术家
以前从未见过的作品 的能力。</p>
</blockquote>
<p>虽然来自艺术家的样本的纯视觉体验有效地教会了受试者识别这些从未见过的艺术品</p>
<p>但当其他上下文信息与原始图像结合呈现时，性能得到了显着提升。</p>
<p>通过添加上下文，消除了可能的混淆来源并提高了识别性能。</p>
<p>琼森等人使用 Gabor、Complex 和 D4 小波结合支持向量机 (SVM)
和阈值对梵高作品中的笔触进行了详细分析。</p>
<p>这种分析是在非常小的范围内完成的，只有 101
张图像，输入是全分辨率复制品。</p>
<p>他们得出结论，<strong>笔触分析有助于艺术家归因</strong>，但它也取决于许多外部因素，如画布退化和颜料损失。</p>
<p>对于大规模的准确分析，艺术品需要按比例缩小的表示形式，并且信息损失最小。</p>
<p>我们建议大规模进行此类分类。</p>
<p>作为较大的艺术数据集之一，Rijks'14 数据集由 Mensink 等人引入，2014
年参加国立博物馆挑战赛。</p>
<p>此时，数据集的基线分数是在Fisher向量中表示的对手和强度SIFT特征的帮助下计算的。</p>
<p>对于分类，他们使用了 liblinear SVM 库 。</p>
<p>在同一数据集上，Van Noord 等使用他们自己的子集和名为 PigeoNET
的卷积神经网络进行艺术家归属。</p>
<p>他们的实现具有五个卷积层和三个堆叠的全连接层，例如 Alexnet。</p>
<p>具有艺术家属性的表演报告在具有三个变化来源的子集上：1)
异质性与同质性，2) 集合中的艺术家数量和 3) 每个艺术家的艺术品数量。</p>
<p>从这项研究得出的结论表明</p>
<blockquote>
<p><strong>模型的性能与每类的样本数量成正比——每类更多的样本等于更好的归因能力。</strong></p>
</blockquote>
<p>此外，当模型在单一类型的艺术品（例如仅印刷品）上进行训练时，性能会提高，因为模型不必处理来自同一艺术家的艺术品之间的巨大差异</p>
<p>虽然，类似的艺术品类型提高学习更好特征的能力，有时会由于样本相似性而使分类混乱。</p>
<p>范诺德等人对艺术家属性进行了广泛的分析，但没有使用我们利用并证明对确定艺术品属性有益的其他元数据（时期、材料、类型……）</p>
<p>另一个大量的艺术数据是 WikiArt 数据集。</p>
<p>多种艺术数据分析方法已在 WikiArt
上进行了测试，因为它具有艺术家、时期和艺术类型的质量注释。</p>
<p>但是，由于缺少有关艺术品的材料信息，我们在 OmniArt 挑战中仅包含
WikiArt 数据的子集中用于人物检测的毕加索数据集包含 218
幅毕加索画作，其中大部分已经作为新数据集当前版本的子集包含在内。</p>
<p>谈到艺术，艺术家和时期等有形信息只是拼图的一部分。</p>
<p>风格在识别艺术品的起源方面也起着重要作用。</p>
<p>2016 年，Gatys 等人出了一种使用能量最小化观点的风格转移方法。</p>
<p>他们使用预训练的卷积神经网络作为样式原始图像和样式应转移到的图像的特征提取器。</p>
<p>捕获这些细节并以有意义的方式传输它表明可以使用卷积神经网络从艺术数据中提取质量信息中介绍了另一种最近的艺术数据生成方法，其中朱等人无需使用生成对抗训练配对即可有效地转移图像的风格、增强和变形。</p>
<h3 id="多任务学习">2.2 多任务学习</h3>
<p>多任务学习是归纳迁移的一种范式，其目标是通过利用相关任务中训练数据的特定领域信息来提高泛化性能。</p>
<p>随着数据集变得越来越大，最大限度地利用数据的每次传递的想法变得相当有吸引力，并且一次分析数据的多个方面的方法变得越来越流行。</p>
<p>鉴于多任务学习的优点，本文从分类设置中的多任务角度解决了艺术数据分析。</p>
<p>Kokkinos
引入了一种卷积神经网络架构，该架构联合处理不同级别的视觉特征，称为
UberNet。</p>
<p>在他的工作中，他在通过模型的图像的单次前向传递中生成对象边界、显着性图、语义分割、表面法线和检测。</p>
<p>Ubernet
是我们工作背后动机的一部分，因为它使用类似的端到端范例来解决多任务问题。</p>
<p>虽然它在任务之间使用了明确的分离，但 Ubernet
不允许在任务之间共享重要的信息，除了影响特定输出下所有层的联合损失。</p>
<p>然而，在<strong>不同的任务表示之间共享信息被证明有利于模型在训练中</strong>，如
Misra 等人得出结论。</p>
<p>他们解决的一个拼接层提供了一个共享的单元结构，它将来自多个网络的激活组合成一个端到端的可训练模型。</p>
<p>在自然语言处理中，深度学习的多任务方法也被证明是有益的。</p>
<p>刘等人使用多个共享层对文本进行多域分类申述。</p>
<p>跨越不同领域，多任务学习可以通过显式或隐式信息共享来完成</p>
<p>最近的研究表明</p>
<blockquote>
<p>任务之间的信息共享对于动作检测
、零镜头动作识别、人体姿态估计和用于面部表情改进的自适应视觉反馈生成
是有益的。</p>
</blockquote>
<p>当前的方法<strong>使用不同的层深度来处理具有不同复杂性的任务或在其模型中使用多个输入</strong></p>
<p>因此不同的任务具有适合在最终块中训练分类器/回归器的特征。</p>
<p>在我们的方法中，我们对每个任务使用相同的功能，并<strong>根据任务的性质应用任务特定的缩放和权重</strong>。</p>
<p>我们方法的另一个好处是，<strong>即使不同任务的目标之间存在轻微的相关性，它也可以提高整体模型的性能</strong>。</p>
<p>在我们的方法中，我们<strong>旨在学习任务之间的语义联系，并利用这种洞察力以高效和准确的方式同时预测艺术品的多个属性。</strong></p>
<h2 id="method">3 METHOD</h2>
<blockquote>
<p>为该数据集中的每个任务训练单独的模型是一个计算效率低且耗时的过程。</p>
</blockquote>
<p>与多任务数据集的情况一样，在<strong>前向传递</strong>中通过模型传播的图像对于每个任务都是相同的。</p>
<p>由于标签空间、维度和损失类型的不同，只能在最终分类/回归块的<strong>反向传播中观察到差异</strong>。</p>
<p>此外，这些类型的任务通常在不同的标签类型之间存在相关性，从而影响某个预测的结果。</p>
<p>这意味着具有<strong>已知创作时期和艺术品类型的特定艺术品显着缩小了艺术家归属任务的可能艺术家列表</strong></p>
<p>这个例子的数学解释在等式 1 中以简单的条件概率显示 <span
class="math display">\[
P\left(T_{1} \mid T_{2}, T_{3}\right)=\frac{P\left(T_{1} \cap T_{2} \mid
T_{3}\right)}{P\left(T_{2} \mid T_{3}\right)}
\]</span></p>
<ul>
<li><p>其中 T1 代表属于特定艺术家的艺术品</p></li>
<li><p>而 T2 和 T3 对应于创作时期和使用的材料类型</p></li>
</ul>
<p>这种相关性的一个真实世界的例子是一幅创作时期为 1635
年的绘画和一种布面油画。</p>
<p>这幅画成为梵高的可能性几乎为零，因为梵高直到 1853 年才出生。</p>
<p>它更有可能是伦勃朗，因为他在那个时期很活跃。</p>
<p>因此，我们<strong>假设艺术数据中不同属性之间存在语义纠缠</strong>。</p>
<h3 id="方法概述">3.1 方法概述</h3>
<p>在本文中，我们提出了一种</p>
<blockquote>
<p>基于艺术数据多个属性之间的语义纠缠来学习共享表示的多任务学习方法。</p>
</blockquote>
<p>如图 2 所示，我们的方法包括</p>
<ul>
<li><p>用于<strong>特征提取</strong>的基础层块</p></li>
<li><p><strong>共享表示</strong>块</p></li>
<li><p>聚合所有任务的损失的组合<strong>损失</strong>层</p></li>
<li><p>每个任务的<strong>单独评估</strong>块</p></li>
</ul>
<p>我们在总和中执行损失聚合，根据它们源自的任务类型以及该特定属性对整体表现。</p>
<p>使用这种方法，我们提高了每项任务的判别性能，减少了训练和测试时间，因为一次性数据集遍历。</p>
<figure>
<img src="OmniArt1/2.png" alt="2" />
<figcaption aria-hidden="true">2</figcaption>
</figure>
<h3 id="组合损失层">3.2 组合损失层</h3>
<p>我们提出了一个多任务卷积神经网络，该网络学习关于多种伴随元数据的艺术品的共享表示。</p>
<p>对于每个元数据属性，我们创建单独的任务并在模型中<strong>分配一个单独的分类/回归块</strong>，<strong>每个都有自己的损失函数</strong>。</p>
<p>通过我们的模型有效地传播这些梯度并让它们都正确影响训练的自然方法是总和。</p>
<p>在等式 2 中，Lt 是所有任务的组合损失，而 Li
表示每个单独任务的损失。</p>
<p>参数 wi 和 si
代表任务特定的损失权重矩阵，分别为每个任务分配不同的权重和任务特定的比例因子。
<span class="math display">\[
L_{t} = \sum_i^nw_is_iL_i
\]</span>
需要注意的是，这种组合损失函数的方式仅在它们共享具有可训练参数的层时才有效。</p>
<p>共享这样一个层意味着他们从同一级别获得输入</p>
<p>在损失函数方面，对于艺术家归因等分类任务，我们使用带有
<strong>softmax 函数的分类交叉熵损失</strong>。</p>
<p>对于回归，我们<strong>应用缩放的平均绝对损失和公差为 ±50
年的间隔精度</strong>。</p>
<p>由于区间精度本质上意味着分类，我们利用<strong>平均绝对损失来训练回归块</strong>。</p>
<p>这个挑战中的两个任务是多标签分类任务，因此我们在稀疏标签上使用二元交叉熵损失函数和
sigmoid 激活。</p>
<p>虽然在这样的场景中自然会想到这种类型的损失聚合，但必须考虑损失值的影响和规模</p>
<p>实际上，<strong>产生的损失值回归任务被证明至少是分类任务或多标签任务中生成的任务的
10 倍</strong></p>
<p>这会对学习产生负面影响，因为对共享表示的调整受回归任务的影响最大，而分类损失的重要性则降低了。</p>
<p>为了平衡影响并使分配的任务权重再次可靠，我们在反向传播时手动将回归（平均绝对）损失缩小了
10 倍（sper iod = 10）。</p>
<p><strong>具体的比例因子可以通过在验证阶段监测损失值来确定</strong>。</p>
<h3 id="共享表示层">3.3 共享表示层</h3>
<p>由于我们使用深度模型作为特征提取器，我们将<strong>反向传播效果限制为仅附加层</strong>（每个任务的输出和共享层）。</p>
<p>反过来，与模型中包含的参数总数相比，我们只训练少量参数，因为：</p>
<ul>
<li>我们没有足够的标记数据来有效地从头开始调整深度模型中的过滤器（例如，Resnet-50
有 25,583,592 个可训练参数而没有顶级输出层）</li>
<li>仅训练最终输出块可加快整个过程，同时仍能学习到良好的表示</li>
<li>数据维度更易于管理，训练效果更易于研究。</li>
</ul>
<p>鉴于不同任务Ti中的每个类之间存在联合概率，共享层是关于每个任务的数据的联合表示。</p>
<p>我们根据<strong>激活和隐藏单元的数量</strong>通过实验确定共享层配置。</p>
<blockquote>
<p>Tanh和 Sigmoid 等不同的<strong>激活函数</strong>不会像
ReLU那样在我们的表示中促进稀疏性。</p>
</blockquote>
<p>我们认为稀疏因子在共享层的信息吸收中起着关键作用。</p>
<blockquote>
<p>共享层中<strong>隐藏单元</strong>的数量取决于每个任务的输出目标数量和数据的多样性本身。</p>
</blockquote>
<p>从学习的角度来看，这是可以预期的，因为更多的目标需要更多的可训练参数来学习有效的表示，反之亦然。</p>
<p>从这项工作第一阶段的实验中得出的另一种观点表明，并非所有任务都需要相同的<strong>输出深度</strong>。</p>
<p>实验结果表明，类型和周期预测任务可以通过较浅的架构有效地解决。</p>
<p>这也意味着训练时间的加快并减少内存消耗，因为可训练参数的数量在<strong>微调</strong>和<strong>从头开始设置</strong>中都会减少。</p>
<p>然而，<strong>将输出层分散在模型的不同深度意味着我们无法在共享表示层中对影响所有任务的组合数据表示的联合损失进行建模。</strong></p>
<p>由于该层不是网络的输出，它也可以用作高级特征提取点。</p>
<p>如果模型在每个任务上都表现良好，<strong>那么此时提取的特征将是输入数据的有效代表</strong>。</p>
<p>由于共享层中的单元数量有限，因此维度较低，当内存和计算能力有限时，这些特征将是首选。</p>
<h3 id="基础层">3.4 基础层</h3>
<p>我们的方法本质上与选择特征提取器作为共享表示的基础无关</p>
<p>因为用于预测和评估的特征是在共享层中学习的特征。</p>
<p>随着深度模型在视觉识别任务中的成功，我们尝试了许多不同的深度架构，如
VGG-16、VGG-19、Inception V2 和 ResNet-50 作为特征提取器。</p>
<p>我们的实验结果表明，ResNet-50
模型生成了最好的基础特征，因此我们在我们的方法中指定为特征提取器。</p>
<p>从分类器之前的最后一层提取特征，并通过共享表示层传播到每个任务的不同评估块。</p>
<p>组合损失的反向传播会针对每个任务修改共享表示层中的特征</p>
<h2 id="experiments">4 EXPERIMENTS</h2>
<p>通过我们的实验过程，我们旨在回答以下问题：</p>
<ul>
<li><p>哪个深度模型作为艺术数据集中的基本特征提取器表现最好？</p></li>
<li><p>学习多个相互关联的任务的表示是否可以提高整体预测性能？如果是，如何？</p></li>
<li><p>不同类型的任务（分类、回归、多标签）在组合环境中如何相互影响？</p></li>
<li><p>当任务属于不同类型时，哪些参数在共享表示设置中效果最好？</p></li>
<li><p>共享表示能否学习任务之间的语义联系并产生定性洞察？</p></li>
</ul>
<p>我们的实验装置有两个阶段。</p>
<p>在第一阶段，我们评估了国家博物馆挑战赛中<strong>每个任务的单个模型的性能</strong>，并将深度学习与
Mensink 等人和其他最先进的方法进行了比较。</p>
<p>我们还提供新 OmniArt 数据集上四个任务的基线结果。</p>
<p>在评估模型并选择具有最佳预测性能的模型后，我们继续进行第 2 阶段。</p>
<p>实验设置的第 2 阶段侧重于</p>
<blockquote>
<p><strong>针对性能最佳的单任务深度学习模型评估具有不同超参数集、数据集拆分和共享表示大小的多任务模型</strong></p>
</blockquote>
<p>在这个阶段，我们还会生成最终结果。</p>
<h3 id="datasets">4.1 Datasets</h3>
<p>在我们的研究中，我们依赖于多个数据源，如博物馆、艺术维基网站和预编译数据集。</p>
<p>首先，我们从 WikiArt 爬取了一个数据集，其中包含来自 3000
多位艺术家、150 种类型和 14 个不同历史时期的 126,078 张图像。</p>
<p>该数据集已用于评估各种算法，因为它将艺术作品与风格和流派联系起来</p>
<p>但是，WikiArt 缺少材料信息，目前未包含在 OmniArt 中。</p>
<p>数字艺术数据的一个相对较新的补充是大都会博物馆的在线收藏品，其中包含近
50 万件艺术品和广泛的元数据。</p>
<p>在组装这个数据集时，我们注意到几乎一半的样本不属于公共领域或没有照片复制品，无法使用。</p>
<p>一个相当小的仅包含绘画的数据集是包含 5000 个绘画样本的 YourPaintings
数据集，并且可以通过对象级注释公开获得。</p>
<p>法国国家图书馆收藏有 4,000 个样本，也可用
，但由于缺乏材料信息及其依赖于法语注释，如 YourPaintings
数据集，目前也被排除在外。</p>
<p>所有结果都适用于相同的数据集和拆分类型。</p>
<blockquote>
<p>我们将 70% 的数据集用于训练，20% 用于验证，10% 用于测试目的。</p>
</blockquote>
<p>拆分是按class进行的，这样每个class在所有实验阶段都具有相同的分布。</p>
<p>我们的方法依赖于<strong>单次遍历数据集</strong>，因此只能针对单个任务
(Ti) 进行拆分。</p>
<p>在我们的案例中，由于数据集中的类别数量最多且具有细粒度的性质，因此该任务是艺术家归因。</p>
<blockquote>
<p>像这样进行拆分会导致其他任务中每个类别的样本分布不平衡，但我们通过使用它们在验证集上的组合损失中的比率来分配任务权重
(wi) 来解决这个问题。</p>
</blockquote>
<p>由于艺术家是层次结构中最具体的class，我们计算与此任务相关的数据分布。</p>
<p>出于这个原因，我们只能将我们的实验结果与 2014
年的国立博物馆挑战赛，在周期、材料和类型预测的完整数据集上进行比较。</p>
<p>使用我们的方法，我们还报告了新 OmniArt
挑战中相同四个任务的基线性能。</p>
<p>我们将在接受后发布用于特征提取、数据拆分和评估引擎的模型，作为以博物馆为中心的挑战，并继续收集更多数据</p>
<h4 id="rijks14-数据集">4.1.1 Rijks’14 数据集</h4>
<p>Rijks'14 数据集由 T. Mensink 等人介绍。</p>
<p>2014 年，作为提议的国立博物馆挑战的一部分。 该数据集包含 112,039
幅在国立博物馆展出的艺术品的照片复制品</p>
<p>由于 2014
年国立博物馆挑战赛的数据集是以博物馆为中心的观点，它提供了各种对象类型，包括绘画、照片、陶瓷、家具、雕塑等。</p>
<p>数据集中的每个条目最多有四个标签。</p>
<p><strong>如果标签丢失或未知，则为条目分配未知类。</strong></p>
<h4 id="omniart-challenge-2017.">4.1.2 OmniArt Challenge 2017.</h4>
<p>自 2014 年以来，国立博物馆更新了他们的数字可用内容，其中包含 90,000
多幅来自其收藏的艺术品的摄影复制品。</p>
<p>大都会艺术博物馆实施了一项名为“开放获取”的新政策，根据知识共享零
(CC0)
的规定，它可以广泛且免费地提供其认为属于公共领域的艺术品图像，以供不受限制地使用。</p>
<p>大都会还制作了来自整个在线收藏的元数据，作为其网站上可用的艺术家、标题、时期、媒介和材料和尺寸等图像的注释。</p>
<p>目前估计他们收藏的艺术品总数为 442,554
件，但其中只有一半拥有属于公共领域的摄影复制品。</p>
<p>类似的注释可以在 Web Gallery of Art 数据集中找到，其中 40,000 件（约
28,000
幅绘画）艺术品与丰富的元数据相关联，如艺术家、技术、时期、类型、学校、地理起源等。</p>
<p>使用更新后的国立博物馆藏品，大都会博物馆和网络艺术画廊收藏品中的新藏品，我们创建了一个新的数据集，其中包含
432,217 件艺术品的摄影复制品以及丰富的元数据。</p>
<p>此外，由于所有类型和材料都已翻译成英文，注释的质量也得到了提高。</p>
<blockquote>
<p>在预定义的训练、验证和测试拆分中消除了艺术品上的模糊标签，如未知、匿名和首字母缩略词，以便明确定义分类问题。</p>
</blockquote>
<p>除了以前可用的数据之外，我们还提供了一个元数据扩展，其中包含
IconClass 、颜色代码、当前位置、实际大小和地理起源和技术等属性。</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text { Table 1: Featured datasets task-wise statistics }\\
&amp;\begin{array}{lccccc}
\hline \text { Data set } &amp; \text { Entries } &amp; \text { Artists
} &amp; \text { Types } &amp; \text { Periods } &amp; \text { Materials
} \\
\hline \text { Rijks&#39;14 } &amp; 112,039 &amp; 6,626 &amp; 1,054
&amp; 628 &amp; 406 \\
\text { The Met } &amp; 201,953 &amp; 6,602 &amp; 561 &amp; 2,340 &amp;
5,221 \\
\text { OmniArt } &amp; 432,217 &amp; 21,364 &amp; 837 &amp; 2,389 &amp;
6,385 \\
\hline
\end{array}
\end{aligned}
\]</span></p>
<h3 id="preprocessing">4.2 Preprocessing</h3>
<p><strong>具有深度架构的模型的缺点是需要大量数据才能正确训练和学习相关特征
。</strong></p>
<p>考虑到这一点，我们将数据增强技术应用于我们的数据，以扩展数据集并引入标签安全的变体。</p>
<p>我们尝试了水平翻转、随机旋转、均值减法和 ZCA 美白。</p>
<p>在所有情况下，均值减法会使性能变差。</p>
<p>虽然可能不是预期的，但对于周期、类型和材料的预测是合乎逻辑的，因为输入样本的完整性很重要。</p>
<p>从金属雕刻中减去平均值板会导致原始图像的印象模糊，丢失重要的纹理信息。</p>
<blockquote>
<p>当仅将<strong>水平翻转</strong>应用于数据集中的随机图像时，我们获得了最佳结果，因此这是我们使用的唯一增强。</p>
</blockquote>
<h3 id="tasks-description">4.3 Tasks description</h3>
<p>由于我们使用 The Rijksmuseum 2014
挑战赛的结果作为我们的主要基线，下面我们将描述我们评估模型的挑战赛中提出的不同任务。</p>
<h4 id="艺术家归属">4.3.1 艺术家归属</h4>
<p>OmniArt 数据集中有超过 21,000 位艺术家，其中 23 位拥有超过 700
件艺术品收藏。</p>
<p>在 OmniArt 数据集中，<strong>有一位未知艺术家的作品</strong>。</p>
<p>对于 Rijks’14 数据集，未知类在表 2 中用 +u 标记。</p>
<p>这些艺术品也被排除在实验之外，因为它们可能属于 OmniArt
挑战的现有类别。</p>
<p>艺术家归属是一项多类分类任务。 此任务的评估块包含一个 <strong>softmax
层</strong>和用于<strong>不平衡数据拆分的分类权重矩阵</strong>（用于
Rijks'14）</p>
<h4 id="创作期估计">4.3.2 创作期估计</h4>
<p>艺术品的范围从古代史前时代到 19 世纪后期。</p>
<p>对于许多可以追溯到早期历史时期的艺术品，没有确切的创作日期，因此提供了估计的创作间隔。</p>
<p>在这些情况下，我们将<strong>间隔的平均值作为创建日期</strong>。</p>
<p>我们将创建周期估计视为<strong>回归任务</strong>，以<strong>平均绝对误差（以年为单位）为指标</strong>。</p>
<p>我们使用从我们的共享表示中提取的特征训练了一个回归器。</p>
<h4 id="材料预测">4.3.3 材料预测</h4>
<p>OmniArt 数据集中的每件艺术品都有超过 6300 种材料。</p>
<p>这个任务是一个<strong>多标签分类问题</strong>，因为每件艺术品都可以有一种或多种材料。</p>
<p>在 OmniArt 挑战赛的 180,387
件艺术品中，<strong>纸张是最常见的材料</strong>。</p>
<blockquote>
<p>由于任务的<strong>不平衡性质</strong>，这有助于<strong>为材料任务分配较低的损失权重</strong>。</p>
</blockquote>
<p>由于材料通常有<strong>嘈杂</strong>的标签，我们将词干应用于材料的每个单词。</p>
<blockquote>
<p><strong>二元交叉熵</strong>被指定为材料预测的损失函数，以平均精度为度量</p>
</blockquote>
<h4 id="类型预测">4.3.4 类型预测</h4>
<p>类型预测是一项多标签分类任务，在 OmniArt 挑战赛中有 837
个不同的类别。</p>
<p>最常见的艺术品类型是paint（s），出现了 108,823 次。</p>
<p>具有大量示例的有趣类型类别包括 36.785 个条目的painting和 31,396
个条目的photograph。</p>
<p>对于这个任务，我们使用与材料预测相同的设置，但任务权重不同。</p>
<h3 id="第一阶段选择基础层">4.4 第一阶段：选择基础层</h3>
<p>在第一阶段，我们评估流行的深度模型在我们四个任务中的每一个的性能。</p>
<p>评估是在<strong>微调设置中进行的</strong>，从<strong>从头开始训练</strong>设置，最后是我们使用<strong>额外的外部数据集</strong>来调整模型的在为手头的任务进行微调之前的权重和过滤器的设置</p>
<p><strong>微调是通过仅训练模型的最后一层而不修改预先学习的过滤器来执行的</strong></p>
<p>此阶段允许查看不同的模型架构如何对我们的每个任务做出反应，并深入了解哪种模型最适合多任务场景中的所有任务。</p>
<p>实验设计的第 1
阶段是关于在单个任务上测试模型性能，以评估用于它们组合的最佳架构。</p>
<p>我们在 ImageNet 上尝试了几种性能最好的深度架构，例如 Resnet-50
、VGG-16、VGG-19 和 Inception v2</p>
<p>我们使用 ResNet-50
模型（没有顶部块）的特征获得了最佳结果，并在其他实验中继续将其用作主要特征提取单元。</p>
<p>我们实验设计的这个阶段特别重要，因为它可以直接与所有四个任务中的最先进方法进行比较，因为我们可以使用相同的数据拆分。</p>
<p>表 2 显示了 Mensink 等人的手工特征方法、CNN 和我们的方法 OmniArt
之间的直接比较。</p>
<figure>
<img src="OmniArt1/table2.png" alt="table2" />
<figcaption aria-hidden="true">table2</figcaption>
</figure>
<h3 id="第二阶段评估我们的多任务方法">4.5
第二阶段：评估我们的多任务方法</h3>
<p>第二阶段包括<strong>构建符合问题多任务性质的最优架构</strong>。</p>
<p>在方法部分中，<strong>我们解释了共享表示可以有利于目标相关的任务</strong>。</p>
<p>配置最佳共享表示是我们在此阶段尝试完成的目标，同时找到适合数据集每个拆分的最佳共享表示大小。</p>
<p>在这个阶段，我们<strong>测试了各种超参数</strong>并选择了整体性能最佳的设置，因为所有任务都有不同的性质，并且容易对架构的变化做出不同的反应。</p>
<p>我们通过比较在 The Rijksmuseum Challenge 中提出的所有 4
个任务的分数来评估我们的多任务模型。</p>
<p>对于 Rijks'14 数据集，我们与 Mensink
等人进行了比较。因为他们是国立博物馆挑战赛的原始创造者，并且对所有提议的任务都有分数。</p>
<p>对于 OmniArt
数据集，我们比较了我们的方法与单任务深度卷积神经网络的性能。</p>
<p>在表 2 中，我们观察到使用多任务方法在所有 4
个任务上都获得了最佳性能。</p>
<p>然而，在 374+u
案例中，手工制作的特征在艺术家归因方面的表现优于深度网络。</p>
<p>这可能是由于每个类的示例数量非常有限，因此无法学习良好的表示，而手工制作的特征即使对于如此少量的数据也能保持其质量。</p>
<p>在表 4 中，我们看到了 OmniArt 方法与单任务深度 CNN 的性能对比。</p>
<p>一个有趣的发现是，随着<strong>每个艺术家阈值的样本降低，艺术家归因性能下降，但对于多标签任务则相反。</strong></p>
<p>我们相信，当我们使用更高百分比的数据集时，我们在多标签设置中每个类获得更多样本，而输出目标的数量保持不变，这对于表示学习很重要。</p>
<figure>
<img src="OmniArt1/table4.png" alt="table4" />
<figcaption aria-hidden="true">table4</figcaption>
</figure>
<blockquote>
<p>共享表示大小对于性能也很重要。</p>
</blockquote>
<p>我们尝试了从 1024 到 8192 单位的大小，并在 6144
单位的所有任务中实现了最佳的整体平衡。</p>
<p>通过本阶段的实验，我们观察到具<strong>有较小的共享表示层有利于周期估计</strong>，其中使用<strong>较小的共享表示大小</strong>可以实现
8.3 年的平均绝对误差下降。</p>
<p>一个有趣的事实是，在保持良好的判别性能的同时，多任务方法显着缩短了训练和测试时间，使其比模型每任务方法更有效。</p>
<p>我们计算出，在测试阶段，所有 4 个任务需要 OmniArt 6.22 秒才能完成 200
批 32 张图像。</p>
<p>对于相同的设置，单任务 CNN 每个任务需要 2.13 秒。</p>
<p>在这种情况下，这是 25% 的加速。</p>
<p>在训练阶段可以观察到执行时间的更大改进。</p>
<p>OmniArt 在单个 Nvidia Titan X 上每类设置 &gt; 1100
个样本进行训练大约需要 73 分钟，而四个单任务模型的组合训练时间为 198
分钟，比我们的多任务方法慢 2.6 倍。</p>
<h3 id="定性分析">4.6 定性分析</h3>
<p>定量表现测量显示出良好的艺术家归属表现，但存在艺术家归属的错误分类。</p>
<p>在我们移除主对角线后，进一步探索在图 5 和图 3
中清晰可见的类之间的内部混淆，揭示了一个有趣的发现，我们称之为
<strong>Luyken 案例</strong>。</p>
<figure>
<img src="OmniArt1/fig3.png" alt="fig3" />
<figcaption aria-hidden="true">fig3</figcaption>
</figure>
<figure>
<img src="OmniArt1/fig5.png" alt="fig5" />
<figcaption aria-hidden="true">fig5</figcaption>
</figure>
<h4 id="the-luyken-case">The Luyken case</h4>
<p>Luyken 案例源于 17 世纪晚期荷兰书籍插画家 Caspar Luyken（第 16 级）和
Jan Luyken（第 12 级）之间的混淆，在图 5 中清晰可见。</p>
<p>Caspar Luyken是 Jan
Luyken（荷兰书籍插画家）的儿子，他的父亲也是他的老师，他们经常一起使用相同的技术和材料。</p>
<p>作为父子，他们也共享同一时期，平均误差为 52.2 年，很容易被忽略。</p>
<p>然而，几乎所有的混淆样本都来自他们共同活跃的时期，而正确的样本要么来自儿子生产生活的后期，要么来自父亲的早期</p>
<p>额外的数据集探索表明，正确指定的艺术品在材料和时期上存在差异，这进一步证明了将其视为多任务问题的好处。</p>
<p>这一发现表明，在与父亲分离并前往德国的一家艺术品经销商工作后，儿子的技术发生了演变
。</p>
<p>这样的结果支持我们的假设，<strong>即不同任务中不同数据属性之间的语义纠缠可以用我们的方法在共享表示中建模。</strong></p>
<h2 id="conclusions">5 CONCLUSIONS</h2>
<p>OmniArt 方法在 Rijks
的数据集上优于当前最先进的方法，并加快了训练和测试时间。</p>
<p>就最佳定量结果而言，性能可以受益于传统的提升技术，例如训练模型集合、应用不同的投票技术和调整超参数。</p>
<p>虽然绝对数字是一项重要的质量和性能衡量标准，但基本改进足以证明我们的方法是有效的。</p>
<p>我们不断扩展和改进的 OmniArt
挑战以挑战的形式呈现，以刺激艺术数据领域的进一步研究和开发。</p>
<p>虽然这项工作只关注艺术数据，但同样的想法可以很容易地适应并转移到不同的领域。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://www.larryai.com">扁同学不发言</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.larryai.com/2022/07/19/OmniArt1/">http://www.larryai.com/2022/07/19/OmniArt1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.larryai.com" target="_blank">扁同学不发言的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/cs-CV/">cs.CV</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E5%B1%9E%E6%80%A7%E8%AF%86%E5%88%AB/">属性识别</a></div><div class="post_share"><div class="social-share" data-image="/2022/07/19/OmniArt1/1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/01/ucart-ai/"><img class="prev-cover" src="/2022/08/01/ucart-ai/.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2022/05/16/SEAN/"><img class="next-cover" src="/2022/05/16/SEAN/fig4.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SEAN:Image Synthesis With Semantic Region-Adaptive Normalization</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/05/04/GanGANv2/" title="GanGANv2"><img class="cover" src="/2022/05/04/GanGANv2/3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-04</div><div class="title">GanGANv2</div></div></a></div><div><a href="/2022/05/04/Image%20Style%20Transfer/" title="Image Style Transfer"><img class="cover" src="/2022/05/04/Image%20Style%20Transfer/fig1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-04</div><div class="title">Image Style Transfer</div></div></a></div><div><a href="/2022/05/04/MASA-SR/" title="MASA-SR"><img class="cover" src="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-04</div><div class="title">MASA-SR</div></div></a></div><div><a href="/2022/05/04/MGUIT/" title="MGUIT"><img class="cover" src="/2022/05/04/MGUIT/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-04</div><div class="title">MGUIT</div></div></a></div><div><a href="/2022/05/04/PLST-SR/" title="Perceptual Losses for Real-Time Style Transfer and Super-Resolution"><img class="cover" src="https://pic4.zhimg.com/v2-8e9936fdcfd4e8371720b9834f8f97d7_r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-04</div><div class="title">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</div></div></a></div><div><a href="/2022/05/16/SEAN/" title="SEAN:Image Synthesis With Semantic Region-Adaptive Normalization"><img class="cover" src="/2022/05/16/SEAN/fig4.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-16</div><div class="title">SEAN:Image Synthesis With Semantic Region-Adaptive Normalization</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/%E6%96%B0%E4%B8%80.JPG" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">扁同学不发言</div><div class="author-info__description">AIArt@HDU</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Larry-zx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Larry-zx" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://www.zhihu.com/people/larry-19-22-31" target="_blank" title="Zhihu"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="https://mobile.twitter.com/Larry37722397" target="_blank" title="Twitter"><i class="fa-brands fa-twitter"></i></a><a class="social-icon" href="https://www.youtube.com/channel/UCzDXYobI-mUP5WgvA37er6A" target="_blank" title="Youtube"><i class="fa-brands fa-youtube"></i></a><a class="social-icon" href="https://www.kaggle.com/larryzxai" target="_blank" title="Kaggle"><i class="fa-brands fa-kaggle"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Hi , welcome to my blog 🤔</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#omniart-multi-task-deep-learning-for-artistic-data-analysis"><span class="toc-text">OmniArt:
Multi-task Deep Learning for Artistic Data Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text">ABSTRACT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text">1 INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text">2 RELATED WORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%89%BA%E6%9C%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-text">2.1 艺术数据分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="toc-text">2.2 多任务学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text">3 METHOD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1 方法概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%84%E5%90%88%E6%8D%9F%E5%A4%B1%E5%B1%82"><span class="toc-text">3.2 组合损失层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E8%A1%A8%E7%A4%BA%E5%B1%82"><span class="toc-text">3.3 共享表示层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%B1%82"><span class="toc-text">3.4 基础层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#experiments"><span class="toc-text">4 EXPERIMENTS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#datasets"><span class="toc-text">4.1 Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#rijks14-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">4.1.1 Rijks’14 数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#omniart-challenge-2017."><span class="toc-text">4.1.2 OmniArt Challenge 2017.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#preprocessing"><span class="toc-text">4.2 Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tasks-description"><span class="toc-text">4.3 Tasks description</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%89%BA%E6%9C%AF%E5%AE%B6%E5%BD%92%E5%B1%9E"><span class="toc-text">4.3.1 艺术家归属</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E4%BD%9C%E6%9C%9F%E4%BC%B0%E8%AE%A1"><span class="toc-text">4.3.2 创作期估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%90%E6%96%99%E9%A2%84%E6%B5%8B"><span class="toc-text">4.3.3 材料预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B1%BB%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-text">4.3.4 类型预测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E9%80%89%E6%8B%A9%E5%9F%BA%E7%A1%80%E5%B1%82"><span class="toc-text">4.4 第一阶段：选择基础层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E8%AF%84%E4%BC%B0%E6%88%91%E4%BB%AC%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%96%B9%E6%B3%95"><span class="toc-text">4.5
第二阶段：评估我们的多任务方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E5%88%86%E6%9E%90"><span class="toc-text">4.6 定性分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#the-luyken-case"><span class="toc-text">The Luyken case</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusions"><span class="toc-text">5 CONCLUSIONS</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/01/ucart-ai/" title="无题"><img src="/2022/08/01/ucart-ai/.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/08/01/ucart-ai/" title="无题">无题</a><time datetime="2022-08-01T07:37:14.000Z" title="发表于 2022-08-01 15:37:14">2022-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/19/OmniArt1/" title="OmniArt: Multi-task Deep Learning for Artistic Data Analysis"><img src="/2022/07/19/OmniArt1/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="OmniArt: Multi-task Deep Learning for Artistic Data Analysis"/></a><div class="content"><a class="title" href="/2022/07/19/OmniArt1/" title="OmniArt: Multi-task Deep Learning for Artistic Data Analysis">OmniArt: Multi-task Deep Learning for Artistic Data Analysis</a><time datetime="2022-07-19T13:52:50.000Z" title="发表于 2022-07-19 21:52:50">2022-07-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/16/SEAN/" title="SEAN:Image Synthesis With Semantic Region-Adaptive Normalization"><img src="/2022/05/16/SEAN/fig4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SEAN:Image Synthesis With Semantic Region-Adaptive Normalization"/></a><div class="content"><a class="title" href="/2022/05/16/SEAN/" title="SEAN:Image Synthesis With Semantic Region-Adaptive Normalization">SEAN:Image Synthesis With Semantic Region-Adaptive Normalization</a><time datetime="2022-05-16T09:53:19.000Z" title="发表于 2022-05-16 17:53:19">2022-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/16/%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%89%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/" title="图像的三种数据格式"><img src="/2022/05/16/%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%89%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/kil.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="图像的三种数据格式"/></a><div class="content"><a class="title" href="/2022/05/16/%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%89%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/" title="图像的三种数据格式">图像的三种数据格式</a><time datetime="2022-05-16T01:56:13.000Z" title="发表于 2022-05-16 09:56:13">2022-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/16/video2pic/" title="OPENCV-视频与图像的转换"><img src="/2022/05/16/video2pic/a13.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="OPENCV-视频与图像的转换"/></a><div class="content"><a class="title" href="/2022/05/16/video2pic/" title="OPENCV-视频与图像的转换">OPENCV-视频与图像的转换</a><time datetime="2022-05-16T01:27:34.000Z" title="发表于 2022-05-16 09:27:34">2022-05-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By 扁同学不发言</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="#">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'H2XPsPqmRrVmO7wLvvlhsgyi-gzGzoHsz',
      appKey: 'qUDSqSmyX1JHlTwo3fQLXiSU',
      avatar: 'monsterid',
      serverURLs: 'https://h2xpspqm.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script>((window.gitter = {}).chat = {}).options = {
  disableDefaultChat: true,
};
document.addEventListener('gitter-sidecar-ready', (e) => {
  const GitterChat = e.detail.Chat
  let chat

  function initGitter () {
    chat = new GitterChat({
      room: 'babylearnDL/community',
      activationElement: '#chat_btn'
    });
  }

  initGitter()

  if (false) {
    document.addEventListener('pjax:complete', () => {
      chat.destroy()
      initGitter()
    })
  }

})</script><script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async="async" defer="defer"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>