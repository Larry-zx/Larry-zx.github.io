<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer | Hexo</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Abstract 主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。 性能最好的模型还通过注意力机制连接编码器和解码器。 我们提出了一种新的简单网络架构 Transformer 它完全基于注意力机制，完全摒弃了递归和卷积 对两个机器翻译任务的实验表明，这些模型在质量上更优越，同时更可并行化，并且需要的训练时间显着减少。  我们的模型在 WMT 2014 英德翻译任务上达到了 28">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://example.com/2022/05/03/Transformer/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Abstract 主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。 性能最好的模型还通过注意力机制连接编码器和解码器。 我们提出了一种新的简单网络架构 Transformer 它完全基于注意力机制，完全摒弃了递归和卷积 对两个机器翻译任务的实验表明，这些模型在质量上更优越，同时更可并行化，并且需要的训练时间显着减少。  我们的模型在 WMT 2014 英德翻译任务上达到了 28">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-05-03T09:11:37.977Z">
<meta property="article:modified_time" content="2022-05-03T17:25:35.288Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/05/03/Transformer/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-04 01:25:35'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Hexo</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-05-03T09:11:37.977Z" title="Created 2022-05-03 17:11:37">2022-05-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-05-03T17:25:35.288Z" title="Updated 2022-05-04 01:25:35">2022-05-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="abstract">Abstract</h1>
<p>主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。</p>
<p>性能最好的模型还通过注意力机制连接编码器和解码器。</p>
<p>我们提出了一种新的简单网络架构 <strong>Transformer</strong></p>
<p><strong>它完全基于注意力机制，完全摒弃了递归和卷积</strong></p>
<p>对两个机器翻译任务的实验表明，这些模型在质量上更优越，同时更可并行化，并且需要的训练时间显着减少。</p>
<ul>
<li>我们的模型在 WMT 2014 英德翻译任务上达到了 28.4
BLEU，比现有的最佳结果（包括合奏）提高了 2 BLEU 以上。</li>
<li>在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5
天后，建立了一个新的单模型 state-of-the-art BLEU 得分
41.0，这是最好的训练成本的一小部分 文献中的模型。</li>
</ul>
<h1 id="introduction">1.Introduction</h1>
<p>循环神经网络(RNN)、长短期记忆(LSTM)和门控循环神经网络，尤其是在语言建模和机器翻译等序列建模和转导问题已被牢固确立为最先进的方法此后，许多努力继续推动循环语言模型和编码器-解码器架构的界限</p>
<p>循环模型通常沿输入和输出序列的符号位置考虑计算。</p>
<p>将位置与计算时间的步骤对齐，它们生成一系列隐藏状态
ht，作为先前隐藏状态 ht-1 和位置 t 的输入的函数。</p>
<p>这种固有的顺序性质排除了训练示例中的并行化，这在更长的序列长度下变得至关重要，因为内存限制限制了示例之间的批处理。</p>
<p>最近的工作通过因式分解技巧 和条件计算
显着提高了计算效率，同时在后者的情况下也提高了模型性能。</p>
<p>然而，顺序计算的基本约束仍然存在。</p>
<p>注意机制已成为各种任务中引人注目的序列建模和转导模型的组成部分，允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离</p>
<p>然而，除了少数情况，这种注意力机制与循环网络结合使用。</p>
<p>在这项工作中，我们提出了
<strong>Transformer，这是一种避免重复的模型架构，而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系</strong></p>
<p>在八个 P100 GPU 上经过短短 12 小时的训练后，Transformer
可以实现更多的并行化，并且可以在翻译质量方面达到新的水平</p>
<h1 id="background">2.Background</h1>
<p>减少顺序计算的目标也构成了扩展神经 GPU 、ByteNet 和 ConvS2S
的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入的隐藏表示和
输出位置。</p>
<p>在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长</p>
<p>对于 ConvS2S 呈线性增长，而对于 ByteNet 则呈对数增长。</p>
<p>这使得学习远距离位置之间的依赖关系变得更加困难</p>
<p>在 Transformer
中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而降低了有效分辨率，我们使用<strong>多头注意力(Multi-Head
Attention)</strong>来抵消这种影响</p>
<p><strong>自注意力(Self-attention)</strong>，有时称为内部注意力，是一种将单个序列的不同位置关联起来以计算序列表示的注意力机制</p>
<p>自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴涵和学习任务无关的句子表示</p>
<p><strong>端到端记忆网络(End-to-end memory
network)</strong>基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务中表现良好</p>
<p>然而，据我们所知</p>
<p><strong>Transformer
是第一个完全依赖自注意力来计算其输入和输出表示而不使用序列对齐 RNN
或卷积的转换模型</strong></p>
<p>在接下来的部分中，我们将描述
Transformer，激发自注意力并讨论其相对于其他模型的优势</p>
<h1 id="model-architecture">3.Model Architecture</h1>
<p>大多数竞争性神经序列转导模型具有编码器-解码器结构</p>
<p>编码器将符号表示的输入序列 (x1, ..., xn) 映射到连续表示的序列 z =
(z1, ..., zn)</p>
<blockquote>
<p>其中z1是一个向量 用一个向量来表示x1</p>
</blockquote>
<p>给定 z，解码器然后一次生成一个元素的符号输出序列 (y1, ..., ym)</p>
<p>在每个步骤中，模型都是自回归(auto-regressive)的,<strong><em>在生成下一个时将先前生成的符号用作附加输入</em></strong></p>
<p>Transformer
遵循这种整体架构，对编码器和解码器使用堆叠的自注意力(self-attention)和point-wise</p>
<p>编码器和解码器的全连接层，分别如图 1 的左半部分和右半部分所示</p>
<p><img src="结构图片/fig1.png" alt="结构图" style="zoom:100%;" /></p>
<h2 id="encoder-and-decoder-stacks">3.1Encoder and Decoder Stacks</h2>
<h4 id="encoder">3.1.1Encoder</h4>
<p>编码器由 N = 6 个相同的层组成 , 每层有两个子层</p>
<ul>
<li><p>第一个子层是 <strong>多头自注意力机制(multi-head self-attention
mechanism)</strong></p></li>
<li><p>第二个子层是simple, position-wise fully connected feed-forward
network.（说简单点就是<strong>MLP</strong>）</p></li>
</ul>
<p>我们在两个子层中的每一个周围使用残差连接，然后进行层归一化</p>
<p>即每个子层的输出为<strong>LayerNorm(x + Sublayer(x))</strong></p>
<p>其中Sublayer(x)是子层自己实现的函数</p>
<p>为了促进这些残差连接，模型中的所有子层以及嵌入层都会产生维度 dmodel =
512 的输出</p>
<blockquote>
<p>LayerNorm的细节可以参考下面链接</p>
<p>https://blog.csdn.net/jump882/article/details/119795466</p>
</blockquote>
<h4 id="decoder">3.1.1Decoder</h4>
<p>解码器也由一堆 N = 6 个相同的层组成。</p>
<p>除了每个编码器层中的两个子层之外，解码器还插入了第三个子层</p>
<p>该子层对编码器堆栈的输出执行多头注意力（multi-head attention）</p>
<p>与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化</p>
<p>我们还修改了解码器堆栈中的自注意力子层，以<strong>防止位置关注后续位置</strong></p>
<p><strong>这种掩蔽与输出嵌入偏移一个位置的事实相结合，确保对位置 i
的预测只能依赖于位置小于 i 的已知输出</strong></p>
<h2 id="attention">3.2Attention</h2>
<p>注意力函数可以描述为将a
query(查询)和一组key-value键值对映射到输出</p>
<p>其中<strong>查询query</strong>、<strong>键key</strong>、<strong>值value</strong>和<strong>输出</strong>都是向量</p>
<p><strong>输出可以理解为计算值value的加权和所得</strong></p>
<p>其中<strong>分配给每个value的权重weight由查询query与相应键key的相似度函数计算</strong></p>
<p>下面给了一张参考图</p>
<p><img src="结构图片/attention1.png" /></p>
<h4 id="scaled-dot-product-attention">3.2.1 Scaled Dot-Product
Attention</h4>
<figure>
<img src="结构图片/fig2_left.png" alt="attention" />
<figcaption aria-hidden="true">attention</figcaption>
</figure>
<p>我们将我们的particular attention称为“Scaled Dot-Product
Attention”</p>
<p>输入由维度 dk 的query和key以及维度 dv 的value组成</p>
<p><strong>我们计算的query和所有keys的点积，将每个key除以 <span
class="math inline">\(\sqrt{d_k}\)</span>，然后应用 softmax
函数来获得value的权重</strong></p>
<p>在实践中，我们同时计算一组querys的注意力函数，并打包到矩阵 Q 中</p>
<p>key和value也打包到矩阵 K 和 V 中</p>
<p>我们将输出矩阵计算为： <span class="math display">\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span> 下面给了一张参考图 当m=1时就跟单独运算一样</p>
<p><img src="结构图片/attention2.png" /></p>
<p>两个最常用的注意功能是</p>
<ul>
<li><p>加性注意 （additive attention ）</p></li>
<li><p>点积（乘法）注意（dot-product (multiplicative)
attention）</p></li>
</ul>
<p>点积注意力与我们的算法相同，除了 <span
class="math inline">\(\sqrt{d_k}\)</span> 的比例因子</p>
<p>Additive attention使用具有单个隐藏层的前馈网络计算兼容性函数</p>
<p>虽然两者在理论上的复杂性相似，但<strong>点积注意力在实践中更快且更节省空</strong>间，因为它可以使用高度优化的矩阵乘法代码来实现</p>
<p>虽然对于较小的 dk
值，这两种机制的性能相似，但加法注意力优于点积注意力，而无需对较大的 dk
值进行缩放</p>
<p>我们怀疑对于较大的 dk 值，点积的幅度会变大，从而将 softmax
函数推入具有极小梯度的区域</p>
<p>为了抵消这种影响，我们将点积缩放<span
class="math inline">\(\sqrt{d_k}\)</span></p>
<blockquote>
<p>注意Mask部分
具体操作就是将qt之后的值给换成一个非常大的负数，在后续的softmax时候就会变成0</p>
<p>使得计算结果只用到了v1到vt-1的结果</p>
</blockquote>
<h4 id="multi-head-attention">3.2.2 Multi-Head Attention</h4>
<figure>
<img src="结构图片/fig2_right.png" alt="attention" />
<figcaption aria-hidden="true">attention</figcaption>
</figure>
<p>与使用 dmodel维度的key、value和query执行单个注意函数不同</p>
<p>我们发现将查询、键和值分别线性投影到 dk、dk 和 dv
维度上的不同学习线性投影是有益的（投影到低维度）</p>
<blockquote>
<p>相当于给h次机会 希望能够学到不一样的投影的方式</p>
<p>使得在投影进去的度量空间里面 能够去匹配不同模式的相似函数</p>
<p>类似卷积神经网络中有多个输出通道的感觉</p>
</blockquote>
<p>然后，在每个查询、键和值的投影版本上，我们并行执行 Scaled Dot-Product
Attention，产生 dv 维输出值。</p>
<p>这些被连接cat起来并再次投影，产生最终值</p>
<p>Multi-Head
Attention允许模型共同关注来自不同位置的不同表示子空间的信息</p>
<p>对于单个注意力头，平均化会抑制这一点</p>
<figure>
<img
src="/Users/编程/md文档/论文解读/transformer/公式图片/multi-head.png"
alt="截屏2022-05-03 21.06.37" />
<figcaption aria-hidden="true">截屏2022-05-03 21.06.37</figcaption>
</figure>
<ul>
<li><p>Q 矩阵从[m,dmodel] 降维到[m , dk] 那么<span
class="math inline">\(W_i^Q \in \mathbb{R}^{d_{model}\times
d_k}\)</span></p></li>
<li><p>K 矩阵从[n,dmodel] 降维到[n , dk] 那么<span
class="math inline">\(W_i^K \in \mathbb{R}^{d_{model}\times
d_k}\)</span></p></li>
<li><p>V 矩阵从[n,dmodel] 降维到[n , dv] 那么<span
class="math inline">\(W_i^V \in \mathbb{R}^{d_{model}\times
d_v}\)</span></p></li>
</ul>
<p>在这项工作中，我们使用 h = 8 个并行注意力层或头</p>
<p>对于其中的每一个，我们使用 dk = dv = dmodel/h = 64</p>
<p>由于每个头的维度减少，总计算成本类似于具有全维度的单头注意力</p>
<h4 id="applications-of-attention-in-our-model">3.2.3 Applications of
Attention in our Model</h4>
<p>Transformer 以三种不同的方式使用多头注意力：</p>
<ul>
<li>在“编码器-解码器注意力”层中，query来自前一个解码器层，记忆key和value来自编码器的输出。
这允许解码器中的每个位置参与输入序列中的所有位置。
这模仿了序列到序列模型中典型的编码器-解码器注意机制</li>
<li>编码器包含自注意力层
在自注意力层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器中前一层的输出。
编码器中的每个位置都可以关注编码器上一层中的所有位置。</li>
<li>类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。
我们需要防止解码器中的信息向左流动，以保持自回归特性。
我们通过屏蔽掉（设置为 -∞）softmax
输入中与非法连接相对应的所有值来实现缩放点积注意力的内部</li>
</ul>
<h2 id="position-wise-feed-forward-networks">3.3Position-wise
Feed-Forward Networks</h2>
<p>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别且相同地应用于每个位置。
这包括两个线性变换，中间有一个 ReLU 激活。 <span class="math display">\[
FFN(x) = max(0,xW_1 + b_1 )W_2 + b_2
\]</span></p>
<blockquote>
<p>输入层 - 隐藏层 - 输出层</p>
<p>输入( n , dmodel = 512 )</p>
<p>隐藏层( n , dmodel*4 = 2048)</p>
<p>输出层（n ， dmodel = 512）</p>
</blockquote>
<p>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。
另</p>
<p>一种描述方式是内核大小为 1 的两个卷积</p>
<p>输入和输出的维度为 dmodel = 512，内层的维度为 dff = 2048</p>
<h2 id="embeddings-and-softmax">3.4 Embeddings and Softmax</h2>
<p>与其他序列转导模型类似，我们<strong>使用learned
embedding将输入标记和输出标记转换为维度 dmodel 的向量</strong></p>
<p>我们还使用通常的学习线性变换和 softmax
函数将解码器输出转换为预测的下一个token概率</p>
<p>在我们的模型中，我们在两个embedding和 pre-softmax
线性变换之间共享相同的权重矩阵</p>
<p>在embedding中，我们将这些权重乘以 <span
class="math inline">\(\sqrt{d_{model}}\)</span></p>
<h2 id="positional-encoding">3.5 Positional Encoding</h2>
<p>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于标记在序列中的相对或绝对位置的信息</p>
<p>为此，我们在输入嵌入编码器和解码器堆栈的底部中添加“位置编码”</p>
<p>位置编码与嵌入具有相同的维度 dmodel，因此可以将两者相加</p>
<p>位置编码有很多选择，学习的和固定的</p>
<p>在这项工作中，我们使用不同频率的正弦和余弦函数： <span
class="math display">\[
PE(pos,2i) = sin(pos/1000^{2i/d_{model}})
\]</span></p>
<p><span class="math display">\[
PE(pos,2i+1) = cos(pos/1000^{2i/d_{model}})
\]</span></p>
<ul>
<li>pos 是位置</li>
<li>i 是维度</li>
</ul>
<p>也就是说，位置编码的每个维度对应一个正弦曲线。</p>
<p>波长形成从 2π 到 10000 · 2π 的几何级数</p>
<p>我们选择这个函数是因为我们假设它可以让模型轻松学习通过相对位置来参与，因为对于任何固定的偏移量
k</p>
<p><span class="math inline">\(PE_{pos+k}\)</span>可以表示为 <span
class="math inline">\(PE_{pos}\)</span> 的线性函数</p>
<p>我们还尝试使用学习的位置嵌入 , 发现这两个版本产生了几乎相同的结果</p>
<p>我们选择了正弦版本，因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度</p>
<h1 id="why-self-attention">4.Why Self-Attention</h1>
<p>表 1：不同层类型的最大路径长度、每层复杂度和最小顺序操作数。 n
是序列长度，d 是表示维度，k 是卷积的核大小，r
是受限自注意中的邻域大小。</p>
<figure>
<img src="结构图片/table1.png" alt="table" />
<figcaption aria-hidden="true">table</figcaption>
</figure>
<p>在本节中，我们将自注意力层的各个方面与循环层和卷积层进行比较</p>
<p>这些层通常用于将一个可变长度的符号表示序列 (x1, ..., xn)
映射到另一个等长序列 (z1, .. ., zn)</p>
<blockquote>
<p><span class="math inline">\(x_i , z_i \in \mathbb{R}^{d}\)</span></p>
</blockquote>
<p>如典型序列转导编码器或解码器中的隐藏层。
为了激发我们对自我关注的使用，我们考虑了三个必要条件。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://example.com">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/05/03/Transformer/">http://example.com/2022/05/03/Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2022/05/04/hello-world/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Hello World</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">John Doe</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction"><span class="toc-number">2.</span> <span class="toc-text">1.Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#background"><span class="toc-number">3.</span> <span class="toc-text">2.Background</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#model-architecture"><span class="toc-number">4.</span> <span class="toc-text">3.Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#encoder-and-decoder-stacks"><span class="toc-number">4.1.</span> <span class="toc-text">3.1Encoder and Decoder Stacks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#encoder"><span class="toc-number">4.1.0.1.</span> <span class="toc-text">3.1.1Encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#decoder"><span class="toc-number">4.1.0.2.</span> <span class="toc-text">3.1.1Decoder</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#attention"><span class="toc-number">4.2.</span> <span class="toc-text">3.2Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#scaled-dot-product-attention"><span class="toc-number">4.2.0.1.</span> <span class="toc-text">3.2.1 Scaled Dot-Product
Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-head-attention"><span class="toc-number">4.2.0.2.</span> <span class="toc-text">3.2.2 Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#applications-of-attention-in-our-model"><span class="toc-number">4.2.0.3.</span> <span class="toc-text">3.2.3 Applications of
Attention in our Model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#position-wise-feed-forward-networks"><span class="toc-number">4.3.</span> <span class="toc-text">3.3Position-wise
Feed-Forward Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-and-softmax"><span class="toc-number">4.4.</span> <span class="toc-text">3.4 Embeddings and Softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#positional-encoding"><span class="toc-number">4.5.</span> <span class="toc-text">3.5 Positional Encoding</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#why-self-attention"><span class="toc-number">5.</span> <span class="toc-text">4.Why Self-Attention</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/05/04/%E9%A6%96%E9%A1%B5/" title="No title"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/2022/05/04/%E9%A6%96%E9%A1%B5/" title="No title">No title</a><time datetime="2022-05-03T16:39:27.162Z" title="Created 2022-05-04 00:39:27">2022-05-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/04/hello-world/" title="Hello World"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2022/05/04/hello-world/" title="Hello World">Hello World</a><time datetime="2022-05-03T16:06:22.921Z" title="Created 2022-05-04 00:06:22">2022-05-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/03/Transformer/" title="Transformer"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer"/></a><div class="content"><a class="title" href="/2022/05/03/Transformer/" title="Transformer">Transformer</a><time datetime="2022-05-03T09:11:37.977Z" title="Created 2022-05-03 17:11:37">2022-05-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By John Doe</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>