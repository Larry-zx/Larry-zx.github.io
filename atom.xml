<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扁同学不发言的个人博客</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-04T04:52:38.677Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>扁同学不发言</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>顺序表</title>
    <link href="http://example.com/2022/05/04/%E9%A1%BA%E5%BA%8F%E8%A1%A8/"/>
    <id>http://example.com/2022/05/04/%E9%A1%BA%E5%BA%8F%E8%A1%A8/</id>
    <published>2022-05-04T03:37:36.000Z</published>
    <updated>2022-05-04T04:52:38.677Z</updated>
    
    <content type="html"><![CDATA[<h1 id="顺序表">顺序表</h1><h3 id="源代码清单">1.1源代码清单:</h3><ul><li><h5 id="sqlist.cpp">SqList.cpp</h5></li><li><h5 id="sqlist.h">SqList.h</h5></li></ul><h3 id="顺序表数据结构">1.2顺序表数据结构:</h3><blockquote><p>本程序元素类型为int,读者可根据需求自行更改数据类型。</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//线性表结构体</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">int</span>* elem; <span class="comment">//线性表数据</span></span><br><span class="line">    <span class="type">int</span> length; <span class="comment">//线性表长度</span></span><br><span class="line">    <span class="type">int</span> listSize; <span class="comment">//线性表大小</span></span><br><span class="line">&#125;SqList;</span><br></pre></td></tr></table></figure><h3 id="该顺序表设计到的方法">1.3该顺序表设计到的方法:</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span>  <span class="title">InitList</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//初始化线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">DestroyList</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//销毁线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">ClearList</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//置空线性表</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ListEmpty</span><span class="params">(SqList L)</span></span>; <span class="comment">//判断是否为空</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">ListLength</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//线性表长度</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">GetElem</span><span class="params">(SqList L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>; <span class="comment">//获取值</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Compare</span><span class="params">(<span class="type">int</span> a , <span class="type">int</span> b)</span></span>; <span class="comment">//比较</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">LocateElem</span><span class="params">(SqList L, <span class="type">int</span> e, <span class="type">bool</span>(Compare)(<span class="type">int</span>, <span class="type">int</span>))</span></span>; <span class="comment">//定位</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">PriorElem</span><span class="params">(SqList L, <span class="type">int</span> cur_e , <span class="type">int</span> &amp;pre_e)</span></span>; <span class="comment">//前驱</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">NestElem</span><span class="params">(SqList L , <span class="type">int</span> cur_e , <span class="type">int</span> &amp;next_e)</span></span>; <span class="comment">//后继</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">InsertList</span><span class="params">(SqList &amp;L,<span class="type">int</span> i ,<span class="type">int</span> elem)</span></span>; <span class="comment">//插入</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">ListDelete</span><span class="params">(SqList &amp;L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>; <span class="comment">//删除</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Visit</span><span class="params">(<span class="type">int</span> x)</span></span>; <span class="comment">//访问</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">visitList</span><span class="params">(SqList L,<span class="type">void</span>(Visit)(<span class="type">int</span>))</span></span>; <span class="comment">//输出顺序表</span></span><br></pre></td></tr></table></figure><h3 id="sqlist.h-1">1.4 SqList.h</h3><ul><li><h4 id="宏定义">宏定义</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">/* 状态码 */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> TRUE        1   <span class="comment">// 真/是</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FALSE       0   <span class="comment">// 假/否</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OK          1   <span class="comment">// 通过/成功</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ERROR       0   <span class="comment">// 错误/失败</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//系统中已有此状态码定义，要防止冲突</span></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> OVERFLOW</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OVERFLOW    -2  <span class="comment">//堆栈上溢</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//线性表的宏定义</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> listInitSize 20</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> listSizeStride 10 <span class="comment">//空间增加的步长</span></span></span><br></pre></td></tr></table></figure><ul><li><h4 id="初始化表">初始化表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">InitList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="comment">//给线性表数据部分分配空间</span></span><br><span class="line">    L.elem = (<span class="type">int</span>*)<span class="built_in">malloc</span>(listInitSize * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    <span class="comment">//分配空间失败</span></span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="comment">// 存储内存失败</span></span><br><span class="line">        <span class="built_in">exit</span>(OVERFLOW);</span><br><span class="line">    <span class="comment">//线性表长度</span></span><br><span class="line">    L.length=<span class="number">0</span>;</span><br><span class="line">    <span class="comment">//线性表大小</span></span><br><span class="line">    L.listSize=listInitSize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="销毁线性表">销毁线性表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//销毁线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">DestroyList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="comment">//确保线性表存在</span></span><br><span class="line">    <span class="keyword">if</span>( L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放内存</span></span><br><span class="line">    <span class="built_in">free</span>(L.elem);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//置空指针</span></span><br><span class="line">    L.elem = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//长度内存的改变</span></span><br><span class="line">    L.length =<span class="number">0</span>;</span><br><span class="line">    L.listSize=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="置空线性表">置空线性表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ClearList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    L.length=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="判断是否为空">判断是否为空</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span>  <span class="title">ListEmpty</span><span class="params">(SqList L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> L.length==<span class="number">0</span>?TRUE:ERROR;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="线性表长度">线性表长度</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListLength</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> L.length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="获取值">获取值</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">GetElem</span><span class="params">(SqList L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    e = L.elem[i];</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="比较">比较</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Compare</span><span class="params">(<span class="type">int</span> a , <span class="type">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a==b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="定位">定位</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">LocateElem</span><span class="params">(SqList L, <span class="type">int</span> e, <span class="type">bool</span>(Compare)(<span class="type">int</span>, <span class="type">int</span>))</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//下标</span></span><br><span class="line">    <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="comment">//新建指针指向数据域</span></span><br><span class="line">    <span class="type">int</span>* p = L.elem;</span><br><span class="line">    <span class="comment">//遍历</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;L.length &amp;&amp; !<span class="built_in">Compare</span>(*p++,e))&#123;</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;L.length)</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;<span class="comment">//没找到</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="前驱">前驱</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span>  <span class="title">PriorElem</span><span class="params">(SqList L, <span class="type">int</span> cur_e , <span class="type">int</span> &amp;pre_e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> *p = L.elem;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;L.length &amp;&amp; !<span class="built_in">Compare</span>(*p++, cur_e))&#123;</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>||i&gt;=L.length)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        pre_e = L.elem[i<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="后继">后继</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">NestElem</span><span class="params">(SqList L , <span class="type">int</span> cur_e , <span class="type">int</span> &amp;next_e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> *p = L.elem;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;L.length &amp;&amp; !<span class="built_in">Compare</span>(*p++, cur_e))&#123;</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i&gt;=L.length<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        next_e = L.elem[i+<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="插入">插入</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">InsertList</span><span class="params">(SqList &amp;L,<span class="type">int</span> i ,<span class="type">int</span> elem)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//插入位置是否正确</span></span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">0</span> || i&gt;L.length)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* newbase;<span class="comment">//预计新建空间</span></span><br><span class="line">    <span class="comment">//是否空间不足</span></span><br><span class="line">    <span class="keyword">if</span>(L.length&gt;=L.listSize)&#123;</span><br><span class="line">        newbase = (<span class="type">int</span>*) <span class="built_in">realloc</span>(L.elem, (L.listSize+listSizeStride)*<span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">        <span class="comment">//创建是否成功</span></span><br><span class="line">        <span class="keyword">if</span>(newbase==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="built_in">exit</span>(OVERFLOW);</span><br><span class="line">        L.elem = newbase;</span><br><span class="line">        L.listSize += listSizeStride;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>*q = &amp;L.elem[i]; <span class="comment">//q指向要插入的位置</span></span><br><span class="line">    <span class="type">int</span>*p = &amp;L.elem[L.length<span class="number">-1</span>]; <span class="comment">//p指向数据尾部位</span></span><br><span class="line">    <span class="comment">//右移元素</span></span><br><span class="line">    <span class="keyword">while</span>(p&gt;=q)&#123;</span><br><span class="line">        *(p+<span class="number">1</span>) = *p;</span><br><span class="line">        p--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//插入元素</span></span><br><span class="line">    *q = elem;</span><br><span class="line">    <span class="comment">//表长加1</span></span><br><span class="line">    L.length++;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="删除">删除</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListDelete</span><span class="params">(SqList &amp;L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//删除位置是否正确</span></span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">0</span> || i&gt;L.length)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> *q = &amp;L.elem[i] ;<span class="comment">//指向要被删除的位置</span></span><br><span class="line">    <span class="type">int</span> *p = &amp;L.elem[L.length<span class="number">-1</span>]; <span class="comment">// 指向表尾元素</span></span><br><span class="line">    <span class="comment">//要删除的元素赋值给e</span></span><br><span class="line">    e = *q;</span><br><span class="line">    <span class="keyword">while</span>(q&lt;p)&#123;</span><br><span class="line">        *q = *(q+<span class="number">1</span>);</span><br><span class="line">        q++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//表长减1</span></span><br><span class="line">    L.length--;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="访问元素">访问元素</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Visit</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    cout&lt;&lt;x&lt;&lt;<span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="输出顺序表">输出顺序表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">visitList</span><span class="params">(SqList L,<span class="type">void</span>(Visit)(<span class="type">int</span>))</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;数据为空&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;L.length;i++)</span><br><span class="line">        <span class="built_in">Visit</span>(L.elem[i]);</span><br><span class="line">    cout&lt;&lt;<span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="sqlist.cpp-1">1.5 SqList.cpp</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;SqList.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">const</span> <span class="type">char</span> * argv[])</span> </span>&#123;</span><br><span class="line">    SqList myList;</span><br><span class="line">    <span class="built_in">InitList</span>(myList);</span><br><span class="line">    <span class="type">int</span> num;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入数据数量:&quot;</span>&lt;&lt;endl;</span><br><span class="line">    cin&gt;&gt;num;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入数据:&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> elem;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span> ; i&lt;num;i++)&#123;</span><br><span class="line">        cin&gt;&gt;elem;</span><br><span class="line">        <span class="built_in">InsertList</span>(myList, i, elem);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">visitList</span>(myList, Visit);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;长度是:&quot;</span>&lt;&lt;<span class="built_in">ListLength</span>(myList);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;是否为空:&quot;</span>&lt;&lt;<span class="built_in">ListEmpty</span>(myList);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入一个想定位的数据:&quot;</span>;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    cin&gt;&gt;data;</span><br><span class="line">    cout&lt;&lt;data&lt;&lt;<span class="string">&quot;定位是:&quot;</span>&lt;&lt;<span class="built_in">LocateElem</span>(myList, data, Compare)&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> pre_data;</span><br><span class="line">    <span class="built_in">PriorElem</span>(myList, data, pre_data);</span><br><span class="line">    <span class="type">int</span> after_data;</span><br><span class="line">    <span class="built_in">NestElem</span>(myList, data, after_data);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;前驱是:&quot;</span>&lt;&lt;pre_data&lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt;<span class="string">&quot;后继是:&quot;</span>&lt;&lt;after_data&lt;&lt;<span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;删除元素&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> deldata;</span><br><span class="line">    <span class="built_in">ListDelete</span>(myList, <span class="number">3</span>, deldata);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;删除的元素是:&quot;</span>&lt;&lt;deldata&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">visitList</span>(myList, Visit);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;顺序表&quot;&gt;顺序表&lt;/h1&gt;
&lt;h3 id=&quot;源代码清单&quot;&gt;1.1源代码清单:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h5 id=&quot;sqlist.cpp&quot;&gt;SqList.cpp&lt;/h5&gt;&lt;/li&gt;
&lt;li&gt;&lt;h5 id=&quot;sqlist.h&quot;&gt;SqList.h&lt;/h5&gt;&lt;/l</summary>
      
    
    
    
    <category term="数据结构" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="C++" scheme="http://example.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>MASA-SR翻译</title>
    <link href="http://example.com/2022/05/04/MASA-SR/"/>
    <id>http://example.com/2022/05/04/MASA-SR/</id>
    <published>2022-05-03T18:58:52.000Z</published>
    <updated>2022-05-04T04:52:17.610Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" /></p><h2 id="主要组成">主要组成</h2><ul><li><p>编码器 Encoder</p></li><li><p>匹配与提取模块（MEM） Math &amp; Extraction Modules</p></li><li><p>空间自适应模块（SAM） Spatial Adaptation Modules</p><blockquote><p>将Ref特征的分布映射到LR特征的分布</p></blockquote></li><li><p>双残差聚合模块（DRAM）Dual Residual Aggregation Modules</p><blockquote><p>进行有效的特征融合</p></blockquote></li></ul><h3 id="名词解释">名词解释</h3><blockquote><ul><li>LR 低分辨率图像</li><li>Ref↓ 表示 x4双三次下采样参考图</li><li>Ref 参考图</li></ul></blockquote><h2 id="编码器">编码器</h2><ul><li><p>与之前使用预先训练过的VGG作为自然提取器的方法不同</p></li><li><p>这里的编码器与网络的其他部分一起从头开始训练的</p></li><li><p>编码器含有三个构造块</p><ul><li>第二个和第三个 利用stride=2的方法将featue map大小折半</li></ul></li><li><p>将Ref参考图传入编码器分别经过三个构造块得到三个不同比例的特征</p><ul><li>生成<span class="math inline">\(F_{Ref}^s\)</span> 其中s =1,2,4</li></ul></li><li><p>LR图像和Ref↓ 图像只经管编码器的第一个构造块</p><ul><li>生成<span class="math inline">\(F_{LR}\)</span> 和 <spanclass="math inline">\(F_{Ref↓ }\)</span></li></ul></li></ul><h2 id="matching-extraction-module-mem">Matching &amp; Extraction Module(MEM)</h2><h3 id="概述">概述</h3><p>​众所周知，在自然图像的局部区域中，相邻像素可能来自公共对象共享相似的颜色统计数据。以往对自然图像先验的研究也表明，一幅图像中的相邻斑块很可能会发现它们之间的对应关系在空间上是一致的。</p><p>​这促使我们提出了一种从粗到精的匹配方案，即粗块匹配和精块匹配。请注意，在我们的方法中，block和patch是两个不同的概念，block的大小大于patch（在我们的实验中patch为3×3的大小）。</p><p>​ 如图3所示，我们首先只在featurespace中找到block的对应关系。具体来说，我们将LR特征(<spanclass="math inline">\(F_{LR}\)</span>)展开为不重叠的block块。每个LRblock将找到其最相关的Ref↓ block。</p><p>​ 与以前的方法相比，通过这样做，匹配的计算成本显著降低。</p><p>​ 为了达到足够的精度，我们进一步对每一个&lt;LR block，Ref↓block&gt;对进行密集patch匹配</p><p>​ 在最后一个阶段，我们根据获得的对应信息提取有用的Ref feature</p><p><imgsrc="https://pic3.zhimg.com/80/v2-e551112dcfc05e03aecc23d0b457189e_1440w.jpg" /></p><h3 id="stage-1-coarse-matching粗匹配">Stage 1: Coarsematching（粗匹配）</h3><p>将<span class="math inline">\(F_{LR}\)</span>展开成K个不重复的blocks<span class="math display">\[\left\{  B_{LR}^0,B_{LR}^1,B_{LR}^2 ,...B_{LR}^{K-1} \right\}\]</span></p><blockquote><p>一个Block中有很多patch 对每个<spanclass="math inline">\(B_{LR}^k\)</span>块找到与它<strong>最相关</strong>的<spanclass="math inline">\(F_{Ref↓}\)</span> block 记作<spanclass="math inline">\(B_{Ref↓}^k\)</span></p></blockquote><p>首先将<span class="math inline">\(B_{LR}^k\)</span>的<strong>centerpatch</strong>与<spanclass="math inline">\(F_{Ref↓}\)</span>中的每一个patch进行计算<strong>余弦相似度</strong>(cosinesimilarity) <span class="math display">\[r_{c,j}^k = \left \langle \frac{p_c^k}{\left \| p_c^k \right \|} ,\frac{q_j}{\left \| q_j \right \|} \right \rangle\]</span></p><ul><li><span class="math inline">\(p_c^k\)</span>是<spanclass="math inline">\(B_{LR}^k\)</span>块的center patch</li><li><span class="math inline">\(q_j\)</span>是<spanclass="math inline">\(F_{Ref↓}\)</span>块的第j个patch</li><li><spanclass="math inline">\(r_{c,j}^k\)</span>表示相似性大小（similarityscores）</li></ul><p>通过<strong>相似性大小</strong>(similarity score)我们可以找到<spanclass="math inline">\(F_{Ref↓}\)</span> 中与<spanclass="math inline">\(p_c^k\)</span>最相似的patch</p><p>然后crop一个围绕着这个最相似patchb并且大小为<spanclass="math inline">\([dx,dy]\)</span>的block 记作<spanclass="math inline">\(B_{Ref↓}^k\)</span></p><blockquote><p>根据局部相干特性,每个<spanclass="math inline">\(B_{LR}^k\)</span>块的center patch都能找到与在<spanclass="math inline">\(F_{Ref↓}\)</span>其中最相似的patch后找到对应的<spanclass="math inline">\(B_{Ref↓}^k\)</span></p><p>另一方面，我们可以在<spanclass="math inline">\(F_{Ref}^s\)</span>中剪切相应的大小为<spanclass="math inline">\([s*dx,s*dy]\)</span>的block，记作<spanclass="math inline">\(B_{Ref}^{s,k}\)</span></p><p>这将用于<strong>特征提取阶段</strong></p></blockquote><p>注意问题⚠️</p><ul><li>如果<spanclass="math inline">\(B_{LR}^k\)</span>的大小远大于其centerpatch的大小，则center patch可能无法代表<spanclass="math inline">\(B_{LR}^k\)</span>的全部内容</li><li>这可能会误导我们找到不相关的<spanclass="math inline">\(B_{Ref↓}^k\)</span></li></ul><p>解决方法：</p><ul><li>使用具有<strong>不同膨胀率</strong>的中心块来计算相似度</li><li>细节如图3的第1阶段所示，<ul><li>其中蓝色虚线表示<em>dilation</em>=1的情况</li><li>橙色虚线表示<em>dilation</em>=2的情况</li><li>然后将相似性得分计算为<strong>不同扩张的结果之和</strong></li></ul></li></ul><blockquote><p>这个阶段我们得到了（<spanclass="math inline">\(B_{LR}^k\)</span>，<spanclass="math inline">\(B_{Ref↓}^k\)</span>，<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>）</p><p>在下面的精细匹配阶段阶段我们将限制<spanclass="math inline">\(B_{LR}^k\)</span> to <spanclass="math inline">\(B_{Ref↓}^k\)</span> 的搜索空间</p></blockquote><h3 id="stage-2-fine-matching精细匹配阶段阶段">Stage 2: Finematching（精细匹配阶段阶段）</h3><blockquote><p>对<span class="math inline">\(B_{LR}^k\)</span> to <spanclass="math inline">\(B_{Ref↓}^k\)</span>进行dense pathmatching（密集补丁匹配）</p><p>得到index maps集合 <span class="math inline">\(\left\{ D^0 , D^1 ,... D^{K-1} \right\}\)</span></p><p>similarity maps 集合<span class="math inline">\(\left\{ R^0 , R^1 ,... R^{K-1} \right\}\)</span></p></blockquote><p>拿第k对(<span class="math inline">\(B_{LR}^k\)</span> , <spanclass="math inline">\(B_{Ref↓}^k\)</span>)为例 我们计算每个patch in<span class="math inline">\(B_{LR}^k\)</span> 与 每个patch in <spanclass="math inline">\(B_{Ref↓}^k\)</span>之间的相似性分数 <spanclass="math display">\[r_{i,j}^k = \left \langle \frac{p_i^k}{\left \| p_i^k \right \|} ,\frac{q_j^k}{\left \| q_j^k \right \|} \right \rangle\]</span></p><ul><li><span class="math inline">\(p_i^k\)</span>是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch</li><li><span class="math inline">\(q_j^k\)</span>是<spanclass="math inline">\(B_{Ref↓}^k\)</span>的第j个patch</li><li>k表示第k对(<span class="math inline">\(B_{LR}^k\)</span> , <spanclass="math inline">\(B_{Ref↓}^k\)</span>)</li><li>$r_{i,j}^k $表示它们的相似性大小</li></ul><blockquote><p><span class="math inline">\(D^k\)</span>的第i个元素的值j表示的是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch与<spanclass="math inline">\(B_{Ref↓}^k\)</span>中第j个ptach最相似</p></blockquote><p><span class="math display">\[D_i^k = \mathop{\arg \max}_j \ r_{i,j}^k\]</span></p><blockquote><p><span class="math inline">\(R^k\)</span>的第i个元素表示的是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch相对应的highestsimilarity score(最高相似分数)</p></blockquote><p><span class="math display">\[R_i^k = \mathop{\max}_j \ r_{i,j}^k\]</span></p><h3 id="stage-3-feature-extraction特征提取">Stage 3: Featureextraction(特征提取)</h3><p>根据index map <span class="math inline">\(D^k\)</span> 从<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>中提取patches，生成新的featuremap <span class="math inline">\(B_M^{s,k}\)</span></p><p>更精准的说</p><p><span class="math inline">\(D_i^k\)</span>表示<spanclass="math inline">\(B_{Ref↓}^k\)</span>中与<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch最相似的patch</p><p>我们将<span class="math inline">\(D_i^k\)</span>个<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>中的patch作为<spanclass="math inline">\(B_M^{s,k}\)</span>的第i个patch</p><p>此外，由于相似性<strong>分数较高的Ref特征更有用</strong></p><p>我们将<span class="math inline">\(B_M^{s,k}\)</span>与相应的<spanclass="math inline">\(R^k\)</span>相乘获得<strong>加权特征块</strong><span class="math display">\[B_M^{s,k} := B_M^{s,k}\odot(R^k)\uparrow\]</span></p><ul><li><span class="math inline">\(()\uparrow\)</span>表示双线性插值</li><li><span class="math inline">\(\odot\)</span> 表示element-wise mul</li></ul><p>MEM的最终结果就是将 <span class="math inline">\(\left\{ B_M^{s,0} ,B_M^{s,1} , B_M^{s,2},...B_M^{s,K-1}\right\}\)</span>进行折叠在一起获得，折叠操作是步骤一的逆向操作</p><h3 id="分析">分析</h3><h4 id="以往的配对方法">以往的配对方法</h4><ul><li><p>图像LR的像素 为 m pixels ; 图像<spanclass="math inline">\(Ref\downarrow\)</span>的像素 为 n pixels</p></li><li><p>计算复杂度为O(mn)</p></li></ul><h4 id="mem方法">MEM方法</h4><ul><li>假设<span class="math inline">\(Ref\downarrow\)</span> block 有 n’pixels</li><li>计算复杂度将被降为 O(Kn+mn’)</li><li>K远小于m</li><li>n‘ 远小于n</li><li>通过这种从粗到精的匹配方案，计算量显著降低</li></ul><h2 id="spatial-adaptation-modulesam空间自适应模块">Spatial AdaptationModule（SAM）空间自适应模块</h2><p><imgsrc="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" /></p><p>在许多情况下，LR和Ref图像可能具有相似的内容和纹理，颜色和亮度不一样</p><p>因此，提取的REF特征的分布可能与LR特征的分布不一致。因此，简单地将Ref和LR特性连接到一起，并将它们输入到下面的卷积层中并不是最佳选择。我们建议使用空间自适应模块（SAM）将提取的Ref特征的分布重新映射到LR特征的分布</p><p>首先将LR特征和提取的Ref特征连接(Cat)起来，然后预先送入卷积层，以产生两个参数β和γ，这两个参数的大小与LR特征相同。</p><p>我们用特征的平均值和标准偏差更新β和γ <span class="math display">\[\beta \leftarrow \beta + \mu_{LR}\\\gamma \leftarrow \gamma +\sigma_{LR}\]</span></p><blockquote><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span>的产生方式跟<spanclass="math inline">\(\mu_{Ref}\)</span> <spanclass="math inline">\(\sigma_{Ref}\)</span> 一样</p><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span> 的大小是 C x 1 x 1</p><p>C表示一共有C个通道</p></blockquote><p>然后将实例规范化(Instance Norm)应用于Ref特征，如下所示： <spanclass="math display">\[F_{Ref}^c \leftarrow \frac{F_{Ref}^c - \mu_{Ref}^c}{\sigma_{Ref}^c}\]</span> <span class="math inline">\(\mu_{Ref}^c\)</span> <spanclass="math inline">\(\sigma_{Ref}^c\)</span>分别表示Ref特征图在通道c的均值和方差 <span class="math display">\[\mu_{Ref}^c = \frac{1}{HW} \sum_{y,x}F_{Ref}^{c,y,x}\]</span></p><p><span class="math display">\[\sigma_{Ref}^c  = \sqrt{\frac{1}{HW}\sum_{y,x}(F_{Ref}^{c,y,x} -\mu_{ref}^c)^2}\]</span></p><p>最后，将γ和β添加到归一化Ref特征中，如下所示： <spanclass="math display">\[F_{Ref} \leftarrow F_{Ref}·\gamma + \beta\]</span> 由于Ref特征和LR特征之间的差异随空间位置而变化</p><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span> <spanclass="math inline">\(\mu_{Ref}\)</span> <spanclass="math inline">\(\sigma_{Ref}\)</span> 的大小为 C x 1 x 1</p><p>我们使用可学习卷积来预测两个空间参数β和γ</p><p>不同于只使用分割图(segmentation)去生成两个参数，SAM中的卷积将Ref和LR特征作为输入，以了解它们的差异。此外，在从卷积中获得β和γ后，我们将它们与LR特征的均值和标准偏差相加</p><h2 id="dual-residual-aggregation-module-dram-双残差聚合模块">DualResidual Aggregation Module (DRAM) 双残差聚合模块</h2><p>在空间自适应后，使用我们提出的双剩余聚合模块（DRAM），将传输的Ref特征与LR特征融合</p><p>DRAM由两个分支组成，即 <strong>LR分支</strong> 和<strong>Ref分支</strong></p><p><strong>Ref分支</strong></p><blockquote><p>旨在细化Ref功能的高频细节</p></blockquote><ul><li>首先使用stride=2的卷积对Ref Feature 进行下采样</li><li>将下采样后的Ref Feature 减去 LR Feature 得到Res_Ref 残余特征</li><li>将Res_Ref使用转置卷积（逆卷积）上采样后加上原始的RefFeatu得到新的Ref ‘ 特征图</li></ul><p><span class="math display">\[\begin{cases}    Res_{Ref} = Conv(F_{Ref}) - F_{LR}\\    F&#39;_{Ref}= F_{Ref} + DeConv(Res_{Ref})\end{cases}\]</span></p><p><strong>LR分支</strong></p><blockquote><p>LR功能的高频细节细化</p></blockquote><ul><li>首先使用stride=2的卷积对Ref Feature 进行下采样</li><li>将 <strong>LR Feature</strong> 减去<strong>下采样后的RefFeature</strong> 得到Res_LR 残余特征</li><li>将Res_LR 加上 LR Feature 相加后 进行上采样的懂新的LR’ 特征图</li></ul><p><span class="math display">\[\begin{cases}    Res_{LR} =  F_{LR}-Conv(F_{Ref})\\    F&#39;_{LR}= DeConv(F_{LR}+Res_{LR})\end{cases}\]</span></p><p>最后，将两个分支的输出串联起来，并以步长1通过另一个卷积层。</p><p>通过这种方式，LR和Ref功能中的细节得到了增强和聚合，从而产生了更具代表性的功能。</p><h2 id="loss-functions">Loss Functions</h2><h3 id="reconstruction-loss重建损失">1.Reconstructionloss(重建损失)</h3><p>采用L1损失作为重建损失</p><p><span class="math display">\[\mathcal{L}_{rec} = \left \|  I_{HR} - I_{SR}\right\|_1\]</span></p><h3 id="perceptual-loss知觉损失">2.Perceptual loss(知觉损失)</h3><p>知觉损失的表达为 <span class="math display">\[\mathcal{L}_{per} = \left \|  \phi_i(I_{HR}) - \phi_i(I_{SR}) \right\|_2\]</span></p><blockquote><p><span class="math inline">\(\phi_i\)</span>表示VGG19的第i层这里是用conv5_4</p></blockquote><h3 id="adversarial-loss对抗性损失">3.Adversarial loss(对抗性损失)</h3><p>它可以有效地生成具有自然细节的视觉愉悦图像 <spanclass="math display">\[\mathcal{L}_D =-\mathbb{E}_{I_{HR}}[\log(D(I_{HR},I_{SR}))]-\mathbb{E}_{I_{SR}}[\log(1-D(I_{HR},I_{SR}))],\\\mathcal{L}_G=-\mathbb{E}_{I_{HR}}[\log(1-D(I_{HR},I_{SR}))]-\mathbb{E}_{I_{SR}}[\log(D(I_{SR},I_{HR}))]\]</span></p><h3 id="full-objective">4.Full objective</h3><p><span class="math display">\[\mathcal{L} =\lambda_{rec}\mathcal{L}_{rec}+\lambda_{per}\mathcal{L}_{per}+\lambda_{adv}\mathcal{L_{adv}}\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;主要组成&quot;&gt;主要组成&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;</summary>
      
    
    
    
    <category term="cs.CV" scheme="http://example.com/categories/cs-CV/"/>
    
    
    <category term="超分重建" scheme="http://example.com/tags/%E8%B6%85%E5%88%86%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
    <link href="http://example.com/2022/05/04/%E8%A7%A3%E8%AF%BB/"/>
    <id>http://example.com/2022/05/04/%E8%A7%A3%E8%AF%BB/</id>
    <published>2022-05-03T18:56:42.000Z</published>
    <updated>2022-05-04T04:52:11.644Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络模型">1.网络模型</h2><p><imgsrc="https://pic4.zhimg.com/v2-8e9936fdcfd4e8371720b9834f8f97d7_r.jpg" /></p><h3 id="组成部分">1.1 组成部分</h3><blockquote><p>网络模型总体分为两部分:Image Transform Net和VGG-16</p></blockquote><ul><li><strong>Image Transform Net</strong>是参数W待训练的网络</li><li><strong>VGG-16</strong>是已经预训练好参数的网络</li></ul><h3 id="工作原理">1.2 工作原理</h3><p><strong>(1) 输入</strong>为 :</p><ul><li>原始图像<span class="math inline">\(x\)</span></li><li>风格目标图<span class="math inline">\(y_s\)</span></li><li>内容目标图<span class="math inline">\(y_c\)</span></li></ul><p><strong>(2) Image Transform Net</strong>作用：</p><ul><li>将原始图像<span class="math inline">\(x\)</span>经过<strong>ImageTransform Net</strong>得到输出图像<spanclass="math inline">\(\hat{y}\)</span></li><li>映射关系为: <span class="math inline">\(\hat{y} =f_W(x)\)</span></li><li>其中W是Images Transform Net的参数 ， x是网络输入，y是网络输出。</li></ul><p><strong>(3) VGG-16</strong>作用：</p><ul><li><p>内容层面</p><blockquote><p>将<span class="math inline">\(\hat{y}\)</span> 与<spanclass="math inline">\(y_c\)</span>在VGG中间层的欧式距离作为Loss训练<strong>图像转换网络</strong></p><p>使得Image Transform Net输出的<spanclass="math inline">\(\hat{y}\)</span>与目标内容图<spanclass="math inline">\(y_c\)</span>越来越接近</p></blockquote></li><li><p>风格层面</p><blockquote><p>将<span class="math inline">\(\hat{y}\)</span> 与<spanclass="math inline">\(y_s\)</span>在VGG多个中间层得到的featuremap生成的Gram矩阵的欧式距离加权和作为Loss训练<strong>图像转换网络</strong></p><p>使得Image Transform Net输出的<spanclass="math inline">\(\hat{y}\)</span>与目标风格图<spanclass="math inline">\(y_s\)</span>越来越接近</p></blockquote></li></ul><h2 id="损失函数">2.损失函数</h2><h3 id="特征内容损失feature-reconstruction-loss">2.1特征内容损失(FeatureReconstruction Loss)</h3><p><span class="math display">\[\ell_{feat}^{\phi , j}(\hat{y},y) = \frac{1}{C_jH_jW_j}\Vert\phi_j(\hat{y})-\phi_j(y)\Vert_2\]</span></p><ul><li>j 表示VGG-16中间层代号</li><li>y表示特征目标图像</li><li><span class="math inline">\(\hat{y}\)</span>表示image transform net输出的图像</li><li><span class="math inline">\(\phi_{j}(y)\)</span>表示图像y在VGG-16中间层j时的输出</li><li><span class="math inline">\(\phi_{j}(\hat{y})\)</span> 表示图像<spanclass="math inline">\(\hat{y}\)</span>在VGG-16中间层j时的输出</li><li><spanclass="math inline">\(C_jH_jW_j\)</span>分别表示在VGG-16中间层j时的通道数、高度、宽度</li></ul><blockquote><p><strong>Feature Reconstruction Loss</strong>这数学公式就可以理解为两个图像在VGG-16中间层j的欧氏距离</p><p>越小说明VGG-16网络认为这两张图越接近</p></blockquote><h3 id="风格损失style-reconstruction-loss">2.2风格损失(<strong>StyleReconstruction Loss</strong>)</h3><ul><li><strong>Gram特征矩阵中的元素</strong> <span class="math display">\[G_{j}^{\phi}(x)_{c,c^{&#39;}} = \frac{1}{C_jH_jW_j} \sum_{h=1}^{H_j}\sum_{w=1}^{W_j}\phi_j(x)_{h,w,c}\phi_{j}(x)_{h,w,c^{&#39;}}\]</span></li></ul><blockquote><p>VGG中间层j的<strong>feature map</strong>大小为[<spanclass="math inline">\(C_j\)</span>,<spanclass="math inline">\(H_j\)</span>,<spanclass="math inline">\(W_j\)</span>]</p><p>我们经过<strong>flatten</strong>和<strong>矩阵转置</strong>操作可以变形为[<spanclass="math inline">\(C_j\)</span> , <spanclass="math inline">\(H_j*W_j\)</span>]和的[<spanclass="math inline">\(H_j*W_j\)</span> , <spanclass="math inline">\(C_j\)</span>]矩阵</p><p>再对两个作<strong>内积</strong>得到Gram Matrices大小为[<spanclass="math inline">\(C_j,C_j\)</span>]</p></blockquote><ul><li><strong>中间层j的风格损失</strong></li></ul><p><span class="math display">\[\ell_{style}^{\phi,j}(\hat{y},y) =\Vert G_j^{\phi}(\hat{y}) -G_j^{\phi}(y)\Vert_{F}^{2}\]</span></p><blockquote><p>计算图像<span class="math inline">\(y\)</span>和图像<spanclass="math inline">\(\hat{y}\)</span>两者VGG-16中间层j中gram矩阵距离的平方和</p></blockquote><h3 id="简单损失函数">2.3简单损失函数</h3><ul><li><p><strong>像素损失</strong></p><p>像素损失是输出图和目标图之间标准化的差距 <spanclass="math display">\[\ell_{pixel}({\hat{y},y}) = \frac{1}{CHW}\Vert \hat{y}-y\Vert_2^2\]</span></p></li><li><p><strong>全变差正则化</strong></p></li></ul><p>​为使得输出图像比较平滑，遵循了前人在特征反演上的研究，在超分辨率重建上使用了全变差正则化<spanclass="math inline">\(\ell_{TV}(\hat{y})\)</span></p><h2 id="image-transform-net细节">3.Image Transform Net细节</h2><h3 id="风格迁移">3.1 风格迁移</h3><table><thead><tr class="header"><th style="text-align: center;">Layer</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">9x9 conv , stride=2</td></tr><tr class="even"><td style="text-align: center;">3x3 conv , stride=2</td></tr><tr class="odd"><td style="text-align: center;">Residual blocks</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">...</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">3x3 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr><tr class="even"><td style="text-align: center;">9x9 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr></tbody></table><p><strong>具体解释</strong></p><blockquote><p>1.输入<span class="math inline">\(x\)</span> 大小为3x256x256</p><p>2.使用2层 stride=2 的卷积层进行<strong>下采样</strong></p><p>3.使用5个残差模块</p><p>4.使用2层stride=<spanclass="math inline">\(\frac{1}{2}\)</span>的卷积层进行<strong>上采样</strong></p><p>5.输出<span class="math inline">\(\hat{y}\)</span>大小为3x256x256</p></blockquote><p><strong>输入图像与输出图像大小相同先下采样再上采样的好处</strong></p><ul><li><strong><em>可计算复杂性</em></strong><ul><li>比较</li><li>3x3的C个卷积核 在CxHxW的图像上 需要 <spanclass="math inline">\(9C^2HW\)</span></li><li>3x3的DC个卷积核 在DC x <spanclass="math inline">\(\frac{H}{D}\)</span>x<spanclass="math inline">\(\frac{W}{D}\)</span> 的图像上 也需要<spanclass="math inline">\(9C^2HW\)</span></li><li>在下采样之后，我们可以使用一个<strong>更大的网络来获得相同的计算成本</strong></li></ul></li><li><strong><em>有效的感受野大小</em></strong><ul><li>优势就在于在输出中的每个像素都有输入中的<strong>大面积有效的感受野</strong></li><li>一个附加的3x3卷积层都能把感受野的大小<strong>增加2倍</strong></li><li>在用因子D进行下采样后，每个3x3的卷积增加<strong>感受野的大小到2D</strong></li><li>下采样使得相同数量的层<strong>给出了更大的感受野大小</strong></li></ul></li></ul><h3 id="超分辨率">3.2超分辨率</h3><p>假设<strong>上采样因子</strong>为<spanclass="math inline">\(f\)</span></p><table><thead><tr class="header"><th style="text-align: center;">Layer</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Residual blocks</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">...</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">3x3 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr><tr class="even"><td style="text-align: center;">3x3 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr><tr class="odd"><td style="text-align: center;">(<spanclass="math inline">\(一共使用\log_2{f}个conv\)</span>)</td></tr><tr class="even"><td style="text-align: center;">9x9 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr></tbody></table><p><strong>具体解释</strong></p><blockquote><p>1.输入<span class="math inline">\(x\)</span> 大小为3 x <spanclass="math inline">\(\frac{288}{f}\)</span> x <spanclass="math inline">\(\frac{288}{f}\)</span></p><p>2.使用 <strong>5</strong>个残差模块</p><p>3.使用_2{f}个stride=<spanclass="math inline">\(\frac{1}{2}\)</span>的卷积层进行<strong>上采样</strong></p><p>5.输出<span class="math inline">\(\hat{y}\)</span>大小为3x288x288</p></blockquote><h3 id="残差连接">3.3 残差连接</h3><p><imgsrc="https://pic2.zhimg.com/80/v2-c7677c713fa2df00682e864f41d581e9_720w.png" /></p><h3 id="其他细节">3.4其他细节</h3><ul><li>除开<strong>第一个和最后一个</strong>层用<strong>9x9</strong>的kernel其他所有卷积层都用<strong>3x3</strong>的kernels</li><li>优化方法选的是SGD（随机梯度下降法）</li><li>除去最后一层卷积层后连接Tanh激活层，其他非残差卷积层都连接BatchNorm归一层和ReLu激活层</li><li>上面的做法可以使得输出图像的像素值在 [0<em>,</em> 255]这个范围</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;网络模型&quot;&gt;1.网络模型&lt;/h2&gt;
&lt;p&gt;&lt;img
src=&quot;https://pic4.zhimg.com/v2-8e9936fdcfd4e8371720b9834f8f97d7_r.jpg&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;组成部分&quot;&gt;1.1 组成部分&lt;/h3&gt;
</summary>
      
    
    
    
    <category term="cs.CV" scheme="http://example.com/categories/cs-CV/"/>
    
    
    <category term="风格迁移" scheme="http://example.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>图像风格迁移鼻祖</title>
    <link href="http://example.com/2022/05/04/%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    <id>http://example.com/2022/05/04/%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/</id>
    <published>2022-05-03T18:54:56.000Z</published>
    <updated>2022-05-04T04:52:04.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像风格迁移">图像风格迁移</h1><figure><img src="fig1.png" alt="网络模型" /><figcaption aria-hidden="true">网络模型</figcaption></figure><h2 id="损失函数组成">损失函数组成</h2><blockquote><h4 id="loss-w1-lc-w2-ls">Loss = w1 * Lc + w2 * Ls</h4></blockquote><ul><li><h3 id="loss-of-contentlc"><strong>Loss ofcontent(Lc)</strong></h3></li><li><h3 id="loss-of-stylels"><strong>Loss ofstyle(Ls)</strong></h3></li></ul><h2 id="loss-of-content">Loss of content</h2><blockquote><p>内容图和随机噪声图经过多次卷积滤波后，conten和noise在第4层的featuremap的距离的平方和</p></blockquote><figure><img src="fig2.png" alt="Lc" /><figcaption aria-hidden="true">Lc</figcaption></figure><h2 id="loss-of-style">Loss of style</h2><blockquote><p>先对风格图和噪声图的每一层卷积得到feature map</p><p>对feature map求gram矩阵</p><p>计算两者gram距离的平方和</p><p>将5层的结果加权求和</p></blockquote><figure><img src="fig5.png" alt="Ls" /><figcaption aria-hidden="true">Ls</figcaption></figure><h2 id="实验图">实验图</h2><figure><img src="fig4.png" alt="卷积效果" /><figcaption aria-hidden="true">卷积效果</figcaption></figure><blockquote><p>随着卷积网络层数增加，获得的特征映射更加抽象。</p><p>上图可以看出，层数增高的时候：</p><ul><li><p>内容<strong>重构图可变化性</strong>增加，具有更大的风格变化能力。</p></li><li><p>风格随着使用的层数越多，<strong>风格迁移的稳定性越强</strong>。</p></li></ul></blockquote><h2 id="gram矩阵">Gram矩阵</h2><h3 id="定义">定义</h3><blockquote><p>n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的<strong>格拉姆矩阵<em>(Grammatrix)</em></strong>，很明显，这是一个对称矩阵。</p></blockquote><figure><img src="fig6.png" alt="gram" /><figcaption aria-hidden="true">gram</figcaption></figure><figure><img src="fig7.png" alt="Gram" /><figcaption aria-hidden="true">Gram</figcaption></figure><h3 id="计算">计算</h3><blockquote><p>输入图像的feature map为<strong>[ ch, h, w]</strong>。</p><p>我们经过<strong>flatten</strong>和<strong>矩阵转置</strong>操作</p><p>可以变形为<strong>[ ch, h*w]</strong>和<strong>[ h*w,ch]</strong>的矩阵</p><p>再对两个作<strong>内积</strong>得到Gram Matrices</p></blockquote><h3 id="理解">理解</h3><blockquote><p>格拉姆矩阵可以看做feature之间的偏心协方差矩阵（即没有减去均值的协方差矩阵）</p><p>在featuremap中，每个数字都来自于一个特定滤波器在特定位置的卷积，因此<strong>每个数字代表一个特征的强度</strong></p><p>Gram计算的实际上是<strong>两两特征之间的相关性</strong>，哪两个特征是同时出现的，哪两个是此消彼长的等等。</p><p>因为为乘法操作 两两特征同时为高 结果才高</p></blockquote><blockquote><p>格拉姆矩阵用于度量<strong>各个维度自己的特性</strong>以及<strong>各个维度之间的关系</strong></p><p>内积之后得到的多尺度矩阵中:</p><ul><li><p>对角线元素提供了<strong>不同特征图各自的信息</strong></p></li><li><p>其余元素提供了<strong>不同特征图之间的相关信息</strong>。这样一个矩阵，既能体现出有哪些特征，又能体现出不同特征间的紧密程度</p></li></ul></blockquote><blockquote><p>gram矩阵是计算每个通道 i 的feature map与每个通道 j 的featuremap的内积</p><p>gram matrix的每个值可以说是代表 <strong>I 通道的feature map与 j通道的feature map的互相关程度</strong></p></blockquote><h2 id="参考链接">参考链接</h2><ul><li>https://www.cnblogs.com/yifanrensheng/p/12862174.html</li><li>https://blog.csdn.net/weixin_40759186/article/details/87804316</li><li>https://www.cnblogs.com/subic/p/8110478.html</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图像风格迁移&quot;&gt;图像风格迁移&lt;/h1&gt;
&lt;figure&gt;
&lt;img src=&quot;fig1.png&quot; alt=&quot;网络模型&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;网络模型&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;损失</summary>
      
    
    
    
    <category term="cs.CV" scheme="http://example.com/categories/cs-CV/"/>
    
    
    <category term="风格迁移" scheme="http://example.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://example.com/2022/05/04/transformer/"/>
    <id>http://example.com/2022/05/04/transformer/</id>
    <published>2022-05-03T18:42:20.000Z</published>
    <updated>2022-05-04T04:51:53.015Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。</p><p>性能最好的模型还通过注意力机制连接编码器和解码器。</p><p>我们提出了一种新的简单网络架构 <strong>Transformer</strong></p><p><strong>它完全基于注意力机制，完全摒弃了递归和卷积</strong></p><p>对两个机器翻译任务的实验表明，这些模型在质量上更优越，同时更可并行化，并且需要的训练时间显着减少。</p><ul><li>我们的模型在 WMT 2014 英德翻译任务上达到了 28.4BLEU，比现有的最佳结果（包括合奏）提高了 2 BLEU 以上。</li><li>在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5天后，建立了一个新的单模型 state-of-the-art BLEU 得分41.0，这是最好的训练成本的一小部分 文献中的模型。</li></ul><h1 id="introduction">1.Introduction</h1><p>循环神经网络(RNN)、长短期记忆(LSTM)和门控循环神经网络，尤其是在语言建模和机器翻译等序列建模和转导问题已被牢固确立为最先进的方法此后，许多努力继续推动循环语言模型和编码器-解码器架构的界限</p><p>循环模型通常沿输入和输出序列的符号位置考虑计算。</p><p>将位置与计算时间的步骤对齐，它们生成一系列隐藏状态ht，作为先前隐藏状态 ht-1 和位置 t 的输入的函数。</p><p>这种固有的顺序性质排除了训练示例中的并行化，这在更长的序列长度下变得至关重要，因为内存限制限制了示例之间的批处理。</p><p>最近的工作通过因式分解技巧 和条件计算显着提高了计算效率，同时在后者的情况下也提高了模型性能。</p><p>然而，顺序计算的基本约束仍然存在。</p><p>注意机制已成为各种任务中引人注目的序列建模和转导模型的组成部分，允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离</p><p>然而，除了少数情况，这种注意力机制与循环网络结合使用。</p><p>在这项工作中，我们提出了<strong>Transformer，这是一种避免重复的模型架构，而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系</strong></p><p>在八个 P100 GPU 上经过短短 12 小时的训练后，Transformer可以实现更多的并行化，并且可以在翻译质量方面达到新的水平</p><h1 id="background">2.Background</h1><p>减少顺序计算的目标也构成了扩展神经 GPU 、ByteNet 和 ConvS2S的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入的隐藏表示和输出位置。</p><p>在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长</p><p>对于 ConvS2S 呈线性增长，而对于 ByteNet 则呈对数增长。</p><p>这使得学习远距离位置之间的依赖关系变得更加困难</p><p>在 Transformer中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而降低了有效分辨率，我们使用<strong>多头注意力(Multi-HeadAttention)</strong>来抵消这种影响</p><p><strong>自注意力(Self-attention)</strong>，有时称为内部注意力，是一种将单个序列的不同位置关联起来以计算序列表示的注意力机制</p><p>自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴涵和学习任务无关的句子表示</p><p><strong>端到端记忆网络(End-to-end memorynetwork)</strong>基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务中表现良好</p><p>然而，据我们所知</p><p><strong>Transformer是第一个完全依赖自注意力来计算其输入和输出表示而不使用序列对齐 RNN或卷积的转换模型</strong></p><p>在接下来的部分中，我们将描述Transformer，激发自注意力并讨论其相对于其他模型的优势</p><h1 id="model-architecture">3.Model Architecture</h1><p>大多数竞争性神经序列转导模型具有编码器-解码器结构</p><p>编码器将符号表示的输入序列 (x1, ..., xn) 映射到连续表示的序列 z =(z1, ..., zn)</p><blockquote><p>其中z1是一个向量 用一个向量来表示x1</p></blockquote><p>给定 z，解码器然后一次生成一个元素的符号输出序列 (y1, ..., ym)</p><p>在每个步骤中，模型都是自回归(auto-regressive)的,<strong><em>在生成下一个时将先前生成的符号用作附加输入</em></strong></p><p>Transformer遵循这种整体架构，对编码器和解码器使用堆叠的自注意力(self-attention)和point-wise</p><p>编码器和解码器的全连接层，分别如图 1 的左半部分和右半部分所示</p><p><img src="fig1.png" alt="结构图" style="zoom:100%;" /></p><h2 id="encoder-and-decoder-stacks">3.1Encoder and Decoder Stacks</h2><h4 id="encoder">3.1.1Encoder</h4><p>编码器由 N = 6 个相同的层组成 , 每层有两个子层</p><ul><li><p>第一个子层是 <strong>多头自注意力机制(multi-head self-attentionmechanism)</strong></p></li><li><p>第二个子层是simple, position-wise fully connected feed-forwardnetwork.（说简单点就是<strong>MLP</strong>）</p></li></ul><p>我们在两个子层中的每一个周围使用残差连接，然后进行层归一化</p><p>即每个子层的输出为<strong>LayerNorm(x + Sublayer(x))</strong></p><p>其中Sublayer(x)是子层自己实现的函数</p><p>为了促进这些残差连接，模型中的所有子层以及嵌入层都会产生维度 dmodel =512 的输出</p><blockquote><p>LayerNorm的细节可以参考下面链接</p><p>https://blog.csdn.net/jump882/article/details/119795466</p></blockquote><h4 id="decoder">3.1.1Decoder</h4><p>解码器也由一堆 N = 6 个相同的层组成。</p><p>除了每个编码器层中的两个子层之外，解码器还插入了第三个子层</p><p>该子层对编码器堆栈的输出执行多头注意力（multi-head attention）</p><p>与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化</p><p>我们还修改了解码器堆栈中的自注意力子层，以<strong>防止位置关注后续位置</strong></p><p><strong>这种掩蔽与输出嵌入偏移一个位置的事实相结合，确保对位置 i的预测只能依赖于位置小于 i 的已知输出</strong></p><h2 id="attention">3.2Attention</h2><p>注意力函数可以描述为将aquery(查询)和一组key-value键值对映射到输出</p><p>其中<strong>查询query</strong>、<strong>键key</strong>、<strong>值value</strong>和<strong>输出</strong>都是向量</p><p><strong>输出可以理解为计算值value的加权和所得</strong></p><p>其中<strong>分配给每个value的权重weight由查询query与相应键key的相似度函数计算</strong></p><p>下面给了一张参考图</p><p><img src="attention1.png" /></p><h4 id="scaled-dot-product-attention">3.2.1 Scaled Dot-ProductAttention</h4><figure><img src="fig2_left.png" alt="attention" /><figcaption aria-hidden="true">attention</figcaption></figure><p>我们将我们的particular attention称为“Scaled Dot-ProductAttention”</p><p>输入由维度 dk 的query和key以及维度 dv 的value组成</p><p><strong>我们计算的query和所有keys的点积，将每个key除以 <spanclass="math inline">\(\sqrt{d_k}\)</span>，然后应用 softmax函数来获得value的权重</strong></p><p>在实践中，我们同时计算一组querys的注意力函数，并打包到矩阵 Q 中</p><p>key和value也打包到矩阵 K 和 V 中</p><p>我们将输出矩阵计算为： <span class="math display">\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span> 下面给了一张参考图 当m=1时就跟单独运算一样</p><p><img src="attention2.png" /></p><p>两个最常用的注意功能是</p><ul><li><p>加性注意 （additive attention ）</p></li><li><p>点积（乘法）注意（dot-product (multiplicative)attention）</p></li></ul><p>点积注意力与我们的算法相同，除了 <spanclass="math inline">\(\sqrt{d_k}\)</span> 的比例因子</p><p>Additive attention使用具有单个隐藏层的前馈网络计算兼容性函数</p><p>虽然两者在理论上的复杂性相似，但<strong>点积注意力在实践中更快且更节省空</strong>间，因为它可以使用高度优化的矩阵乘法代码来实现</p><p>虽然对于较小的 dk值，这两种机制的性能相似，但加法注意力优于点积注意力，而无需对较大的 dk值进行缩放</p><p>我们怀疑对于较大的 dk 值，点积的幅度会变大，从而将 softmax函数推入具有极小梯度的区域</p><p>为了抵消这种影响，我们将点积缩放<spanclass="math inline">\(\sqrt{d_k}\)</span></p><blockquote><p>注意Mask部分具体操作就是将qt之后的值给换成一个非常大的负数，在后续的softmax时候就会变成0</p><p>使得计算结果只用到了v1到vt-1的结果</p></blockquote><h4 id="multi-head-attention">3.2.2 Multi-Head Attention</h4><figure><img src="fig2_right.png" alt="attention" /><figcaption aria-hidden="true">attention</figcaption></figure><p>与使用 dmodel维度的key、value和query执行单个注意函数不同</p><p>我们发现将查询、键和值分别线性投影到 dk、dk 和 dv维度上的不同学习线性投影是有益的（投影到低维度）</p><blockquote><p>相当于给h次机会 希望能够学到不一样的投影的方式</p><p>使得在投影进去的度量空间里面 能够去匹配不同模式的相似函数</p><p>类似卷积神经网络中有多个输出通道的感觉</p></blockquote><p>然后，在每个查询、键和值的投影版本上，我们并行执行 Scaled Dot-ProductAttention，产生 dv 维输出值。</p><p>这些被连接cat起来并再次投影，产生最终值</p><p>Multi-HeadAttention允许模型共同关注来自不同位置的不同表示子空间的信息</p><p>对于单个注意力头，平均化会抑制这一点</p><figure><img src="multi-head.png" alt="截屏2022-05-03 21.06.37" /><figcaption aria-hidden="true">截屏2022-05-03 21.06.37</figcaption></figure><ul><li><p>Q 矩阵从[m,dmodel] 降维到[m , dk] 那么<spanclass="math inline">\(W_i^Q \in \mathbb{R}^{d_{model}\timesd_k}\)</span></p></li><li><p>K 矩阵从[n,dmodel] 降维到[n , dk] 那么<spanclass="math inline">\(W_i^K \in \mathbb{R}^{d_{model}\timesd_k}\)</span></p></li><li><p>V 矩阵从[n,dmodel] 降维到[n , dv] 那么<spanclass="math inline">\(W_i^V \in \mathbb{R}^{d_{model}\timesd_v}\)</span></p></li></ul><p>在这项工作中，我们使用 h = 8 个并行注意力层或头</p><p>对于其中的每一个，我们使用 dk = dv = dmodel/h = 64</p><p>由于每个头的维度减少，总计算成本类似于具有全维度的单头注意力</p><h4 id="applications-of-attention-in-our-model">3.2.3 Applications ofAttention in our Model</h4><p>Transformer 以三种不同的方式使用多头注意力：</p><ul><li>在“编码器-解码器注意力”层中，query来自前一个解码器层，记忆key和value来自编码器的输出。这允许解码器中的每个位置参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制</li><li>编码器包含自注意力层在自注意力层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器中前一层的输出。编码器中的每个位置都可以关注编码器上一层中的所有位置。</li><li>类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的信息向左流动，以保持自回归特性。我们通过屏蔽掉（设置为 -∞）softmax输入中与非法连接相对应的所有值来实现缩放点积注意力的内部</li></ul><h2 id="position-wise-feed-forward-networks">3.3Position-wiseFeed-Forward Networks</h2><p>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性变换，中间有一个 ReLU 激活。 <span class="math display">\[FFN(x) = max(0,xW_1 + b_1 )W_2 + b_2\]</span></p><blockquote><p>输入层 - 隐藏层 - 输出层</p><p>输入( n , dmodel = 512 )</p><p>隐藏层( n , dmodel*4 = 2048)</p><p>输出层（n ， dmodel = 512）</p></blockquote><p>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另</p><p>一种描述方式是内核大小为 1 的两个卷积</p><p>输入和输出的维度为 dmodel = 512，内层的维度为 dff = 2048</p><h2 id="embeddings-and-softmax">3.4 Embeddings and Softmax</h2><p>与其他序列转导模型类似，我们<strong>使用learnedembedding将输入标记和输出标记转换为维度 dmodel 的向量</strong></p><p>我们还使用通常的学习线性变换和 softmax函数将解码器输出转换为预测的下一个token概率</p><p>在我们的模型中，我们在两个embedding和 pre-softmax线性变换之间共享相同的权重矩阵</p><p>在embedding中，我们将这些权重乘以 <spanclass="math inline">\(\sqrt{d_{model}}\)</span></p><h2 id="positional-encoding">3.5 Positional Encoding</h2><p>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于标记在序列中的相对或绝对位置的信息</p><p>为此，我们在输入嵌入编码器和解码器堆栈的底部中添加“位置编码”</p><p>位置编码与嵌入具有相同的维度 dmodel，因此可以将两者相加</p><p>位置编码有很多选择，学习的和固定的</p><p>在这项工作中，我们使用不同频率的正弦和余弦函数： <spanclass="math display">\[PE(pos,2i) = sin(pos/1000^{2i/d_{model}})\]</span></p><p><span class="math display">\[PE(pos,2i+1) = cos(pos/1000^{2i/d_{model}})\]</span></p><ul><li>pos 是位置</li><li>i 是维度</li></ul><p>也就是说，位置编码的每个维度对应一个正弦曲线。</p><p>波长形成从 2π 到 10000 · 2π 的几何级数</p><p>我们选择这个函数是因为我们假设它可以让模型轻松学习通过相对位置来参与，因为对于任何固定的偏移量k</p><p><span class="math inline">\(PE_{pos+k}\)</span>可以表示为 <spanclass="math inline">\(PE_{pos}\)</span> 的线性函数</p><p>我们还尝试使用学习的位置嵌入 , 发现这两个版本产生了几乎相同的结果</p><p>我们选择了正弦版本，因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。&lt;/p&gt;
&lt;p&gt;性能最好的模型还通过注意力机制连接编码器和解码器。&lt;/p&gt;
&lt;p&gt;我们提出了一种新的简单网络架构 &lt;strong&gt;Transf</summary>
      
    
    
    
    <category term="cs.CV" scheme="http://example.com/categories/cs-CV/"/>
    
    
    <category term="Transformer" scheme="http://example.com/tags/Transformer/"/>
    
  </entry>
  
</feed>
