<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扁同学不发言的个人博客</title>
  
  
  <link href="http://www.larryai.com/atom.xml" rel="self"/>
  
  <link href="http://www.larryai.com/"/>
  <updated>2022-05-31T12:31:35.024Z</updated>
  <id>http://www.larryai.com/</id>
  
  <author>
    <name>扁同学不发言</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux计划任务</title>
    <link href="http://www.larryai.com/2022/05/31/linx-plan/"/>
    <id>http://www.larryai.com/2022/05/31/linx-plan/</id>
    <published>2022-05-31T09:23:50.000Z</published>
    <updated>2022-05-31T12:31:35.024Z</updated>
    
    <content type="html"><![CDATA[<h2 id="at命令">at命令</h2><p>at 命令是依靠atd服务工作的。at的计划任务都是一次性的，也就是只执行一次。</p><h3 id="启动at服务">启动at服务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service atd start</span><br></pre></td></tr></table></figure><h3 id="添加计划任务">添加计划任务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">at now + 1 minute </span><br><span class="line">at&gt; <span class="built_in">date</span> &gt;&gt; at.log </span><br><span class="line">at&gt; &lt;EOT&gt;  <span class="comment"># Ctrl+D结束 </span></span><br></pre></td></tr></table></figure><p>上述命令表示1分钟之后执行任务date。按Ctrl+D结束任务编辑。</p><p>at命令中，有很灵活的指定时间的形式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">at -f file.sh 05pm + 3 days  <span class="comment"># 3天后的下午5点</span></span><br><span class="line">at -f file.sh tomorrow <span class="comment"># 明天  today</span></span><br><span class="line">at -f file.sh noon <span class="comment"># 中午 midnight/teatime</span></span><br><span class="line">at -f file.sh 16:00 05/05/2021 <span class="comment"># 完整的日期和时间</span></span><br></pre></td></tr></table></figure><h3id="可以管理哪些用户可以使用at命令">可以管理哪些用户可以使用at命令</h3><ul><li>/etc/at.allow</li><li>/etc/at.deny</li></ul><p>at命令先查询 <code>/etc/at.allow</code>文件，凡事出现在该文件中的用户都可以使用at命令，这个就是所谓的<code>白名单</code>。</p><p>如果没有这个文件，则查找 <code>/etc/at.deny</code>，这个就是<code>黑名单</code> ，凡是不在该文件中的用户都可以使用at命令。</p><p>一般系统都是允许所有正常用户使用at命令，所以在/etc目录中存放一个空的at.deny文件。</p><h3 id="查看当前等待运行的任务">查看当前等待运行的任务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">atq  <span class="comment">#查看当前还有哪些待执行任务 </span></span><br><span class="line">atrm &lt;num&gt; <span class="comment">#删除计划任务，后跟的是任务的号码 </span></span><br></pre></td></tr></table></figure><h2 id="crontab命令">crontab命令</h2><h3 id="简介">简介</h3><p>crontab命令常见于设置<strong>周期性</strong>被执行的命令，实现自动进行系统管理的目的。</p><p>crontab 命令从输入设备读取指令，并将其存放于 crontab文件中，以供之后读取和执行。</p><p>通常，crontab 储存的指令被守护进程激活，crond 为其守护进程，crond常常在后台运行，每一分钟会检查一次是否有预定的作业需要执行。</p><p>通过 crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell　 script 脚本。</p><p>时间间隔的单位可以是分钟、小时、日、月、周的任意组合。</p><p>这里我们看一看 crontab 的格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Example of job definition:</span><br><span class="line"># .---------------- minute (0 - 59)</span><br><span class="line"># |  .------------- hour (0 - 23)</span><br><span class="line"># |  |  .---------- day of month (1 - 31)</span><br><span class="line"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span><br><span class="line"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span><br><span class="line"># |  |  |  |  |</span><br><span class="line"># *  *  *  *  * user-name command to be executed</span><br></pre></td></tr></table></figure><h3 id="准备">准备</h3><p>crontab 在本实验环境中需要做一些特殊的准备，首先我们会启动rsyslog，以便我们可以通过日志中的信息来了解我们的任务是否真正的被执行了（在自己本地中Ubuntu 会默认自行启动不需要手动启动）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y rsyslog</span><br><span class="line">sudo service rsyslog start</span><br></pre></td></tr></table></figure><h3 id="基本用法">基本用法</h3><p>下面将开始 crontab的使用了，我们通过下面一个命令来添加一个计划任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure><p>第一次启动让我们选择编辑的工具，选择第二个基本的 vim 就可以了。</p><p>有的系统中没有提示，直接就是vim编辑器，也有的系统中第一个选项是nano编辑器。</p><p>每行的开头五个域，指定计划任务执行的时间和日期等，从左开始依次是：</p><ul><li>分钟 m(minute)</li><li>小时 h(hour)</li><li>日 dom(day of month)</li><li>月 mon(month)</li><li>星期 dow(day of week)</li></ul><p>最后是要执行的命令，也可以是脚本。</p><h3 id="查看计划任务删除任务">查看计划任务/删除任务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab -l</span><br><span class="line">crontab -r</span><br></pre></td></tr></table></figure><h2 id="anacron">anacron</h2><p>anacron是执行按天为最小计划时间单位的计划任务的，它并不要求计算机24×7连续运行，对于一些笔记本或者台式计算机来说，经常处于关机不工作状态，如果使用cron就会错过计划任务，但是使用anacron就可以在下一次启动后执行计划任务。</p><p>如果系统中没有anacron，则需要安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y anacron</span><br></pre></td></tr></table></figure><p>然后可以看到/etc/anacrontab文件，打开可以查看文件的内容。</p><p><img src="anacron1.png" /></p><p>anacron的配置跟cron配置不同，包含四个域：</p><p><img src="anacron2.png" /></p><p>这四个域表示的含义如下图所示：</p><p><img src="anacron3.png" /></p><ul><li><p>period： 指明天数</p><ul><li>1 - daily</li><li>7 - weekly</li><li>30 - monthly</li><li>N - number of days</li><li><span class="citation" data-cites="monthly">@monthly</span> -每月执行job</li></ul></li><li><p>delay： 执行job之前延迟的分钟数</p></li><li><p>identifier：记录job执行的时间戳的文件名</p></li><li><p>command：要执行的job，命令或者shell脚本。</p><p>在/var/spool/anacron/目录下会出现一些文件，这些文件对应着配置的job，记录了最近一次执行job的日期。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;at命令&quot;&gt;at命令&lt;/h2&gt;
&lt;p&gt;at 命令是依靠atd服务工作的。
at的计划任务都是一次性的，也就是只执行一次。&lt;/p&gt;
&lt;h3 id=&quot;启动at服务&quot;&gt;启动at服务&lt;/h3&gt;
&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux日志系统管理</title>
    <link href="http://www.larryai.com/2022/05/31/linux-log/"/>
    <id>http://www.larryai.com/2022/05/31/linux-log/</id>
    <published>2022-05-31T08:57:20.000Z</published>
    <updated>2022-05-31T12:31:29.366Z</updated>
    
    <content type="html"><![CDATA[<p>日志是一个系统管理员，一个运维人员，甚至是开发人员不可或缺的东西，系统用久了偶尔也会出现一些错误，我们需要日志来给系统排错，在一些网络应用服务不能正常工作的时候，我们需要用日志来做问题定位，日志还是过往时间的记录本，我们可以通过它知道我们是否被不明用户登录过等等。</p><p>在 Linux 中大部分的发行版都内置使用 syslog系统日志。常见的日志一般存放在 <code>/var/log</code> 中</p><p>我们可以根据服务对象粗略的将日志分为两类</p><ul><li>系统日志</li><li>应用日志</li></ul><blockquote><p>系统日志主要是存放系统内置程序或系统内核之类的日志信息如<code>alternatives.log</code> 、<code>btmp</code> 等等</p></blockquote><blockquote><p>应用日志主要是我们装的第三方应用所产生的日志如 <code>tomcat7</code>、<code>apache2</code> 等等。</p></blockquote><p>接下来我们来看看常见的<strong>系统日志</strong>有哪些，他们都记录了怎样的信息</p><table><colgroup><col style="width: 23%" /><col style="width: 76%" /></colgroup><thead><tr class="header"><th>日志名称</th><th>记录信息</th></tr></thead><tbody><tr class="odd"><td>alternatives.log</td><td>系统的一些更新替代信息记录</td></tr><tr class="even"><td>apport.log</td><td>应用程序崩溃信息记录</td></tr><tr class="odd"><td>apt/history.log</td><td>使用 apt-get 安装卸载软件的信息记录</td></tr><tr class="even"><td>apt/term.log</td><td>使用 apt-get 时的具体操作，如 package 的下载、打开等</td></tr><tr class="odd"><td>auth.log</td><td>登录认证的信息记录，也可能是secure文件</td></tr><tr class="even"><td>boot.log</td><td>系统启动时的程序服务的日志信息</td></tr><tr class="odd"><td>btmp</td><td>错误的信息记录</td></tr><tr class="even"><td>Consolekit/history</td><td>控制台的信息记录</td></tr><tr class="odd"><td>dist-upgrade</td><td>dist-upgrade 这种更新方式的信息记录</td></tr><tr class="even"><td>dmesg</td><td>启动时，显示屏幕上内核缓冲信息,与硬件有关的信息</td></tr><tr class="odd"><td>dpkg.log</td><td>dpkg 命令管理包的日志。</td></tr><tr class="even"><td>faillog</td><td>用户登录失败详细信息记录</td></tr><tr class="odd"><td>fontconfig.log</td><td>与字体配置有关的信息记录</td></tr><tr class="even"><td>kern.log</td><td>内核产生的信息记录，在自己修改内核时有很大帮助</td></tr><tr class="odd"><td>lastlog</td><td>用户的最近信息记录</td></tr><tr class="even"><td>wtmp</td><td>登录信息的记录。wtmp可以找出谁正在进入系统，谁使用命令显示这个文件或信息等</td></tr><tr class="odd"><td>syslog</td><td>系统信息记录</td></tr></tbody></table><p>这些日志是如何产生的？通过上面的例子我们可以看出大部分的日志信息似乎格式都很类似，并且都出现在这个文件夹中。</p><p>这样的实现可以通过两种方式：</p><ul><li>一种是由软件开发商自己来自定义日志格式然后指定输出日志位置；</li><li>一种方式就是 Linux 提供的日志服务程序，而我们这里系统日志是通过syslog 来实现，提供日志管理服务。</li></ul><p>syslog 是一个系统日志记录程序，在早期的大部分 Linux 发行版都是内置syslog，让其作为系统的默认日志收集工具，虽然随着时代的进步与发展，syslog已经年老体衰跟不上时代的需求，所以他被 rsyslog 所代替了，较新的Ubuntu、Fedora 等等都是默认使用 rsyslog 作为系统的日志收集工具</p><p>rsyslog 的全称是 rocket-fast system forlog，它提供了高性能，高安全功能和模块化设计。rsyslog能够接受各种各样的来源，将其输入，输出的结果到不同的目的地。rsyslog可以提供超过每秒一百万条消息给目标文件。</p><p>这样能实时收集日志信息的程序是有其守护进程的，如 rsyslog的守护进程便是 rsyslogd</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;日志是一个系统管理员，一个运维人员，甚至是开发人员不可或缺的东西，系统用久了偶尔也会出现一些错误，我们需要日志来给系统排错，在一些网络应用服务不能正常工作的时候，我们需要用日志来做问题定位，日志还是过往时间的记录本，我们可以通过它知道我们是否被不明用户登录过等等。&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux文件系统操作与磁盘管理</title>
    <link href="http://www.larryai.com/2022/05/31/linux-sysfile/"/>
    <id>http://www.larryai.com/2022/05/31/linux-sysfile/</id>
    <published>2022-05-31T02:44:56.000Z</published>
    <updated>2022-05-31T12:31:24.055Z</updated>
    
    <content type="html"><![CDATA[<h2 id="df命令">df命令</h2><p>查看磁盘的容量</p><h2 id="du命令">du命令</h2><p>查看目录的容量</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认同样以 块 的大小展示</span></span><br><span class="line"><span class="built_in">du</span> &lt;文件&gt;或者&lt;目录&gt;</span><br><span class="line"><span class="comment"># 加上`-h`参数，以更易读的方式展示</span></span><br><span class="line"><span class="built_in">du</span> -h &lt;文件&gt;或者&lt;目录&gt;</span><br></pre></td></tr></table></figure><p><code>-d</code>参数指定查看目录的深度</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只查看1级目录的信息</span></span><br><span class="line"><span class="built_in">du</span> -h -d 0  ~   <span class="comment">#查看当前目录</span></span><br><span class="line"><span class="comment"># 查看2级</span></span><br><span class="line"><span class="built_in">du</span> -h -d 1  ~   <span class="comment">#查看当前目录及下一级目录</span></span><br></pre></td></tr></table></figure><h2 id="dd命令">dd命令</h2><h3 id="转换和复制文件">转换和复制文件</h3><p><code>dd</code> 命令用于转换和复制文件，不过它的复制不同于<code>cp</code>。</p><p>之前提到过关于 Linux 的很重要的一点，<strong>一切即文件</strong></p><p>在 Linux 上，硬件的设备驱动（如硬盘）和特殊设备文件（如<code>/dev/zero</code> 和 <code>/dev/random</code>）都像普通文件一样，只是在各自的驱动程序中实现了对应的功能</p><p><code>dd</code> 也可以读取文件或写入这些文件。</p><p>这样，<code>dd</code>也可以用在备份硬件的引导扇区、获取一定数量的随机数据或者空数据等任务中</p><p><code>dd</code> 程序也可以在复制时处理数据，例如转换字节序、或在ASCII 与 EBCDIC 编码间互换。</p><p><code>dd</code> 的命令行语句与其他的 Linux程序不同，因为它的命令行选项格式为<code>选项=值</code>，而不是更标准的<code>--选项 值</code>或<code>-选项=值</code>。</p><p><code>dd</code>默认从标准输入中读取，并写入到标准输出中，但可以用选项<code>if</code>（input file，输入文件）和 <code>of</code>（outputfile，输出文件）改变。</p><p>我们先来试试用 <code>dd</code>命令从标准输入读入用户的输入到标准输出或者一个文件中：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出到文件</span></span><br><span class="line"><span class="built_in">dd</span> of=<span class="built_in">test</span> bs=10 count=1 <span class="comment"># 或者 dd if=/dev/stdin of=test bs=10 count=1</span></span><br><span class="line"><span class="comment"># 输出到标准输出</span></span><br><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/stdin of=/dev/stdout bs=10 count=1</span><br><span class="line"><span class="comment"># 注:在打完了这个命令后，继续在终端打字，作为你的输入</span></span><br></pre></td></tr></table></figure><ul><li><p><code>bs</code>（block size）用于指定块大小（缺省单位为Byte，也可为其指定如'K'，'M'，'G'等单位）</p></li><li><p><code>count</code>用于指定块数量</p></li></ul><h4 id="创建虚拟镜像文件">创建虚拟镜像文件</h4><p>从 <code>/dev/zero</code> 设备创建一个容量为 256M 的空文件：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=virtual.img bs=1M count=256</span><br><span class="line"><span class="built_in">du</span> -h virtual.img</span><br></pre></td></tr></table></figure><p>然后我们要将这个文件格式化（写入文件系统），这里我们要学到一个（准确的说是一组）新的命令来完成这个需求。</p><h4 id="使用-mkfs-命令格式化磁盘我们这里是自己创建的虚拟磁盘镜像">使用mkfs 命令格式化磁盘（我们这里是自己创建的虚拟磁盘镜像）</h4><p>你可以在命令行输入 <code>sudo mkfs</code>然后按下<code>Tab</code>键，你可以看到很多个以 <code>mkfs</code>为前缀的命令</p><p>这些不同的后缀其实就是表示着不同的文件系统，可以用 <code>mkfs</code>格式化成的文件系统。</p><p>我们可以简单的使用下面的命令来将我们的虚拟磁盘镜像格式化为<code>ext4</code>文件系统：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkfs.ext4 virtual.img</span><br></pre></td></tr></table></figure><h4 id="使用-mount-命令挂载磁盘到目录树">使用 mount命令挂载磁盘到目录树</h4><p>用户在 Linux/UNIX的机器上打开一个文件以前，包含该文件的文件系统必须先进行挂载的动作，此时用户要对该文件系统执行<code>mount</code> 的指令以进行挂载。</p><p>该指令通常是使用在 USB或其他可移除存储设备上，而根目录则需要始终保持挂载的状态。</p><p>又因为 Linux/UNIX文件系统可以对应一个文件而不一定要是硬件设备，所以可以挂载一个包含文件系统的文件到目录树。</p><p>Linux/UNIX 命令行的 mount指令是告诉操作系统，对应的文件系统已经准备好，可以使用了，而该文件系统会对应到一个特定的点（称为挂载点）。</p><p>挂载好的文件、目录、设备以及特殊文件即可提供用户使用。</p><p>我们先来使用<code>mount</code>来查看下主机已经挂载的文件系统</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount</span><br></pre></td></tr></table></figure><blockquote><p>输出的结果中每一行表示一个设备或虚拟设备,每一行最前面是设备名，然后是on 后面是挂载点，type后面表示文件系统类型，再后面是挂载选项（比如可以在挂载时设定以只读方式挂载等等）。</p></blockquote><p>那么我们如何挂载真正的磁盘到目录树呢，<code>mount</code>命令的一般格式如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount [options] [<span class="built_in">source</span>] [directory]</span><br></pre></td></tr></table></figure><p>一些常用操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount [-o [操作选项]] [-t 文件系统类型] [-w|--rw|--ro] [文件系统源] [挂载点] </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 命令格式 sudo umount 已挂载设备名或者挂载点，如：</span><br><span class="line">$ sudo umount /mnt</span><br><span class="line"></span><br><span class="line">不过遗憾的是，由于我们环境的问题（环境中使用的 Linux 内核在编译时没有添加对 Loop device 的支持），所以你将无法挂载成功：另外关于 loop 设备,你可能会有诸多疑问，那么请看下面来自维基百科/dev/loop的说明：在类 UNIX 系统中，/dev/loop（或称 vnd （vnode disk）、lofi（循环文件接口））是一种伪设备，这种设备使得文件可以如同块设备一般被访问。在使用之前，循环设备必须与现存文件系统上的文件相关联。这种关联将提供给用户一个应用程序接口，接口将允许文件视为块特殊文件（参见设备文件系统）使用。因此，如果文件中包含一个完整的文件系统，那么这个文件就能如同磁盘设备一般被挂载。这种设备文件经常被用于光盘或是磁盘镜像。通过循环挂载来挂载包含文件系统的文件，便使处在这个文件系统中的文件得以被访问。这些文件将出现在挂载点目录。如果挂载目录中本身有文件，这些文件在挂载后将被禁止使用。</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;df命令&quot;&gt;df命令&lt;/h2&gt;
&lt;p&gt;查看磁盘的容量&lt;/p&gt;
&lt;h2 id=&quot;du命令&quot;&gt;du命令&lt;/h2&gt;
&lt;p&gt;查看目录的容量&lt;/p&gt;
&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux软件包安装和管理</title>
    <link href="http://www.larryai.com/2022/05/31/linux-user/"/>
    <id>http://www.larryai.com/2022/05/31/linux-user/</id>
    <published>2022-05-31T01:53:59.000Z</published>
    <updated>2022-05-31T12:31:17.418Z</updated>
    
    <content type="html"><![CDATA[<h2 id="apt-get命令">apt-get命令</h2><blockquote><p>用于处理<code>apt</code>包的公用程序集，我们可以用它来在线安装、卸载和升级软件包等，下面列出一些<code>apt-get</code>包含的常用的一些工具</p></blockquote><table><colgroup><col style="width: 18%" /><col style="width: 81%" /></colgroup><thead><tr class="header"><th>工具</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>install</code></td><td>其后加上软件包名，用于安装一个软件包</td></tr><tr class="even"><td><code>update</code></td><td>从软件源镜像服务器上下载/更新用于更新本地软件源的软件包列表</td></tr><tr class="odd"><td><code>upgrade</code></td><td>升级本地可更新的全部软件包，但存在依赖问题时将不会升级，通常会在更新之前执行一次<code>update</code></td></tr><tr class="even"><td><code>dist-upgrade</code></td><td>解决依赖关系并升级(存在一定危险性)</td></tr><tr class="odd"><td><code>remove</code></td><td>移除已安装的软件包，包括与被移除软件包有依赖关系的软件包，但不包含软件包的配置文件</td></tr><tr class="even"><td><code>autoremove</code></td><td>移除之前被其他软件包依赖，但现在不再被使用的软件包</td></tr><tr class="odd"><td><code>purge</code></td><td>与 remove 相同，但会完全移除软件包，包含其配置文件</td></tr><tr class="even"><td><code>clean</code></td><td>移除下载到本地的已经安装的软件包，默认保存在/var/cache/apt/archives/</td></tr><tr class="odd"><td><code>autoclean</code></td><td>移除已安装的软件的旧版本软件包</td></tr></tbody></table><p>下面是一些<code>apt-get</code>常用的参数：</p><table><colgroup><col style="width: 25%" /><col style="width: 75%" /></colgroup><thead><tr class="header"><th>参数</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>-y</code></td><td>自动回应是否安装软件包的选项，在一些自动化安装脚本中使用这个参数将十分有用</td></tr><tr class="even"><td><code>-s</code></td><td>模拟安装</td></tr><tr class="odd"><td><code>-q</code></td><td>静默安装方式，指定多个<code>q</code>或者<code>-q=#</code>,#表示数字，用于设定静默级别，这在你不想要在安装软件包时屏幕输出过多时很有用</td></tr><tr class="even"><td><code>-f</code></td><td>修复损坏的依赖关系</td></tr><tr class="odd"><td><code>-d</code></td><td>只下载不安装</td></tr><tr class="even"><td><code>--reinstall</code></td><td>重新安装已经安装但可能存在问题的软件包</td></tr><tr class="odd"><td><code>--install-suggests</code></td><td>同时安装 APT 给出的建议安装的软件包</td></tr></tbody></table><h3 id="安装软件">安装软件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install &lt;软件包名&gt;</span><br></pre></td></tr></table></figure><h3 id="软件升级">软件升级</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">更新软件源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">升级没有依赖问题的软件包</span></span><br><span class="line">sudo apt-get upgrade</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">升级并解决依赖关系</span></span><br><span class="line">sudo apt-get dist-upgrade</span><br></pre></td></tr></table></figure><h3 id="卸载软件">卸载软件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove  &lt;软件包名&gt;</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不保留配置文件的移除</span></span><br><span class="line">sudo apt-get purge &lt;软件包名&gt;</span><br><span class="line"><span class="comment"># 或者 sudo apt-get --purge remove</span></span><br><span class="line"><span class="comment"># 移除不再需要的被依赖的软件包</span></span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure><h3 id="搜索软件">搜索软件</h3><p>当自己刚知道了一个软件，想下载使用，需要确认软件仓库里面有没有，就需要用到搜索功能了，命令如下：</p><p><code>apt-cache</code>命令则是针对本地数据进行相关操作的工具，<code>search</code>顾名思义在本地的数据库中寻找有关 <code>softname1</code><code>softname2</code> …… 相关软件的信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-cache search &lt;软件名&gt;</span><br></pre></td></tr></table></figure><h2 id="dpkg介绍">dpkg介绍</h2><blockquote><p>dpkg 是 Debian 软件包管理器的基础，它被伊恩·默多克创建于 1993年。dpkg 与 RPM 十分相似，同样被用于安装、卸载和供给和 .deb软件包相关的信息。</p></blockquote><blockquote><p>dpkg 本身是一个底层的工具。上层的工具，像是APT，被用于从远程获取软件包以及处理复杂的软件包关系。"dpkg"是"DebianPackage"的简写。</p></blockquote><p>我们经常可以在网络上见到以<code>deb</code>形式打包的软件包，就需要使用<code>dpkg</code>命令来安装。</p><p><code>dpkg</code>常用参数介绍：</p><table><thead><tr class="header"><th>参数</th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>-i</code></td><td>安装指定 deb 包</td></tr><tr class="even"><td><code>-R</code></td><td>后面加上目录名，用于安装该目录下的所有 deb 安装包</td></tr><tr class="odd"><td><code>-r</code></td><td>remove，移除某个已安装的软件包</td></tr><tr class="even"><td><code>-I</code></td><td>显示<code>deb</code>包文件的信息</td></tr><tr class="odd"><td><code>-s</code></td><td>显示已安装软件的信息</td></tr><tr class="even"><td><code>-S</code></td><td>搜索已安装的软件包</td></tr><tr class="odd"><td><code>-L</code></td><td>显示已安装软件包的目录信息</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;apt-get命令&quot;&gt;apt-get命令&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;用于处理
&lt;code&gt;apt&lt;/code&gt;包的公用程序集，我们可以用它来在线安装、卸载和升级软件包等，下面列出一些&lt;code&gt;apt-get&lt;/code&gt;包含的常用的一些工具&lt;/p</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux系统管理</title>
    <link href="http://www.larryai.com/2022/05/22/linux-sys/"/>
    <id>http://www.larryai.com/2022/05/22/linux-sys/</id>
    <published>2022-05-22T05:08:40.000Z</published>
    <updated>2022-05-24T11:10:52.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="系统启动">系统启动</h1><blockquote><p>稍后更新</p></blockquote><h1 id="软件包管理">软件包管理</h1><h2 id="软件发布">软件发布</h2><h3 id="二进制包">二进制包</h3><ul><li>rpm</li><li>deb</li></ul><h3 id="源码包">源码包</h3><ul><li>tar</li><li>tar.gz</li><li>zip</li><li>tar</li><li>xz</li></ul><h2 id="rpm包">rpm包</h2><figure><img src="rpm.png" alt="rpm" /><figcaption aria-hidden="true">rpm</figcaption></figure><figure><img src="rpm1.png" alt="rpm1" /><figcaption aria-hidden="true">rpm1</figcaption></figure><h2 id="deb包">Deb包</h2><figure><img src="Deb.png" alt="Deb" /><figcaption aria-hidden="true">Deb</figcaption></figure><figure><img src="deb1.png" alt="deb1" /><figcaption aria-hidden="true">deb1</figcaption></figure><h2 id="yum软件源">yum软件源</h2><figure><img src="yum.png" alt="yum" /><figcaption aria-hidden="true">yum</figcaption></figure><h2 id="源码">源码</h2><figure><img src="ym.png" alt="ym" /><figcaption aria-hidden="true">ym</figcaption></figure><h1 id="用户管理">用户管理</h1><h2 id="用户相关文件">用户相关文件</h2><ul><li><code>/etc/passwd</code></li><li><code>/ect/shadow</code></li><li><code>/etc/group</code></li><li><code>/etc/gshadow</code></li></ul><h2 id="相关命令">相关命令</h2><figure><img src="用户.png" alt="用户" /><figcaption aria-hidden="true">用户</figcaption></figure><ul><li><p>添加新用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd 用户名</span><br></pre></td></tr></table></figure><ul><li><code>-d</code> 用户主目录的父目录</li><li><code>-g</code>用户添加到某个用户组</li></ul></li><li><p>修改用户登录口令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd 用户名</span><br></pre></td></tr></table></figure></li></ul><h2 id="sudo命令">sudo命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u &lt;用户名&gt; &lt;命令&gt;</span><br></pre></td></tr></table></figure><blockquote><p><code>-u</code>将当前用户提权到<用户名>身份(默认root)，再执行后面的命令</p></blockquote><p>给用户赋权sudo操作</p><ul><li>将用户加入到<code>admin</code>、<code>sudo</code>、<code>wheel</code>组</li><li>修改<code>etc/sudoers</code>文件详细参考https://www.jianshu.com/p/b4aa537786d0</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;系统启动&quot;&gt;系统启动&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;稍后更新&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;软件包管理&quot;&gt;软件包管理&lt;/h1&gt;
&lt;h2 id=&quot;软件发布&quot;&gt;软件发布&lt;/h2&gt;
&lt;h3 id=&quot;二进制包&quot;&gt;二进制包&lt;/h3&gt;
&lt;ul</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux基本指令3</title>
    <link href="http://www.larryai.com/2022/05/17/linux3/"/>
    <id>http://www.larryai.com/2022/05/17/linux3/</id>
    <published>2022-05-17T09:23:24.000Z</published>
    <updated>2022-05-31T12:33:56.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="gzip命令">gzip命令</h2><p>压缩工具</p><h4 id="gzip压缩解压文件">gzip压缩/解压文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gzip file</span><br><span class="line"></span><br><span class="line">gzip -d file</span><br></pre></td></tr></table></figure><blockquote><p>将文件压缩 产生file.gz文件 并把原始文件删除</p></blockquote><h4 id="gzip压缩解压目录">gzip压缩/解压目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gzip -r ../dir  </span><br><span class="line"></span><br><span class="line">gzip -dr ../dir</span><br></pre></td></tr></table></figure><h2 id="tar命令">tar命令</h2><p>打包工具</p><blockquote><p>将多个文件或目录打包</p><p>不会删除原始文件</p></blockquote><ul><li><code>-c</code>创建新文件</li><li><code>-v</code> 列出文件清单 (终端输出)</li><li><code>-f</code> 后面跟归档文件</li><li><code>-x</code> 从归档文件中提取</li></ul><h4 id="打包文件或目录">打包文件或目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -cvf file.tar filelist...</span><br><span class="line">tar -cvf fir.tar dirlist...</span><br></pre></td></tr></table></figure><h4 id="解包">解包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf file.tar</span><br><span class="line">tar -xvf dir.tar</span><br></pre></td></tr></table></figure><h4 id="查看包内容">查看包内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -t -f file.tar</span><br></pre></td></tr></table></figure><h4 id="与压缩有关的选项">与压缩有关的选项</h4><p><code>-z</code> 相当于<code>--gzip</code> , <code>--gunzip</code>先打包后压缩 导致最后的文件为<code>tar.gz</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">打包+压缩</span></span><br><span class="line">tar -zcvf file.tar.gz filelist...</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">解包+解压缩</span></span><br><span class="line">tar -zxvf file.tar.gz</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看包内容</span></span><br><span class="line">tar -t -f file.tar.gz</span><br></pre></td></tr></table></figure><h2 id="校验工具">校验工具</h2><ul><li>md5sum</li><li>sha1sum</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">md5sum file</span><br><span class="line"></span><br><span class="line">sha1sum file</span><br></pre></td></tr></table></figure><h2 id="命令组合">命令组合</h2><ul><li>管道 <code>|</code></li><li>分号<code>;</code> 命令按照顺序执行</li><li><code>&amp;&amp;</code>前面的命令执行成功 执行后续命令直到某个命令不成功</li><li><code>||</code> 前面的命令不成功 执行后续命令直到某个命令成功为止</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;gzip命令&quot;&gt;gzip命令&lt;/h2&gt;
&lt;p&gt;压缩工具&lt;/p&gt;
&lt;h4 id=&quot;gzip压缩解压文件&quot;&gt;gzip压缩/解压文件&lt;/h4&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux基本指令2</title>
    <link href="http://www.larryai.com/2022/05/17/linux2/"/>
    <id>http://www.larryai.com/2022/05/17/linux2/</id>
    <published>2022-05-17T07:51:26.000Z</published>
    <updated>2022-05-31T12:33:42.884Z</updated>
    
    <content type="html"><![CDATA[<h2 id="grep指令">grep指令</h2><p>根据一个模式进行输出</p><p><code>grep 表达式 目标文件</code></p><p>案例</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> -l | grep <span class="string">&quot;^-r.x&quot;</span></span><br></pre></td></tr></table></figure><ul><li><p><code>ls</code> <code>grep</code>两个命令</p></li><li><p><code>grep</code>没有文件参数</p></li><li><p><code>｜</code> 管道（<strong>将前面的输出作为后面的输入</strong>）</p></li></ul><p>grep中的模式用<strong>正则表达式</strong>来表示</p><ul><li><code>^</code> 表示行首</li><li><code>$</code> 表示行尾</li><li><code>.</code> 表示任一字符<ul><li>与通配符 <code>?</code>要区分</li></ul></li><li>-,r,x 就是字符本身</li></ul><h2 id="xargs命令">xargs命令</h2><ul><li><p>xargs命令单独使用可以吧接受的数据一行输出</p></li><li><p>xargs不能加文件名 使用重定向读取文件<code>xargs &lt; file</code></p></li><li><p><code>-n</code>指定每行最大参数量 每行n个参数默认分割符号为<code>空格</code></p></li><li><p><code>-d</code> 自定义分隔符(split())</p></li></ul><p><img src="xargs.png" /></p><h2 id="cut命令">cut命令</h2><p>从文件中的每一行选择指定的部分打印到standard output上</p><p>常用选项</p><p><img src="cut.png" /></p><ul><li><p><code>-b</code> 以字节分割</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cut</span> -b 5 file <span class="comment">#选取每行第五个字节</span></span><br><span class="line"><span class="built_in">cut</span> -b 5-9 file <span class="comment">#选取每行第5-9个字节</span></span><br><span class="line"><span class="built_in">cut</span> -b -5,10,14- file <span class="comment">#选择每行 从开头到第5字节 第10字节 从第14字节到结尾的内容</span></span><br></pre></td></tr></table></figure></li><li><p><code>-c</code> 以字符分割</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cut</span> -b -5 file <span class="comment">#选取每行开头的五个字节</span></span><br><span class="line"><span class="built_in">cut</span> -c -5 file <span class="comment">#选取每行开头的五个字符</span></span><br><span class="line">cur -nb -5 file <span class="comment">#不对多字节字符进行分隔</span></span><br><span class="line"><span class="comment"># urf-8 每行汉子3个字节</span></span><br></pre></td></tr></table></figure></li><li><p><code>-d</code>自定义分隔符 <code>-f</code>选取的区域</p></li></ul><h2 id="wc命令">wc命令</h2><p>统计每一行的xx数量</p><p>选项</p><ul><li><code>-c</code> 按照字节统计</li><li><code>-m</code>按照字符统计</li><li><code>-l</code> 按照行统计</li><li><code>-L</code> 最大显示宽度</li><li><code>-w</code>按照词统计</li><li><code>--help</code> 显示帮助</li><li><code>--version</code> 输出版本信息</li></ul><h2 id="sort命令">sort命令</h2><p>对行进行排序</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sort</span> filename <span class="comment">#正排序后的文档</span></span><br><span class="line"><span class="built_in">sort</span> -r filename <span class="comment">#逆排序后的文档</span></span><br></pre></td></tr></table></figure><h2 id="cmp命令">cmp命令</h2><p>将两个文件逐个字节进行对比</p><ul><li>发现第一个不同的地方就结束并输出</li><li>适合比较两个文件是否完全相同</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmp file1 file2</span><br></pre></td></tr></table></figure><h2 id="comm命令">comm命令</h2><p>对两个排序后的文件进行逐行比较</p><ul><li>输出有三列<ul><li>第一列：file1所独有的</li><li>第二列：file2所独有的</li><li>第三列：file1 和 file2 共有的</li></ul></li><li>使用 <code>-n</code> 显示第n列</li></ul><h2 id="diff命令">diff命令</h2><p>对两个文件进行逐行比较</p><figure><img src="diff.png" alt="diff" /><figcaption aria-hidden="true">diff</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;grep指令&quot;&gt;grep指令&lt;/h2&gt;
&lt;p&gt;根据一个模式进行输出&lt;/p&gt;
&lt;p&gt;&lt;code&gt;grep 表达式 目标文件&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;案例&lt;/p&gt;
&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>SEAN:Image Synthesis With Semantic Region-Adaptive Normalization</title>
    <link href="http://www.larryai.com/2022/05/16/SEAN/"/>
    <id>http://www.larryai.com/2022/05/16/SEAN/</id>
    <published>2022-05-16T09:53:19.000Z</published>
    <updated>2022-05-16T10:04:53.546Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="图片/fig1.png" alt="fig1" /><figcaption aria-hidden="true">fig1</figcaption></figure><h1 id="abstract">Abstract</h1><p>我们提出语义区域自适应归一化（SEAN）</p><p>这是生成对抗网络的一个简单但有效的构建块，其条件是<strong>分割掩码</strong>，描述了所需输出图像中的语义区域</p><p>使用 SEAN归一化，我们可以构建一个可以单独控制每个语义区域的样式的网络架构</p><p>例如，我们可以为每个区域指定一个样式参考图像</p><p>在重建质量、可变性和视觉质量方面，SEAN比以前最好的方法更适合编码、转移和合成风格</p><p>我们在多个数据集上评估 SEAN，并报告比当前技术水平更好的定量指标（例如FID、PSNR）</p><p>SEAN 还推动了<strong>交互式图像编辑</strong>的前沿</p><p>我们可以通过<strong>更改分割掩码或任何给定区域的样式来交互式地编辑图像</strong></p><p>我们还可以从每个区域的两个参考图像中插入样式</p><p>代码：https://github.com/ZPdesu/SEAN</p><h1 id="introduction">1. Introduction</h1><p>在本文中，我们使用条件生成对抗网络 (cGAN) 解决合成图像生成问题</p><p>具体来说，我们想使用分割掩码来控制生成图像的布局</p><p><strong>每个语义区域都有标签</strong>，并根据标签为每个区域“添加”逼真的风格</p><ul><li><p>面部生成应用程序将使用区域标签，如眼睛、头发、鼻子、嘴巴等</p></li><li><p>风景画应用程序将使用水、森林、天空、云等标签</p></li></ul><p>虽然存在多种非常好的框架来解决这个问题</p><p>目前最好的架构是 SPADE （也称为 GauGAN)。因此，我们决定使用 SPADE作为我们研究的起点。</p><p>通过分析 SPADE 结果，我们发现了我们希望在工作中改进的两个缺点。</p><p>首先</p><p><strong><em>SPADE仅使用一个样式代码来控制图像的整个样式，这不足以进行高质量的合成或细节控制</em></strong></p><p>例如，所需输出图像的分割掩码很容易包含输入风格图像的分割掩码中不存在的标记区域。</p><p>在这种情况下，缺失区域的样式是未定义的，这会产生低质量的结果</p><p>此外，SPADE 不允许对分割掩码中的每个区域使用不同样式的输入图像</p><p>因此，<strong>我们的第一个主要思想是单独控制每个区域的样式</strong></p><p>即<strong>我们提出的架构接受每个区域（或每个区域实例）的一个样式图像作为输入</strong></p><p>其次</p><p>我们认为<strong><em>仅在网络开头插入样式信息不是一个好的架构选择</em></strong></p><p>最近的架构 [26, 32, 2]已经证明，如果将样式信息作为归一化参数注入到网络中的多个层中，则可以获得更高质量的结果，例如使用 AdaIN</p><p>然而，这些先前的网络都没有使用样式信息来生成空间变化的归一化参数</p><p>为了缓解这个缺点，我们的第二个主要想法是设计一个标准化构建块，称为SEAN</p><p><strong>它可以使用样式输入图像为每个语义区域创建空间变化的归一化参数</strong></p><p>这项工作的一个重要方面是<strong>空间变化的归一化参数取决于分割掩码以及样式输入图像</strong></p><p>根据经验，我们在几个具有挑战性的数据集上对我们的方法进行了广泛的评估</p><p>CelebAMaskHQ 、CityScapes 、ADE20K 和我们自己的 Facades 数据集</p><p>定量地，我们评估我们在广泛的指标上的工作，包括 FID、PSNR、RMSE和分割性能</p><p>定性地，我们展示了可以通过视觉检查评估的合成图像的示例</p><p>我们的实验结果表明，与当前最先进的方法相比有了很大的改进</p><p>综上所述，我们引入了一种新的架构构建块 SEAN，它具有以下优点：</p><ul><li>SEAN 提高了条件 GAN 合成图像的质量。 我们与最先进的方法 SPADE 和Pix2PixHD 进行了比较，并在定量指标（例如 FID分数）和目视检查方面取得了明显的改进。</li><li>SEAN 改进了每个区域的风格编码，因此可以使重建的图像更类似于通过 PSNR和视觉检查测量的输入风格图像</li><li>SEAN 允许用户为每个语义区域选择不同风格的输入图像。与当前最先进的方法相比，这使图像编辑功能能够产生更高的质量并提供更好的控制。示例图像编辑功能是逐个区域的交互式风格转换和每个区域的风格插值（参见图1、2 和 5）</li></ul><figure><img src="图片/fig2.png" alt="fig2" /><figcaption aria-hidden="true">fig2</figcaption></figure><h1 id="related-work">2. Related Work</h1><h2 id="generative-adversarial-networks">2.1 Generative AdversarialNetworks</h2><p>自 2014 年推出以来，生成对抗网络 (GAN)已成功应用于各种图像合成任务</p><p>例如 图像修复 ，图像处理 和纹理合成</p><p>随着 GAN 架构 、损失函数 和 正则化 的不断改进 GAN合成的图像变得越来越逼真</p><p>例如，StyleGAN生成的人脸图像质量非常高，与未经训练的观看者的照片几乎没有区别</p><p>传统的 GAN 使用噪声向量作为输入，因此几乎不提供用户控制</p><p>这激发了条件GAN（cGAN）的发展，用户可以通过向生成器提供条件信息来控制合成</p><p>示例包括类标签 文本 和图像</p><p>我们的工作建立在具有图像输入的条件 GAN之上，旨在解决图像到图像的转换问题</p><h2 id="image-to-image-translation">2.2 Image-to-Image Translation</h2><p>图像到图像的转换是一个总括概念，可用于描述计算机视觉和计算机图形学中的许多问题</p><p>作为一个里程碑，Isola 等人 首先表明图像条件 GAN可以用作各种图像到图像转换问题的通用解决方案</p><p>从那时起，他们的方法通过以下几项工作扩展到场景，包括：</p><p>无监督学习 ，少样本学习 ，高分辨率图像合成 ，多模态图像合成和多模态图像合成 域图像合成</p><p>在各种图像到图像的翻译问题中，语义图像合成是一种特别有用的类型，因为它可以通过修改输入的语义布局图像来轻松控制</p><p>迄今为止，SPADE 模型（也称为 GauGAN）产生了最高质量的结果</p><p>在本文中，我们将通过引入 per-region 样式编码来改进 SPADE</p><h2 id="style-encoding">2.3 Style Encoding</h2><p>样式控制是各种图像合成和处理应用程序的重要组成部分</p><p>样式通常不是由用户手动设计的，而是从参考图像中提取的</p><p>在大多数现有方法中，样式在三个地方进行编码：</p><ul><li><p>图像特征的统计</p></li><li><p>神经网络权重（例如快速风格迁移）</p></li><li><p>网络标准化层的参数</p></li></ul><p>当应用于风格控制时</p><p>第一种编码方法通常很耗时，因为它需要一个缓慢的优化过程来匹配图像分类网络提取的图像特征的统计数据</p><p>第二种方法实时运行，但每个神经网络只编码所选参考图像的样式</p><p>因此，需要一个单独的神经网络为每个风格的图像进行训练，这限制了它在实践中的应用</p><p>迄今为止，第三种方法是最好的，因为它可以实时进行任意风格迁移</p><p>并且被 StyleGAN 、FUNIT 和 SPADE 等高质量网络使用</p><p>我们的按区域样式编码也建立在这种方法之上</p><p>我们将展示我们的方法产生更高质量的结果并实现更详细的用户控制。</p><h1 id="per-region-style-encoding-and-control">3. Per Region StyleEncoding and Control</h1><p>Per Region Style Encoding and Control 按区域样式编码和控制</p><p>给定一个输入风格图像及其对应的分割掩码，本节展示</p><ul><li><p>如何根据掩码从图像中提取每个区域的风格</p></li><li><p>如何使用提取的每个区域的风格代码来合成照片般逼真的图像</p></li></ul><h2 id="how-to-encode-style">3.1 How to Encode Style?</h2><p><strong>Per-Region Style Encoder(按区域样式编码器)</strong></p><p>为了提取每个区域的样式，我们提出了一种新颖的样式编码器网络</p><p>可以同时从输入图像的每个语义区域中提取相应的样式代码（参见图 4 (A)中的子网络样式编码器）</p><p>风格编码器的输出是一个 512×s 维度的风格矩阵 ST，其中 s是输入图像中语义区域的数量</p><p>矩阵的每一列对应一个语义区域的样式代码</p><p>与基于简单缩减卷积神经网络的标准编码器不同</p><p><strong>我们的（Per-Region StyleEncoder）每区域风格编码器采用“瓶颈”结构从输入图像中删除与风格无关的信息</strong></p><p>结合样式应该独立于语义区域形状的先验知识</p><p>我们将网络块 TConv-Layers 生成的中间特征图（512个通道）通过区域平均池化层并将它们减少到 512 个集合维向量</p><p>作为实现细节，我们要说明的是，我们使用 s作为数据集中语义标签的数量，并将输入图像中不存在的区域对应的列设置为0</p><p>作为这种架构的变体，我们还可以提取具有实例标签的数据集的每个区域实例的样式，例如 CityScapes</p><figure><img src="图片/fig4.png" alt="fig4" /><figcaption aria-hidden="true">fig4</figcaption></figure><h2 id="how-to-control-style">3.2 How to Control Style?</h2><p>以每个区域的<strong>样式代码</strong>和分割掩码作为输入</p><p>我们提出了一种<strong>新的条件归一化技术</strong>，称为<strong>语义区域自适应归一化(SEAN)</strong></p><p>以实现对逼真图像合成的样式的详细控制</p><p>与现有的归一化技术类似，SEAN 通过调制生成器激活的尺度和偏差来工作</p><p>与所有现有方法相比，<strong>SEAN学习的调制参数取决于样式代码和分段掩码</strong></p><p>在 SEAN 块中（图 3）</p><p><strong>样式映射</strong> 是第一代通过根据输入<strong>分割掩码</strong> 将 <strong>样式代码</strong>广播到其相应的语义区域来进行操作</p><p>然后将该 样式图 与 输入<strong>分割掩码</strong>一起<strong>通过两个单独的卷积神经网络来学习两组调制参数</strong></p><p>它们的加权和用作最终的 SEAN 参数，以调节生成器激活的尺度和偏差</p><p>权重参数在训练期间也是可学习的。 SEAN的正式定义介绍如下</p><figure><img src="图片/fig3.png" alt="fig3" /><figcaption aria-hidden="true">fig3</figcaption></figure><p><strong>Semantic Region-Adaptive Normalization(SEAN)</strong>（语义区域自适应归一化）</p><p>一个 SEAN 块有两个输入：</p><ul><li><p>一个编码每个区域样式代码的<strong>样式矩阵 ST</strong></p></li><li><p>一个<strong>分割掩码 M</strong></p></li></ul><p>让 h 表示在深度卷积网络中当前 SEAN 块的输入激活，用于一批 N个样本</p><p>令 H、W 和 C 为激活图中的高度、宽度和通道数</p><p>位点 (n ∈ N, c ∈ C, y ∈ H, x ∈ W ) 处的调制激活值由下式给出 <spanclass="math display">\[\gamma_{c, y, x}(\mathbf{S T}, \mathbf{M}) \frac{h_{n, c, y,x}-\mu_{c}}{\sigma_{c}}+\beta_{c, y, x}(\mathbf{S T}, \mathbf{M})\]</span></p><ul><li><span class="math inline">\(h_{n, c, y, x}\)</span> 是标准化前</li><li>调制参数<span class="math inline">\(\gamma_{c, y, x}\)</span>和<spanclass="math inline">\(\beta_{c, y, x}\)</span>是 <spanclass="math inline">\(\gamma_{c, y, x}^{s}, \gamma_{c, y,x}^{o}\)</span> 和 <span class="math inline">\(\beta_{c, y, x}^{s},\beta_{c, y, x}^{o}\)</span>的加权和</li></ul><p><span class="math display">\[\begin{aligned}&amp;\gamma_{c, y, x}(\mathbf{S T}, \mathbf{M})=\alpha_{\gamma}\gamma_{c, y, x}^{s}(\mathbf{S T})+\left(1-\alpha_{\gamma}\right)\gamma_{c, y, x}^{o}(\mathbf{M}) \\&amp;\beta_{c, y, x}(\mathbf{S T}, \mathbf{M})=\alpha_{\beta} \beta_{c,y, x}^{s}(\mathbf{S T})+\left(1-\alpha_{\beta}\right) \beta_{c, y,x}^{o}(\mathbf{M})\end{aligned}\]</span></p><ul><li><span class="math inline">\(\mu_c\)</span> 和 <spanclass="math inline">\(\sigma_{c}\)</span>是在通道c上的激活均值和标准差<span class="math display">\[\begin{gathered}\mu_{c}=\frac{1}{N H W} \sum_{n, y, x} h_{n, c, y, x} \\\sigma_{c}=\sqrt{\frac{1}{N H W}\left(\sum_{n, y, x} h_{n, c, y,x}^{2}\right)-\mu_{c}^{2}}\end{gathered}\]</span></li></ul><h1 id="experimental-setup">4. Experimental Setup</h1><h2 id="network-architecture">4.1 Network Architecture</h2><figure><img src="图片/fig4.png" alt="fig4" /><figcaption aria-hidden="true">fig4</figcaption></figure><p>图 4 (A) 显示了我们的生成器网络的概述，它建立在 SPADE 的基础上</p><p>我们<strong>使用了一个由几个带有上采样层的 SEAN ResNet 块 (SEANResBlk) 组成的生成器</strong></p><p><strong>SEAN ResBlk</strong></p><p>图 4 (B) 显示了我们的 SEAN ResBlk 的结构</p><p>它由三个卷积层组成，其尺度和偏差分别由三个 SEAN 块调制</p><p>每个 SEAN 块有两个输入：</p><ul><li><p>一组每个区域的样式代码 ST</p></li><li><p>一个语义掩码 M</p></li></ul><p>请注意，两个输入都在开始时进行了调整：</p><ul><li><p><strong>输入分割掩码被下采样到与特征图相同的高度和宽度层</strong></p></li><li><p><strong>来自 ST 的输入样式代码使用 1 × 1 卷积层 Aij对每个区域进行转换</strong></p></li></ul><p>我们观察到初始转换是架构中不可或缺的组成部分，因为它们根据每个神经网络层的不同角色转换样式代码</p><p>例如，早期层可能控制人脸图像的发型（例如波浪形、直发），而后面的层可能涉及照明和颜色</p><p>此外，我们观察到在 SEAN 的输入中添加噪声可以提高合成图像的质量</p><p><strong>这种噪声的大小由训练期间学习的每通道缩放因子 B进行调整</strong>，类似于 StyleGAN</p><h2 id="training-and-inference">4.2 Training and Inference</h2><h3 id="training">4.2.1 Training</h3><p><strong>我们将训练制定为图像重建问题</strong></p><p>也就是说，<strong>风格编码器被训练为根据其对应的分割掩码从输入图像中提取每个区域的风格代码</strong></p><p>生成器网络经过训练，以<strong>提取的每个区域样式代码和相应的分割掩码作为输入来重建输入图像</strong></p><p>继 SPADE 和 Pix2PixHD之后，<strong>输入图像和重建图像之间的差异通过由三个损失项组成的整体损失函数来衡量</strong>：</p><ul><li><p>条件对抗损失</p></li><li><p>特征匹配损失</p></li><li><p>感知损失</p></li></ul><p>损失函数的详细信息包含在补充材料中</p><h3 id="inference">4.2.2 Inference</h3><p>在推理过程中，我们将任意分割掩码作为掩码输入</p><p>并实现每个区域的样式通过为每个语义区域选择单独的 512维样式代码作为样式输入来进行控制</p><p>这使得各种高质量的图像合成应用成为可能，这将在下一节中介绍</p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;图片/fig1.png&quot; alt=&quot;fig1&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;fig1&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>图像的三种数据格式</title>
    <link href="http://www.larryai.com/2022/05/16/%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%89%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/"/>
    <id>http://www.larryai.com/2022/05/16/%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%89%E7%A7%8D%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/</id>
    <published>2022-05-16T01:56:13.000Z</published>
    <updated>2022-05-16T14:09:51.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像的三种数据格式">1.图像的三种数据格式</h1><ul><li>numpy</li><li>tensor</li><li>PIL.Image</li></ul><h1 id="图像格式的转换基础转换">2.图像格式的转换——基础转换</h1><h2 id="numpy与tensor的转换">numpy与tensor的转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor_data = torch.tensor(np_data)</span><br><span class="line">np_data = np.array(tensor_data)</span><br></pre></td></tr></table></figure><h2 id="numpy与pil.image的转换">numpy与PIL.Image的转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">np_data = np.array(PIL_img, dtype=np.uint8)</span><br><span class="line">PIL_img = Image.fromarray(numpy_img)</span><br></pre></td></tr></table></figure><h2 id="pil.image与tensor的转换">PIL.Image与tensor的转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms </span><br><span class="line"></span><br><span class="line">tensor_img  = transforms.ToTensor()(PIL_img)</span><br><span class="line">PIL_img = transforms.ToPILImage()(tensor_img) </span><br></pre></td></tr></table></figure><h1 id="不同图像格式的保存">3.不同图像格式的保存</h1><h2 id="numpy格式图像保存">numpy格式图像保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"></span><br><span class="line">save_path = <span class="string">&#x27;&#x27;</span></span><br><span class="line">np_img = <span class="string">&#x27;&#x27;</span></span><br><span class="line">cv.imwrite(save_path , np_img)</span><br></pre></td></tr></table></figure><h2 id="pil.image格式图像保存">PIL.Image格式图像保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PIL_img.save(save_path)</span><br></pre></td></tr></table></figure><h2 id="tensor格式的图像保存">tensor格式的图像保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line">save_image(tensor_img , <span class="string">&#x27;save_path&#x27;</span>)</span><br></pre></td></tr></table></figure><h1id="深度学习图像生成情况下的保存图像">4.深度学习图像生成情况下的保存图像</h1><h2 id="得到pil.image图像">得到PIL.Image图像</h2><ul><li>创建生成网络G</li><li>输入图像img（PIL.Image格式）</li><li>print工具</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># print工具 定义输出size</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_print_tool</span>(<span class="params">size=<span class="number">512</span></span>):</span><br><span class="line">    <span class="keyword">return</span> torch.hub.load(<span class="string">&quot;bryandlee/animegan2-pytorch:main&quot;</span>, <span class="string">&quot;face2paint&quot;</span>, size=size)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 生成网络</span></span><br><span class="line">  G = <span class="literal">None</span></span><br><span class="line">  <span class="comment"># 输入图像 （由路径获得）</span></span><br><span class="line">  PIL_img  = Image,<span class="built_in">open</span>(path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">  out_imgPIL = imgpaint(netG, PIL_img)</span><br></pre></td></tr></table></figure><p>得到PIL.Image格式图片后 ， 接下来有两者保存方式</p><p>1.直接保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_imgPIL.save(save_path)</span><br></pre></td></tr></table></figure><p>2.变成numpy格式（方便拼接）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img_np = np.array(out_imgPIL) <span class="comment">#变成numpy格式</span></span><br><span class="line">img_bgr = cv.cvtColor(img_np, cv.COLOR_RGB2BGR) <span class="comment">#变成BGR格式</span></span><br><span class="line"><span class="comment"># 一系列别的操作 #</span></span><br><span class="line">cv.imwrite(path,img_bgr)</span><br></pre></td></tr></table></figure><h2 id="得到tensor变量">得到tensor变量</h2><p>得到tensor格式图片后 ， 接下来有两者保存方式</p><ul><li>直接用tensor的保存方式</li><li>变为numpy进行一系列操作后保存【同上】</li></ul><h1 id="web端部署是处理的情况">5.web端部署是处理的情况</h1><p>在web端部署深度学习图像相关的应用时，我们需要处理从web获取图片并且返回图像的问题</p><p>1.从web获取图像变成PIL格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(BytesIO(image)).convert(<span class="string">&quot;RGB&quot;</span>)</span><br></pre></td></tr></table></figure><p>2.由numpy格式的图像返回到web</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确保是BGR格式的 如果不是 参考上面的代码</span></span><br><span class="line">img_result = cv.imencode(<span class="string">&#x27;.jpg&#x27;</span>, img_bgr)[<span class="number">1</span>].tostring()</span><br><span class="line">img_result = base64.b64encode(img_result).decode()</span><br><span class="line"><span class="comment"># 将img_result返回即可</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图像的三种数据格式&quot;&gt;1.图像的三种数据格式&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;li&gt;tensor&lt;/li&gt;
&lt;li&gt;PIL.Image&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;图像格式的转换基础转换&quot;&gt;2.图像格式的转换——基础转换&lt;/h1&gt;
&lt;</summary>
      
    
    
    
    
    <category term="Python" scheme="http://www.larryai.com/tags/Python/"/>
    
    <category term="Pytorch" scheme="http://www.larryai.com/tags/Pytorch/"/>
    
    <category term="Numpy" scheme="http://www.larryai.com/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>OPENCV-视频与图像的转换</title>
    <link href="http://www.larryai.com/2022/05/16/video2pic/"/>
    <id>http://www.larryai.com/2022/05/16/video2pic/</id>
    <published>2022-05-16T01:27:34.000Z</published>
    <updated>2022-05-16T01:39:36.654Z</updated>
    
    <content type="html"><![CDATA[<h2 id="video2pics">video2pics</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">video_path = <span class="literal">None</span>  <span class="comment"># 视频来源路径</span></span><br><span class="line">save_path = <span class="literal">None</span>  <span class="comment"># 图片保存路径</span></span><br><span class="line">cap = cv.VideoCapture(video_path)  <span class="comment"># 创建视频捕捉对象</span></span><br><span class="line">fps = <span class="built_in">int</span>(cap.get(cv.CAP_PROP_FPS))  <span class="comment"># 获取帧率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;FPS:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(fps))</span><br><span class="line"><span class="comment"># 获取视频总长</span></span><br><span class="line">rate = cap.get(<span class="number">5</span>)</span><br><span class="line">frame_num = cap.get(<span class="number">7</span>)</span><br><span class="line">duration = frame_num / rate</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;video total time:&#123;:.2f&#125;s&#x27;</span>.<span class="built_in">format</span>(duration))</span><br><span class="line"></span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line">num = <span class="number">0</span></span><br><span class="line">interval = <span class="number">1</span>  </span><br><span class="line">process_num = frame_num // interval</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;process frame:&#123;:.0f&#125;&#x27;</span>.<span class="built_in">format</span>(process_num))</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"><span class="keyword">while</span> cap.isOpened():</span><br><span class="line">    ret, frame = cap.read()</span><br><span class="line">    <span class="keyword">if</span> ret:</span><br><span class="line">        cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cnt % interval == <span class="number">0</span>:</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># frame = cv.resize(frame, (width, height))</span></span><br><span class="line">            cv.imwrite(save_path + <span class="string">&quot;/%d.jpg&quot;</span> % num, frame)</span><br><span class="line">            remain_frame = process_num - num</span><br><span class="line">            t1 = time.time() - t0</span><br><span class="line">            t0 = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Processing %d.jpg, remain frame: %d, remain time: %.2fs&quot;</span> % (num, remain_frame, remain_frame * t1))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="pics2video">pics2video</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">fps = <span class="number">30</span>  <span class="comment"># 帧率</span></span><br><span class="line">photo_size = (<span class="number">828</span>, <span class="number">456</span>)  <span class="comment"># 尺寸大小</span></span><br><span class="line">fourcc = cv2.VideoWriter_fourcc(*<span class="string">&#x27;X264&#x27;</span>)  <span class="comment"># 格式</span></span><br><span class="line">video = <span class="string">&#x27;xxx.mov&#x27;</span>  <span class="comment"># 保存的视频路径</span></span><br><span class="line">videoWriter = cv2.VideoWriter(video, fourcc, fps, photo_size)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    image = <span class="string">&quot;out/gl&quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;.jpg&quot;</span>  <span class="comment"># 图片路径</span></span><br><span class="line">    frame = cv2.imread(image)</span><br><span class="line">    videoWriter.write(frame)</span><br><span class="line">videoWriter.release()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;video2pics&quot;&gt;video2pics&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span</summary>
      
    
    
    
    
    <category term="OPENCV" scheme="http://www.larryai.com/tags/OPENCV/"/>
    
  </entry>
  
  <entry>
    <title>Linux基本指令1</title>
    <link href="http://www.larryai.com/2022/05/14/Linux%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/"/>
    <id>http://www.larryai.com/2022/05/14/Linux%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/</id>
    <published>2022-05-14T07:32:55.000Z</published>
    <updated>2022-05-31T12:33:12.070Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本概念">基本概念</h1><h2 id="标准文件">标准文件</h2><ul><li>stdin 标准输入</li><li>stdout 标准输出</li><li>stderror 标准错误</li></ul><h1 id="基本命令">基本命令</h1><h2 id="cat命令">Cat命令</h2><p>拼接文件</p><blockquote><p>cat命令的功能是拼接文件 ， cat并不适合查看文件</p><p>因为当cat命令默认重定向到标准输出stdout ，所以能看到显示屏上看到文件</p></blockquote><p>当文件内容较多时，cat命令显示文件就会快速滚屏，且没有滚动条</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> 1.txt 2.txt &gt; 3.txt</span><br></pre></td></tr></table></figure><h2 id="more命令">more命令</h2><p>查看文件</p><blockquote><p>但是只能向后翻页</p></blockquote><h2 id="lesszless命令">less/zless命令</h2><p>查看文件内容</p><h2 id="cd命令">cd命令</h2><p>前往目录/文件</p><h3 id="绝对路径">绝对路径</h3><p>路径以‘/’开始</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/username</span><br></pre></td></tr></table></figure><h3 id="相对路径">相对路径</h3><p>路径以‘..’开始</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../src/linux</span><br></pre></td></tr></table></figure><h2 id="pwd命令">pwd命令</h2><p>以绝对路径的形式显示当前位置</p><h2 id="ls命令">ls命令</h2><p>以列表的形式显示当前目录的内容（目录项）</p><h4 id="基本选项">基本选项</h4><ul><li><p><code>ls -i</code></p><p>显示文件的索引节点号</p><p>共享文件就是共享同一个索引节点号</p></li><li><p><code>ls -a</code></p><p>不忽略以‘ . ’开头的目录项</p></li><li><p><code>ls -l</code></p><p>以长格式显示信息</p></li></ul><figure><img src="ls-l.png" alt="ls-l" /><figcaption aria-hidden="true">ls-l</figcaption></figure><h2 id="cp命令">cp命令</h2><p>复制文件和目录 <code>cp  源  目的</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line"><span class="built_in">cp</span> file1.txt file2.txt</span><br><span class="line"><span class="comment"># 复制目录</span></span><br><span class="line"><span class="built_in">cp</span> -r dir1 dir2</span><br></pre></td></tr></table></figure><blockquote><p><code>-r</code> 递归地复制目录 ， 因为目录里面可能还有目录</p></blockquote><h2 id="touch命令">touch命令</h2><p>改变时间戳的命令</p><blockquote><p>使用<code>touch</code> 会改变文件的 <strong>访问时间(AccessTime)</strong>和 <strong>修改时间(modification time)</strong>都改为当前时间</p></blockquote><ul><li><code>-a</code>只改变访问时间</li><li><code>-m</code>只改变修改时间</li><li><code>-d</code>指定要修改的时间</li><li><code>-t</code>指定要修改的时间</li></ul><p>当<code>touch</code>命令后面的文件不存在时，创建一个空文件</p><h2 id="mv命令">mv命令</h2><p>移动文件或重命名</p><blockquote><p>位置相同-重命名</p><p>位置不同-移动文件</p></blockquote><h2 id="mkdir命令">mkdir命令</h2><p>创建新目录</p><h2 id="rmdir命令">rmdir命令</h2><p>删除空目录</p><h2 id="rm命令">rm命令</h2><ul><li>删除文件 <code>rm filename</code></li><li>删除目录 <code>rm -r dirname</code></li></ul><h2 id="ln命令">ln命令</h2><p>创建共享文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> file1 file2</span><br><span class="line"></span><br><span class="line"><span class="built_in">ln</span> -s file1 file3 <span class="comment">#file3只记录了file1的位置</span></span><br></pre></td></tr></table></figure><figure><img src="ln.png" alt="ln" /><figcaption aria-hidden="true">ln</figcaption></figure><p><code>ln</code> 如果不带有任何选项 -&gt;创建硬链接</p><ul><li>i节点相同</li><li>实际为同文件<ul><li>内容相同</li><li>修改同时</li></ul></li></ul><p><code>ln -s</code>创建软链接共享文件</p><p>软链接与硬链接的对比</p><ul><li>i节点不同</li><li>对文件的链接数量影响不同</li><li>删除原文件结果影响不同<ul><li>删除file1.txt file2.txt还在 但是file3.txt没有</li></ul></li><li>软链接文件类型是l</li><li>软连接权限都是rwx</li></ul><h2 id="file命令">file命令</h2><p>查看文件的类型</p><h2 id="type命令">type命令</h2><p>展示命令的信息</p><h2 id="stat命令">stat命令</h2><p>查看 文件/文件系统 的状态</p><h2 id="find命令">find命令</h2><p>查找文件</p><blockquote><p>根据文件名的查找格式</p><p><code>find  起始位置 -name 文件名</code></p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find ./home fins -name hello.c</span><br></pre></td></tr></table></figure><p>更加复杂的案例</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find ~/mike ~/bill -size +1000 -atime 30 -ok <span class="built_in">rm</span> &#123;&#125; \;</span><br></pre></td></tr></table></figure><ul><li><p>两个起始位置 <code>/mike</code> <code>/bill</code></p></li><li><p><code>-size</code>选项表示根据文件大小查找</p><figure><img src="find-size.png" alt="find-size" /><figcaption aria-hidden="true">find-size</figcaption></figure></li><li><p><code>-atime</code>表示根据文件的最后访问时间进行查找单位为天</p></li><li><p><code>-ok</code> 与<code>-exec</code>相似 执行后面的命令</p><ul><li>ok会询问 exec不会询问</li><li>如果命令运行的 他的standard input重定向到<code>/dev/null</code></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基本概念&quot;&gt;基本概念&lt;/h1&gt;
&lt;h2 id=&quot;标准文件&quot;&gt;标准文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;stdin 标准输入&lt;/li&gt;
&lt;li&gt;stdout 标准输出&lt;/li&gt;
&lt;li&gt;stderror 标准错误&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;基本命令&quot;&gt;基本命</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>StyTR2</title>
    <link href="http://www.larryai.com/2022/05/06/StyTR2/"/>
    <id>http://www.larryai.com/2022/05/06/StyTR2/</id>
    <published>2022-05-06T12:18:40.000Z</published>
    <updated>2022-05-07T00:11:38.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p><strong><em>风格转换的目标是在保持原有内容的同时，在风格参照的指导下呈现出具有艺术特征的图像</em></strong></p><p>由于卷积神经网络（CNN）的局部性，很难提取和维护输入图像的全局信息。</p><p>因此，<strong>传统的神经风格转换方法面临着有偏见的内容表示</strong></p><p>为了解决这个关键问题，我们提出了一种基于转换器的方法，称为StyTr2 &gt;将输入图像的长期依赖性考虑到图像样式传输中</p><p>与其他视觉任务的视觉转换器不同<strong>StyTr2包含两个不同的转换器编码器，分别为内容和样式生成特定于域的序列</strong>在编码器之后<strong>采用多层转换器解码器根据样式序列对内容序列进行样式化</strong></p><p>我们还分析了现有位置编码方法的不足，提出了<strong>内容感知位置编码（CAPE）</strong>&gt; 它具有尺度不变性，更适合于图像样式传输任务</p><p>定性和定量实验表明，与最先进的基于CNN和基于流的方法相比，所提出的StyTr2是有效的。</p><p>代码和模型可在https://github.com/diyiiyiii/StyTR-2</p><h1 id="introduce">1.Introduce</h1><p>此处省略一些内容 详细可查看原论文</p><p>总之，我们的主要贡献包括 -一个名为StyTr2的基于转换器的风格转换框架，以生成风格化结果，并保留输入内容图像的结构和细节- 一种基于内容的位置编码方案，具有尺度不变性，适用于样式转换任务 -综合实验表明，StyTr2形成了基线方法，并以理想的内容结构和风格模式取得了显著的效果</p><h1 id="relate-work">2.Relate Work</h1><ul><li>图像风格迁移</li><li>视觉任务的transformer &gt;在本文中，我们介绍了用于样式转换任务的基于变换器的结构，可以将其视为图像块的序列到序列生成</li><li>位置编码 &gt;在本文中，我们提出了一种基于内容的位置编码机制，该机制具有尺度不变性，更适合于图像生成任务</li></ul><h1 id="method">3.Method</h1><p>为了利用transformers的功能捕获图像特征的长距离依赖性以进行样式转换</p><p>我们将该问题描述为一个连续的补丁生成任务</p><p>给定一个内容图像image (H,W,3) 并显示一个样式图像style (H,W,3)</p><p>我们将两幅图像分割成块patch（类似于NLP任务中的标记）</p><p>使用线性投影层将输入块投影到型如L×dim中嵌入 <spanclass="math inline">\(\varepsilon\)</span> 的序列特征中</p><p><span class="math display">\[L = \frac{H\times W}{m\times m}\]</span></p><blockquote><p>L是特征序列的长度</p><p>m=8是patches的size</p><p>dim是特征序列的维度</p></blockquote><h2 id="content-aware-positional-encodingcape">3.1Content-AwarePositional Encoding（CAPE）</h2><p>当使用transformer-based的模型时，位置编码（PE）应包含在输入序列中，以获取结构信息</p><p>第i个patch和第j个patch的注意力得分计算如下：</p><p><img src="公式1.png" /> &gt; Wq 用于查询的参数矩阵 &gt;Wk用于密钥计算的参数矩阵 &gt; Pi 第i个一维的PE</p><blockquote><p>在二维情况下两个像素点(xi,yi) (xj,yj)之间的位置相对关系:</p></blockquote><p><img src="公式2.png" /></p><ul><li><span class="math inline">\(w_{k} =\frac{1}{1000^{(\frac{2k}{128})}}\)</span></li><li>d = 512</li></ul><p><strong>两个patch之间的位置相对关系仅取决于它们的空间距离</strong></p><p>因此，我们提出两个问题:</p><p><strong>第一</strong></p><p>对于图像生成任务，在计算PE时是否应该考虑图像语义？</p><p>传统的PE是为按逻辑排列的句子设计的，但图像补丁是根据内容组织的。</p><p>我们将两个patch之间的距离表示为d( · , · )</p><p><img src="fig3.png" /></p><p>在图3(a)的右侧</p><p><strong>d((x0,y3),(x1,y3))</strong>(红色和绿色补丁) 和<strong>d((x0,y3),(x3,y3))</strong>(红色和青色补丁)之间的差异应该很小</p><blockquote><p>因为我们预计类似的内容patch会有类似的样式化结果</p></blockquote><p><strong>第二</strong></p><p>当输入图像的大小呈指数增长时传统的正弦位置编码是否仍然适用于视觉任务？</p><p>如图3(a)所示</p><p>调整图像大小时相同位置的面片（用蓝色小矩形表示）之间的相对距离可能会发生显著变化</p><p>这可能不适用于视觉任务中的多尺度方法</p><p>为此，我们提出了<strong>内容感知位置编码（CAPE）</strong></p><blockquote><p>它是尺度不变的，更适合风格迁移任务</p></blockquote><p>与仅考虑补丁相对距离的正弦 PE 不同，CAPE 以图像内容的语义为条件</p><p>我们假设使用 n × n 位置编码足以表示图像的语义</p><p>对于图像 <span class="math inline">\(I \in \mathbb{R}^{H \times W\times 3}\)</span> , 我们将固定的 n × n 位置编码重新缩放为<spanclass="math inline">\(\frac{H}{m} \times \frac{H}{m}\)</span> ，如图3(b) 所示</p><p>这样，<strong>各种图像尺度就不会影响两个补丁之间的空间关系</strong></p><p>补丁 (x, y) 的 CAPE 即<strong>PCA(x, y)</strong>被表述为</p><p><img src="公式3.png" /></p><ul><li><span class="math inline">\(AvgPool_{n\times n}\)</span>是平均池化函数</li><li><span class="math inline">\(\mathcal{F}_{pos}\)</span> 是 1 × 1卷积运算，用作可学习的位置编码函数</li><li><span class="math inline">\(\mathcal{P}_{\mathcal{L}}\)</span>是遵循序列<spanclass="math inline">\(\varepsilon\)</span>的可学习PE</li><li>在我们的实验中n 设置为 18</li><li>$a_{kl} $是插值权重，s 是相邻块的数量</li><li>最后，我们将 <span class="math inline">\(P_{CA_{i}}\)</span> 添加到<span class="math inline">\(\varepsilon_{i}\)</span>，作为第 i个补丁在像素位置 (x, y) 的最终特征嵌入</li></ul><h2 id="style-transfer-transformer">3.2 Style Transfer Transformer</h2><h4 id="transformer-编码器">3.2.1 Transformer 编码器</h4><p>我们通过使用基于 Transformer 的结构来学习<strong>顺序视觉表示</strong> 来<strong>捕获图像块的长期依赖关系</strong></p><p>与其他视觉任务不同，tjr风格迁移任务的输入来自两个不同的领域，分别对应于自然图像和艺术绘画</p><p>因此，StyTr2有两个转换器编码器来<strong>编码特定领域的特征</strong>，用于在下一阶段将序列从一个域转换到另一个域</p><p>给定输入内容序列 <span class="math inline">\(Z_c = \{\varepsilon_{ci} + \mathcal{P_{CA}}_i\}\)</span> 的嵌入</p><p>我们首先将其输入到转换器编码器中</p><p>编码器的每一层都由一个多头自注意力模块（MSA）和一个前馈网络（FFN）组成</p><p>输入序列被编码为查询（Q）、键（K）和值（V）： <spanclass="math display">\[Q =Z_cW_q , K=Z_cW_k , V=Z_cW_v\]</span></p><ul><li><span class="math inline">\(W_q , W_k , W_v \in \mathbb{R}^{C \timesd_{head}}\)</span></li></ul><p>multi-head Attention的计算方式</p><p><img src="4.png" /></p><ul><li><span class="math inline">\(W_0 \in \mathbb{R}^{C \timesC}\)</span>是可学习的参数</li><li>N 是注意力头的数量，并且 <span class="math inline">\(d_{head} =\frac{C}{N}\)</span></li></ul><p>应用残差连接来获得编码的内容序列 Yc</p><p><img src="5.png" /></p><ul><li>FFN是激活函数为relu的MLP</li><li>LN被应用到每一个块的末尾</li></ul><p>类似地，输入样式序列 Zs = {Es1, Es2, ..., EsL}的嵌入按照相同的计算过程编码为序列 Ys</p><p>只是不考虑位置编码，因为我们不需要维护 最终输出中的输入样式</p><h4 id="transformer-解码器">3.2.2 Transformer 解码器</h4><p>我们的转换器解码器用于根据编码样式序列 Ys 以回归方式翻译编码内容序列Yc</p><p>与 NLP任务中的自回归过程不同，我们一次将所有顺序补丁作为输入来预测输出</p><p>如图 3(a) 所示，每个 Transformer 解码器层包含两个 MSA 层和一个FFN</p><p>我们的 Transformer 解码器的输入包括编码后的内容序列</p><p>即 <span class="math inline">\(\bar{Y_c}\)</span> = {Yc1 + PCA1, Yc2+ PCA2, ..., YcL + PCAl} 以及样式序列 Ys = {Ys1, Ys2, ..., YSL}</p><blockquote><p>我们使用<strong>内容序列生成查询 Q，并使用样式序列生成键 K 和值V</strong></p></blockquote><p><span class="math display">\[Q =\bar{Y_c}W_q , K=Y_sW_k , V=Y_sW_v\]</span></p><p>然后，transformer解码器的输出序列X可以计算为</p><p><img src="6.png" /></p><h4 id="cnn解码器">3.2.3 CNN解码器</h4><p>Transformer 的输出序列 X 的形状为 【HW/64 , C】</p><p>我们没有直接对输出序列进行上采样来构造最终结果</p><p>而是使用三层 CNN 解码器来细化 Transformer 解码器的输出</p><p>对于每一层，我们通过采用包括 3 × 3 Conv + ReLU + 2 × Upsample在内的一系列操作来扩大规模</p><p>最后，我们可以得到分辨率为 H × W × 3 的最终结果</p><h2 id="network-optimization">3.3 Network Optimization</h2><p>生成的结果应保持原始内容结构和参考样式模式</p><p>因此，我们构造了两个不同的感知损失项来衡量</p><ul><li><p>输出图像 Io 和输入内容图像 Ic之间的<strong>内容差异</strong></p></li><li><p>Io 和输入风格参考 Is 之间的<strong>风格差异</strong></p></li></ul><p>我们使用由预训练的 VGG模型提取的特征图来构建之后的内容损失和样式损失</p><p>内容感知损失 Lc 定义为</p><p><img src="7.png" /></p><ul><li>其中<span class="math inline">\(\phi_i()\)</span> 表示从预训练 VGG19中的第 i 层提取的特征，<span class="math inline">\(N_l\)</span>是层数。</li></ul><p>风格感知损失 Ls 定义为</p><p><img src="8.png" /></p><ul><li>其中 μ(·) 和 σ(·) 分别表示提取特征的均值和方差。</li></ul><p>我们还采用<strong>身份损失</strong>来学习更丰富、更准确的内容和风格表示</p><p>具体来说，我们将两个相同的内容（风格）图像放入 StyTr2，生成的输出Icc(Iss) 应该与输入 Ic(Is) 相同</p><p>因此，我们计算两个身份损失项来衡量 Ic(Is) 和 Icc(Iss)之间的差异：</p><p><img src="9.png" /></p><p>通过最小化以下函数来优化整个网络：</p><p><img src="10.png" /></p><p>我们将 λc、λs、λid1 和 λid2 设置为 10、7、50 和1，以减轻幅度差异的影响</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;风格转换的目标是在保持原有内容的同时，在风格参照的指导下呈现出具有艺术特征的图像&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;由于卷积神经网络（CNN）的局部性，很难提取和维护输入图像的全</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://www.larryai.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Pycharm快捷键</title>
    <link href="http://www.larryai.com/2022/05/06/Pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://www.larryai.com/2022/05/06/Pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/</id>
    <published>2022-05-06T10:54:14.000Z</published>
    <updated>2022-05-06T11:32:16.665Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pycharm-for-mac">Pycharm for Mac</h1><h4 id="l-使代码遵守pep8规范自动整齐代码">⌘ + ⌥ + L使代码遵守pep8规范（自动整齐代码）</h4><h4 id="b-查看申明-查看源代码">⌘ + B 查看申明 / 查看源代码</h4><h4 id="w-关闭当前文件标签">⌘ + W 关闭当前文件标签</h4><h4 id="f-查找">⌘ + F 查找</h4><h4 id="r-替换">⌘ + R 替换</h4><h4 id="k-更新到vcs">⌘ + K 更新到VCS</h4><h4 id="d-复制当前一整行">⌘ + D 复制当前一整行</h4><h4 id="x-剪贴当前一整行">⌘ + X 剪贴当前一整行</h4><h4 id="v-粘贴缓存历史粘贴板全部内容">⌘ + ⇧ + V粘贴缓存（历史粘贴板全部内容）</h4><h4 id="o-查找类名">⌘ + O 查找类名</h4><h4 id="a-查找动作">⌘ + ⇧ + A 查找动作</h4><h4 id="p-参数提示">⌘ + P 参数提示</h4><h4 id="折叠代码">⌘ + - 折叠代码</h4><h4 id="展开代码">⌘ + = 展开代码</h4><h4 id="tab-切换文件窗口">⌃ + Tab 切换文件窗口</h4><h4 id="r-运行">⌃ + R 运行</h4><h4 id="r-换文件运行">⌃ + ⌥ + R 换文件运行</h4><h4 id="d-启动调试">⌃ + D 启动调试</h4><h4 id="h-调用层次结构">⌃ + ⌥ + H 调用层次结构</h4><h4 id="f6-重命名">⇧ + F6 重命名</h4><h4 id="回车-下一行">⇧ + 回车 下一行</h4><h4 id="选择代码开始处-选择代码结束处">⌘ + ⇧ + ⌥ + [ 选择代码开始处]选择代码结束处</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pycharm-for-mac&quot;&gt;Pycharm for Mac&lt;/h1&gt;
&lt;h4 id=&quot;l-使代码遵守pep8规范自动整齐代码&quot;&gt;⌘ + ⌥ + L
使代码遵守pep8规范（自动整齐代码）&lt;/h4&gt;
&lt;h4 id=&quot;b-查看申明-查看源代码&quot;&gt;⌘ + B 查看</summary>
      
    
    
    
    <category term="高效工具" scheme="http://www.larryai.com/categories/%E9%AB%98%E6%95%88%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="快捷键" scheme="http://www.larryai.com/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    
    <category term="Pycharm" scheme="http://www.larryai.com/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>ViT代码复现</title>
    <link href="http://www.larryai.com/2022/05/06/ViT/"/>
    <id>http://www.larryai.com/2022/05/06/ViT/</id>
    <published>2022-05-06T07:58:39.000Z</published>
    <updated>2022-05-06T12:22:57.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="patch-embedding模块">Patch-Embedding模块</h2><h3 id="section"><img src="2.png" /></h3><h3 id="转换流程">转换流程</h3><p>在<strong>Patch Embedding</strong>中</p><p>例如输入图片大小为256x256，将图片分为多个patch，每个patch大小为16x16</p><p>则每张图像会生成256x256/16x16=256个patches</p><ul><li>即输入序列长度L为patch_num = 256</li><li>每个patch维度dim = patchSize x patchSize x chanels=16x16x3 =768</li></ul><blockquote><p>输入的图像原始格式 [B , C , H ,W]</p><p>经过Patch-Embeding之后格式变为 [B , L , dim]</p><p>用上面的例子来讲便是[B , 3 , 256 ,256] -&gt; [B , 256 , 768]</p></blockquote><p><strong>线性投射层</strong>的维度为768xN(N=768)，因此输入通过线性投射层之后的维度依然为256x768</p><p>即一共有256个token，每个token的维度是768</p><p>代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">patch_embedding = nn.Sequential(</span><br><span class="line">  </span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2 = patch_width),</span><br><span class="line">  </span><br><span class="line">            nn.Linear(patch_dim, dim), <span class="comment">#线性投影层</span></span><br><span class="line">  </span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>最后还需要加上一个特殊字符cls，因此最终的维度是<strong>256x768</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = atch_embedding(img) <span class="comment">#对输入图像进行patch-embedding</span></span><br><span class="line">b, n, _ = x.shape <span class="comment">#获取Batch以及token数量</span></span><br><span class="line">cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim)) <span class="comment">#特殊字符cls</span></span><br><span class="line">cls_tokens = repeat(cls_token, <span class="string">&#x27;1 n d -&gt; b n d&#x27;</span>, b = b) <span class="comment">#将cls展到相等Batch</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>) <span class="comment">#进行拼接 token数量+1</span></span><br></pre></td></tr></table></figure><h2 id="positional-encoding模块">Positional encoding模块</h2><p><strong>位置编码</strong>可以理解为一个Nxdim的矩阵</p><ul><li>N就是token的数量</li><li>dim就是一个token的维度</li></ul><p>然后将位置编码进行与patch-embedding后的数据进行相加</p><blockquote><p>⚠️注意，这里是sum而非concat</p></blockquote><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">x += pos_embedding[:, :(n + <span class="number">1</span>)] <span class="comment">#n+1 是patch-embedding后加上cls之后总共的tokens</span></span><br></pre></td></tr></table></figure><h2 id="layernorm模块">LayerNorm模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LayerNorm</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br></pre></td></tr></table></figure><h2 id="attention模块">Attention模块</h2><p><img src="1.png" /></p><p>经过patch embedding 、pos_embedding 、LayerNorm之后</p><p>我们的数据要被分为VKQ那么会产生3个分支</p><p>研究发现将查询、键和值分别线性投影到 dk、dk 和 dv维度上的不同学习线性投影是有益的（投影到低维度）</p><blockquote><p>相当于给h次机会 希望能够学到不一样的投影的方式</p><p>使得在投影进去的度量空间里面 能够去匹配不同模式的相似函数</p><p>类似卷积神经网络中有多个输出通道的感觉</p></blockquote><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dk=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dk * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dk == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dk ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># [B , L ,dim] -&gt; [B , L , dk * heads * 3]</span></span><br><span class="line">        <span class="comment"># dk 主要是为了将dim变成dk 投影到低维度</span></span><br><span class="line">        <span class="comment"># heads  产生多个qkv 有多少头 产生多少个qkv</span></span><br><span class="line">        <span class="comment"># 3 数字3是输入复制成3分 给kqv</span></span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),  <span class="comment"># [B,L,dk*heads] - &gt; [B,L,dim]回到跟输入一样对格式</span></span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = [B , L , dim]</span></span><br><span class="line"></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [B , L , dk*heads ]  [B , L , dk*heads ]  [B , L , dk*heads ]</span></span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h=self.heads), qkv)</span><br><span class="line">        <span class="comment"># [B , L ,dk*heads] - &gt; [B , heads , L , dk]</span></span><br><span class="line"></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># 对最后两个维度进行转置 实现QK^T/sqrt(dk)</span></span><br><span class="line">        attn = self.softmax(dots)  <span class="comment"># softmax[QK^T/sqrt(dk)]</span></span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># softmax[QK^T/sqrt(dk)]V</span></span><br><span class="line"></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line">        <span class="comment"># [B , heads , L , dk] -&gt; [B , L , dk * heads]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure><h2 id="算法结构代码汇总">算法结构代码汇总</h2><p><ahref="https://github.com/lucidrains/vit-pytorch/blob/b3e90a265284ba4df00e19fe7a1fd97ba3e3c113/vit_pytorch/vit.py#L47">详细链接访问Github获取</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># tools</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># LayerNorm</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># MLP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dk=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dk * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dk == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dk ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># [B , L ,dim] -&gt; [B , L , dk * heads * 3]</span></span><br><span class="line">        <span class="comment"># dk 主要是为了将dim变成dk 投影到低维度</span></span><br><span class="line">        <span class="comment"># heads  产生多个qkv 有多少头 产生多少个qkv</span></span><br><span class="line">        <span class="comment"># 3 数字3是输入复制成3分 给kqv</span></span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),  <span class="comment"># [B,L,dk*heads] - &gt; [B,L,dim]回到跟输入一样对格式</span></span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = [B , L , dim]</span></span><br><span class="line"></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [B , L , dk*heads ]  [B , L , dk*heads ]  [B , L , dk*heads ]</span></span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h=self.heads), qkv)</span><br><span class="line">        <span class="comment"># [B , L ,dk*heads] - &gt; [B , heads , L , dk]</span></span><br><span class="line"></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># 对最后两个维度进行转置 实现QK^T/sqrt(dk)</span></span><br><span class="line">        attn = self.softmax(dots)  <span class="comment"># softmax[QK^T/sqrt(dk)]</span></span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># softmax[QK^T/sqrt(dk)]V</span></span><br><span class="line"></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line">        <span class="comment"># [B , heads , L , dk] -&gt; [B , L , dk * heads]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),</span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool=<span class="string">&#x27;cls&#x27;</span>, channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 dim_head=<span class="number">64</span>, dropout=<span class="number">0.</span>, emb_dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width // patch_width)</span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1=patch_height, p2=patch_width),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">            nn.Linear(dim, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;1 n d -&gt; b n d&#x27;</span>, b=b)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        x = x.mean(dim=<span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;patch-embedding模块&quot;&gt;Patch-Embedding模块&lt;/h2&gt;
&lt;h3 id=&quot;section&quot;&gt;&lt;img src=&quot;2.png&quot; /&gt;&lt;/h3&gt;
&lt;h3 id=&quot;转换流程&quot;&gt;转换流程&lt;/h3&gt;
&lt;p&gt;在&lt;strong&gt;Patch Embedd</summary>
      
    
    
    
    <category term="代码复现" scheme="http://www.larryai.com/categories/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python笔记</title>
    <link href="http://www.larryai.com/2022/05/06/python%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.larryai.com/2022/05/06/python%E7%AC%94%E8%AE%B0/</id>
    <published>2022-05-06T07:54:55.000Z</published>
    <updated>2022-05-21T11:29:32.941Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rearrange函数">Rearrange函数</h1><h2 id="作用">作用</h2><p><img src="1.png" /></p><p>在<strong>Patch Embedding</strong>中使用这个函数进行处理</p><p>例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16</p><p>则每张图像会生成224x224/16x16=196个patches</p><ul><li>即输入序列长度为patch_num = 196</li><li>每个patch维度 = patchSize x patchSize x chanels=16x16x3 = 768</li></ul><blockquote><p>相当于将[B , C , H , W ]的图片变成 [ B , PatchNum , dimension ]</p><p>其中 PathNum = <span class="math inline">\(\frac{H \times W}{p \timesp}\)</span> , dimension = <span class="math inline">\(p \times p \timesC\)</span></p></blockquote><h2 id="具体实现">具体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">patch_size = <span class="number">16</span></span><br><span class="line">img = torch.ones(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">fun = Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1=patch_size, p2=patch_size)</span><br><span class="line"></span><br><span class="line">out = fun(img)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img.shape)  <span class="comment"># torch.Size([1, 3, 224, 224])</span></span><br><span class="line"><span class="built_in">print</span>(out.shape)  <span class="comment"># torch.Size([1, 196, 768])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="chunks函数">chunks函数</h1><p>该函数在Transformer的Attention模块将输入投影后给QKV时用到</p><h2 id="作用-1">作用</h2><blockquote><p>若对最后一个维度进行chunks操作</p><p>[B , C , N , D*3] -&gt;[B ,C , N ,D] , [B ,C , N ,D] ,[B ,C , N,D]</p></blockquote><h2 id="具体实现-1">具体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.ones(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span> * <span class="number">3</span>)</span><br><span class="line">output = <span class="built_in">input</span>.chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([1, 3, 672])</span></span><br><span class="line"><span class="built_in">print</span>(output[<span class="number">0</span>].shape)  <span class="comment"># torch.Size([1, 3, 224])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="adaptiveavgpool2d函数">AdaptiveAvgPool2d()函数</h1><h2 id="作用-2">作用</h2><blockquote><p>AdaptivePooling，自适应池化层</p><p>函数通过输入原始尺寸和目标尺寸，自适应地计算核的大小和每次移动的步长。</p><p>如告诉函数原来的矩阵是32x32的尺寸，我要得到18x18的尺寸，函数就会自己计算出核多大、该怎么运动。</p></blockquote><h2 id="具体实现-2">具体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.ones(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">autoPool = nn.AdaptiveAvgPool2d(output_size=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">output = autoPool(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input.shape&#x27;</span>, <span class="built_in">input</span>.shape) <span class="comment"># [1, 3, 256, 256]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output.shape&#x27;</span>, output.shape) <span class="comment"># [1, 3, 18, 18]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;rearrange函数&quot;&gt;Rearrange函数&lt;/h1&gt;
&lt;h2 id=&quot;作用&quot;&gt;作用&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;1.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;Patch Embedding&lt;/strong&gt;中使用这个函数进行处理&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="函数笔记" scheme="http://www.larryai.com/categories/%E5%87%BD%E6%95%B0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Python" scheme="http://www.larryai.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>MASA-SR</title>
    <link href="http://www.larryai.com/2022/05/04/MASA-SR/"/>
    <id>http://www.larryai.com/2022/05/04/MASA-SR/</id>
    <published>2022-05-04T07:58:52.000Z</published>
    <updated>2022-05-16T04:16:20.361Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" /></p><h2 id="主要组成">主要组成</h2><ul><li><p>编码器 Encoder</p></li><li><p>匹配与提取模块（MEM） Math &amp; Extraction Modules</p></li><li><p>空间自适应模块（SAM） Spatial Adaptation Modules</p><blockquote><p>将Ref特征的分布映射到LR特征的分布</p></blockquote></li><li><p>双残差聚合模块（DRAM）Dual Residual Aggregation Modules</p><blockquote><p>进行有效的特征融合</p></blockquote></li></ul><h3 id="名词解释">名词解释</h3><blockquote><ul><li>LR 低分辨率图像</li><li>Ref↓ 表示 x4双三次下采样参考图</li><li>Ref 参考图</li></ul></blockquote><h2 id="编码器">编码器</h2><ul><li><p>与之前使用预先训练过的VGG作为自然提取器的方法不同</p></li><li><p>这里的编码器与网络的其他部分一起从头开始训练的</p></li><li><p>编码器含有三个构造块</p><ul><li>第二个和第三个 利用stride=2的方法将featue map大小折半</li></ul></li><li><p>将Ref参考图传入编码器分别经过三个构造块得到三个不同比例的特征</p><ul><li>生成<span class="math inline">\(F_{Ref}^s\)</span> 其中s =1,2,4</li></ul></li><li><p>LR图像和Ref↓ 图像只经管编码器的第一个构造块</p><ul><li>生成<span class="math inline">\(F_{LR}\)</span> 和 <spanclass="math inline">\(F_{Ref↓ }\)</span></li></ul></li></ul><h2 id="matching-extraction-module-mem">Matching &amp; Extraction Module(MEM)</h2><h3 id="概述">概述</h3><p>​众所周知，在自然图像的局部区域中，相邻像素可能来自公共对象共享相似的颜色统计数据。以往对自然图像先验的研究也表明，一幅图像中的相邻斑块很可能会发现它们之间的对应关系在空间上是一致的。</p><p>​这促使我们提出了一种从粗到精的匹配方案，即粗块匹配和精块匹配。请注意，在我们的方法中，block和patch是两个不同的概念，block的大小大于patch（在我们的实验中patch为3×3的大小）。</p><p>​ 如图3所示，我们首先只在featurespace中找到block的对应关系。具体来说，我们将LR特征(<spanclass="math inline">\(F_{LR}\)</span>)展开为不重叠的block块。每个LRblock将找到其最相关的Ref↓ block。</p><p>​ 与以前的方法相比，通过这样做，匹配的计算成本显著降低。</p><p>​ 为了达到足够的精度，我们进一步对每一个&lt;LR block，Ref↓block&gt;对进行密集patch匹配</p><p>​ 在最后一个阶段，我们根据获得的对应信息提取有用的Ref feature</p><p><imgsrc="https://pic3.zhimg.com/80/v2-e551112dcfc05e03aecc23d0b457189e_1440w.jpg" /></p><h3 id="stage-1-coarse-matching粗匹配">Stage 1: Coarsematching（粗匹配）</h3><p>将<span class="math inline">\(F_{LR}\)</span>展开成K个不重复的blocks<span class="math display">\[\left\{  B_{LR}^0,B_{LR}^1,B_{LR}^2 ,...B_{LR}^{K-1} \right\}\]</span></p><blockquote><p>一个Block中有很多patch 对每个<spanclass="math inline">\(B_{LR}^k\)</span>块找到与它<strong>最相关</strong>的<spanclass="math inline">\(F_{Ref↓}\)</span> block 记作<spanclass="math inline">\(B_{Ref↓}^k\)</span></p></blockquote><p>首先将<span class="math inline">\(B_{LR}^k\)</span>的<strong>centerpatch</strong>与<spanclass="math inline">\(F_{Ref↓}\)</span>中的每一个patch进行计算<strong>余弦相似度</strong>(cosinesimilarity) <span class="math display">\[r_{c,j}^k = \left \langle \frac{p_c^k}{\left \| p_c^k \right \|} ,\frac{q_j}{\left \| q_j \right \|} \right \rangle\]</span></p><ul><li><span class="math inline">\(p_c^k\)</span>是<spanclass="math inline">\(B_{LR}^k\)</span>块的center patch</li><li><span class="math inline">\(q_j\)</span>是<spanclass="math inline">\(F_{Ref↓}\)</span>块的第j个patch</li><li><spanclass="math inline">\(r_{c,j}^k\)</span>表示相似性大小（similarityscores）</li></ul><p>通过<strong>相似性大小</strong>(similarity score)我们可以找到<spanclass="math inline">\(F_{Ref↓}\)</span> 中与<spanclass="math inline">\(p_c^k\)</span>最相似的patch</p><p>然后crop一个围绕着这个最相似patchb并且大小为<spanclass="math inline">\([dx,dy]\)</span>的block 记作<spanclass="math inline">\(B_{Ref↓}^k\)</span></p><blockquote><p>根据局部相干特性,每个<spanclass="math inline">\(B_{LR}^k\)</span>块的center patch都能找到与在<spanclass="math inline">\(F_{Ref↓}\)</span>其中最相似的patch后找到对应的<spanclass="math inline">\(B_{Ref↓}^k\)</span></p><p>另一方面，我们可以在<spanclass="math inline">\(F_{Ref}^s\)</span>中剪切相应的大小为<spanclass="math inline">\([s*dx,s*dy]\)</span>的block，记作<spanclass="math inline">\(B_{Ref}^{s,k}\)</span></p><p>这将用于<strong>特征提取阶段</strong></p></blockquote><p>注意问题⚠️</p><ul><li>如果<spanclass="math inline">\(B_{LR}^k\)</span>的大小远大于其centerpatch的大小，则center patch可能无法代表<spanclass="math inline">\(B_{LR}^k\)</span>的全部内容</li><li>这可能会误导我们找到不相关的<spanclass="math inline">\(B_{Ref↓}^k\)</span></li></ul><p>解决方法：</p><ul><li>使用具有<strong>不同膨胀率</strong>的中心块来计算相似度</li><li>细节如图3的第1阶段所示，<ul><li>其中蓝色虚线表示<em>dilation</em>=1的情况</li><li>橙色虚线表示<em>dilation</em>=2的情况</li><li>然后将相似性得分计算为<strong>不同扩张的结果之和</strong></li></ul></li></ul><blockquote><p>这个阶段我们得到了（<spanclass="math inline">\(B_{LR}^k\)</span>，<spanclass="math inline">\(B_{Ref↓}^k\)</span>，<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>）</p><p>在下面的精细匹配阶段阶段我们将限制<spanclass="math inline">\(B_{LR}^k\)</span> to <spanclass="math inline">\(B_{Ref↓}^k\)</span> 的搜索空间</p></blockquote><h3 id="stage-2-fine-matching精细匹配阶段阶段">Stage 2: Finematching（精细匹配阶段阶段）</h3><blockquote><p>对<span class="math inline">\(B_{LR}^k\)</span> to <spanclass="math inline">\(B_{Ref↓}^k\)</span>进行dense pathmatching（密集补丁匹配）</p><p>得到index maps集合 <span class="math inline">\(\left\{ D^0 , D^1 ,... D^{K-1} \right\}\)</span></p><p>similarity maps 集合<span class="math inline">\(\left\{ R^0 , R^1 ,... R^{K-1} \right\}\)</span></p></blockquote><p>拿第k对(<span class="math inline">\(B_{LR}^k\)</span> , <spanclass="math inline">\(B_{Ref↓}^k\)</span>)为例 我们计算每个patch in<span class="math inline">\(B_{LR}^k\)</span> 与 每个patch in <spanclass="math inline">\(B_{Ref↓}^k\)</span>之间的相似性分数 <spanclass="math display">\[r_{i,j}^k = \left \langle \frac{p_i^k}{\left \| p_i^k \right \|} ,\frac{q_j^k}{\left \| q_j^k \right \|} \right \rangle\]</span></p><ul><li><span class="math inline">\(p_i^k\)</span>是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch</li><li><span class="math inline">\(q_j^k\)</span>是<spanclass="math inline">\(B_{Ref↓}^k\)</span>的第j个patch</li><li>k表示第k对(<span class="math inline">\(B_{LR}^k\)</span> , <spanclass="math inline">\(B_{Ref↓}^k\)</span>)</li><li>$r_{i,j}^k $表示它们的相似性大小</li></ul><blockquote><p><span class="math inline">\(D^k\)</span>的第i个元素的值j表示的是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch与<spanclass="math inline">\(B_{Ref↓}^k\)</span>中第j个ptach最相似</p></blockquote><p><span class="math display">\[D_i^k = \mathop{\arg \max}_j \ r_{i,j}^k\]</span></p><blockquote><p><span class="math inline">\(R^k\)</span>的第i个元素表示的是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch相对应的highestsimilarity score(最高相似分数)</p></blockquote><p><span class="math display">\[R_i^k = \mathop{\max}_j \ r_{i,j}^k\]</span></p><h3 id="stage-3-feature-extraction特征提取">Stage 3: Featureextraction(特征提取)</h3><p>根据index map <span class="math inline">\(D^k\)</span> 从<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>中提取patches，生成新的featuremap <span class="math inline">\(B_M^{s,k}\)</span></p><p>更精准的说</p><p><span class="math inline">\(D_i^k\)</span>表示<spanclass="math inline">\(B_{Ref↓}^k\)</span>中与<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch最相似的patch</p><p>我们将<span class="math inline">\(D_i^k\)</span>个<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>中的patch作为<spanclass="math inline">\(B_M^{s,k}\)</span>的第i个patch</p><p>此外，由于相似性<strong>分数较高的Ref特征更有用</strong></p><p>我们将<span class="math inline">\(B_M^{s,k}\)</span>与相应的<spanclass="math inline">\(R^k\)</span>相乘获得<strong>加权特征块</strong><span class="math display">\[B_M^{s,k} := B_M^{s,k}\odot(R^k)\uparrow\]</span></p><ul><li><span class="math inline">\(()\uparrow\)</span>表示双线性插值</li><li><span class="math inline">\(\odot\)</span> 表示element-wise mul</li></ul><p>MEM的最终结果就是将 <span class="math inline">\(\left\{ B_M^{s,0} ,B_M^{s,1} , B_M^{s,2},...B_M^{s,K-1}\right\}\)</span>进行折叠在一起获得，折叠操作是步骤一的逆向操作</p><h3 id="分析">分析</h3><h4 id="以往的配对方法">以往的配对方法</h4><ul><li><p>图像LR的像素 为 m pixels ; 图像<spanclass="math inline">\(Ref\downarrow\)</span>的像素 为 n pixels</p></li><li><p>计算复杂度为O(mn)</p></li></ul><h4 id="mem方法">MEM方法</h4><ul><li>假设<span class="math inline">\(Ref\downarrow\)</span> block 有 n’pixels</li><li>计算复杂度将被降为 O(Kn+mn’)</li><li>K远小于m</li><li>n‘ 远小于n</li><li>通过这种从粗到精的匹配方案，计算量显著降低</li></ul><h2 id="spatial-adaptation-modulesam空间自适应模块">Spatial AdaptationModule（SAM）空间自适应模块</h2><p><imgsrc="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" /></p><p>在许多情况下，LR和Ref图像可能具有相似的内容和纹理，颜色和亮度不一样</p><p>因此，提取的REF特征的分布可能与LR特征的分布不一致。因此，简单地将Ref和LR特性连接到一起，并将它们输入到下面的卷积层中并不是最佳选择。我们建议使用空间自适应模块（SAM）将提取的Ref特征的分布重新映射到LR特征的分布</p><p>首先将LR特征和提取的Ref特征连接(Cat)起来，然后预先送入卷积层，以产生两个参数β和γ，这两个参数的大小与LR特征相同。</p><p>我们用特征的平均值和标准偏差更新β和γ <span class="math display">\[\beta \leftarrow \beta + \mu_{LR}\\\gamma \leftarrow \gamma +\sigma_{LR}\]</span></p><blockquote><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span>的产生方式跟<spanclass="math inline">\(\mu_{Ref}\)</span> <spanclass="math inline">\(\sigma_{Ref}\)</span> 一样</p><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span> 的大小是 C x 1 x 1</p><p>C表示一共有C个通道</p></blockquote><p>然后将实例规范化(Instance Norm)应用于Ref特征，如下所示： <spanclass="math display">\[F_{Ref}^c \leftarrow \frac{F_{Ref}^c - \mu_{Ref}^c}{\sigma_{Ref}^c}\]</span> <span class="math inline">\(\mu_{Ref}^c\)</span> <spanclass="math inline">\(\sigma_{Ref}^c\)</span>分别表示Ref特征图在通道c的均值和方差 <span class="math display">\[\mu_{Ref}^c = \frac{1}{HW} \sum_{y,x}F_{Ref}^{c,y,x}\]</span></p><p><span class="math display">\[\sigma_{Ref}^c  = \sqrt{\frac{1}{HW}\sum_{y,x}(F_{Ref}^{c,y,x} -\mu_{ref}^c)^2}\]</span></p><p>最后，将γ和β添加到归一化Ref特征中，如下所示： <spanclass="math display">\[F_{Ref} \leftarrow F_{Ref}·\gamma + \beta\]</span> 由于Ref特征和LR特征之间的差异随空间位置而变化</p><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span> <spanclass="math inline">\(\mu_{Ref}\)</span> <spanclass="math inline">\(\sigma_{Ref}\)</span> 的大小为 C x 1 x 1</p><p>我们使用可学习卷积来预测两个空间参数β和γ</p><p>不同于只使用分割图(segmentation)去生成两个参数，SAM中的卷积将Ref和LR特征作为输入，以了解它们的差异。此外，在从卷积中获得β和γ后，我们将它们与LR特征的均值和标准偏差相加</p><h2 id="dual-residual-aggregation-module-dram-双残差聚合模块">DualResidual Aggregation Module (DRAM) 双残差聚合模块</h2><p>在空间自适应后，使用我们提出的双剩余聚合模块（DRAM），将传输的Ref特征与LR特征融合</p><p>DRAM由两个分支组成，即 <strong>LR分支</strong> 和<strong>Ref分支</strong></p><p><strong>Ref分支</strong></p><blockquote><p>旨在细化Ref功能的高频细节</p></blockquote><ul><li>首先使用stride=2的卷积对Ref Feature 进行下采样</li><li>将下采样后的Ref Feature 减去 LR Feature 得到Res_Ref 残余特征</li><li>将Res_Ref使用转置卷积（逆卷积）上采样后加上原始的RefFeatu得到新的Ref ‘ 特征图</li></ul><p><span class="math display">\[\begin{cases}    Res_{Ref} = Conv(F_{Ref}) - F_{LR}\\    F&#39;_{Ref}= F_{Ref} + DeConv(Res_{Ref})\end{cases}\]</span></p><p><strong>LR分支</strong></p><blockquote><p>LR功能的高频细节细化</p></blockquote><ul><li>首先使用stride=2的卷积对Ref Feature 进行下采样</li><li>将 <strong>LR Feature</strong> 减去<strong>下采样后的RefFeature</strong> 得到Res_LR 残余特征</li><li>将Res_LR 加上 LR Feature 相加后 进行上采样的懂新的LR’ 特征图</li></ul><p><span class="math display">\[\begin{cases}    Res_{LR} =  F_{LR}-Conv(F_{Ref})\\    F&#39;_{LR}= DeConv(F_{LR}+Res_{LR})\end{cases}\]</span></p><p>最后，将两个分支的输出串联起来，并以步长1通过另一个卷积层。</p><p>通过这种方式，LR和Ref功能中的细节得到了增强和聚合，从而产生了更具代表性的功能。</p><h2 id="loss-functions">Loss Functions</h2><h3 id="reconstruction-loss重建损失">1.Reconstructionloss(重建损失)</h3><p>采用L1损失作为重建损失</p><p><span class="math display">\[\mathcal{L}_{rec} = \left \|  I_{HR} - I_{SR}\right\|_1\]</span></p><h3 id="perceptual-loss知觉损失">2.Perceptual loss(知觉损失)</h3><p>知觉损失的表达为 <span class="math display">\[\mathcal{L}_{per} = \left \|  \phi_i(I_{HR}) - \phi_i(I_{SR}) \right\|_2\]</span></p><blockquote><p><span class="math inline">\(\phi_i\)</span>表示VGG19的第i层这里是用conv5_4</p></blockquote><h3 id="adversarial-loss对抗性损失">3.Adversarial loss(对抗性损失)</h3><p>它可以有效地生成具有自然细节的视觉愉悦图像 <spanclass="math display">\[\mathcal{L}_D =-\mathbb{E}_{I_{HR}}[\log(D(I_{HR},I_{SR}))]-\mathbb{E}_{I_{SR}}[\log(1-D(I_{HR},I_{SR}))],\\\mathcal{L}_G=-\mathbb{E}_{I_{HR}}[\log(1-D(I_{HR},I_{SR}))]-\mathbb{E}_{I_{SR}}[\log(D(I_{SR},I_{HR}))]\]</span></p><h3 id="full-objective">4.Full objective</h3><p><span class="math display">\[\mathcal{L} =\lambda_{rec}\mathcal{L}_{rec}+\lambda_{per}\mathcal{L}_{per}+\lambda_{adv}\mathcal{L_{adv}}\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;主要组成&quot;&gt;主要组成&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="超分重建" scheme="http://www.larryai.com/tags/%E8%B6%85%E5%88%86%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>informative-drawing</title>
    <link href="http://www.larryai.com/2022/05/04/informative-drawing/"/>
    <id>http://www.larryai.com/2022/05/04/informative-drawing/</id>
    <published>2022-05-04T07:56:42.000Z</published>
    <updated>2022-05-04T07:59:24.408Z</updated>
    
    <content type="html"><![CDATA[<h1id="learning-to-generate-line-drawings-that-convey-geometry-and-semantics">Learningto generate line drawings that convey geometry and semantics</h1><h2 id="摘要">摘要</h2><p>本文提出了一种从照片中生成线条图的非配对方法。目前的方法通常依赖高质量的成对数据集来生成线条图。然而，由于图纸主题属于特定do-main，或收集的数据量有限，这些数据集通常具有局限性。尽管近年来在无监督图像到图像的翻译方面取得了很大进展，但最新的方法仍然难以生成令人信服的线条图。我们观察到，线条图是场景信息的编码，并试图传达三维形状和语义。我们将这些观察构建成一组目标，并训练图像翻译，将照片映射成线条图。我们引入了一种几何损失，<strong>它从线条图的图像特征中预测深度信息</strong>，<strong>以及一种语义损失</strong>，<strong>它将线条图的剪辑特征与其相应的照片相匹配</strong>。我们的方法优于最先进的未成对图像翻译和线条绘制生成方法，可以从ar位图照片中创建线条图。</p><h2 id="方法">方法</h2><p>我们的目标是训练一个模型，在给定照片数据集和未配对的线图数据集的情况下，自动生成任意照片的线图。我们将这个问题描述为包含照片的域A和代表特定风格线条图的do-mainB之间的未配对图像转换。大多数以前的方法仅仅考虑在循环图中保持PHO图样的外观。相反，我们的方法通过评估几何和语义的目标进一步指导翻译通过线条图传达的信息。此设置如图2所示。我们在第4节中说明，这些新损失对于创建有意义的图形至关重要。</p><figure><img src="1.png" alt="截屏2022-04-11 22.10.53" /><figcaption aria-hidden="true">截屏2022-04-11 22.10.53</figcaption></figure><blockquote><p>给出一张照片a，我们的模型训练网络GA，通过四个主要损失来合成线图GA（a）。带有鉴别器DB的对抗式损失鼓励生成的线条绘制与训练集的风格相匹配。</p><p>线条、外观和几何图形的LOSS强制要求线条分别传达有效的语义、外观和几何图形</p></blockquote><p>我们对域A和域B分别使用带有生成器网络GA、GB和鉴别器DA、DB的对抗性训练设置。</p><ul><li>几何目标通过预先训练的深度网络来实现，该网络根据<strong>线图预测深度图</strong>，并对深度输出施加监督损失。这种损失促使我们的模型在几何上重要的位置（例如遮挡轮廓）绘制线。</li><li>其次，我们引入了一个<strong>CLIPloss</strong>，将语义添加到生成的线条图中。由于任意的照片往往显示复杂的场景，我们使用<strong>视觉CLIP嵌入</strong>，它可以很好地捕捉语义细节。然后，我们规定线条图的CLIP嵌入与原始照片的CLIP嵌入相似。</li><li>我们还使用弱加权<strong>循环一致性损失</strong>来保留外观信息。</li></ul><h1 id="loss">LOSS</h1><h2 id="对抗性损失">对抗性损失</h2><p>鼓励生成的图像长到各自的域[27]。使用LSGAN设置[61]的每个域的损耗公式如下</p><figure><img src="2.png" alt="截屏2022-04-11 22.14.36" /><figcaption aria-hidden="true">截屏2022-04-11 22.14.36</figcaption></figure><h2 id="几何目标损失the-geometry-objective">几何目标损失The geometryobjective</h2><p>​在训练期间，最大限度地利用自动绘制的线条图提供深度信息。我们观察到，线条图通常是3D形状的有效载体,并在培训期间应用此属性。给定一个线图的实体数据集，模型可以在没有任何明确监督的情况下学习这一序列。然而，目前没有这种几何约束的方法无法在有意义的地方放置线（见第4节）。照片数据集和线条图之间的领域差距也是显而易见的。相反，我们<strong>提出了一个几何约束，用于监督线条图的深度预测</strong></p><p>​为了监督线条图的深度预测，有必要获得用于摄影输入的深度图。不幸的是，大多数数据集通常无法获得地面真相深度信息。然而，<strong>最近的方法在为照片绘制高分辨率深度图方面非常成功</strong>。这一进展使我们能够使用从最先进的深度预测网络F获得的伪地面真深度图；实际上，我们使用[62]中的网络，它基于MiDaS[69]。我们注意到，<strong>照片的伪地面真相图仅在培训时需要，而不是在测试时需要</strong></p><p>​<strong>监督几何预测</strong>的一个简单方法是，在培训期间引入网络<spanclass="math inline">\(G_{geom}\)</span>来根据线图预测深度图。然而，这种方法有几个问题。从合成线图中学习深度的培训可能会鼓励线图生成器以一种不需要的形式灌输深度信息</p><blockquote><p>比如一个不易察觉的信号[15]。我们希望避免意外地将不可见的信息嵌入到我们的线条图中。</p><p>由于领域差距，在线条图上使用预训练深度网络不是一个选项</p></blockquote><p>​相反，我们建议学习从通常在照片和线条图之间<strong>共享的图像特征推断深度</strong>。具体来说，我们在给定ImageNet[20]特征的情况下，预先训练一个网络<spanclass="math inline">\(G_{geom}\)</span>来预测深度。这些特征，尤其是在早期阶段，对迁移学习非常有用[49]。这个场景希望通过首先将<strong>线条图编码</strong>(intoa shared representation withphotographs)为带有照片的共享演示，然后应用一个从照片特征中学习深度的网络，来避免不可见的信号问题。</p><ul><li>将线条图编码与和图片加入共享representation</li><li>应用一个从照片特征中学习深度的网络？</li></ul><p>为了获得图像特征，我们将照片输入预先训练好的<strong>Inceptionv3[76]网络</strong>，并从<strong>混合6b节点</strong>提取特征（见补充）。</p><p>我们将input a在这层提取出来的特征定义为<spanclass="math inline">\(I(a)\)</span></p><p>经过预训练后，<spanclass="math inline">\(G_{geom}\)</span>提供线条画的深度估计图</p><p>在练习中 我们在培训线条绘制生成的同时进行微调 <spanclass="math inline">\(G_{geom}\)</span></p><p>几何损失公式如下。</p><ul><li>给定照片a，我们首先将其输入到最先进的深度网络F[62]中，并获得pseudo-ground truth depth map <spanclass="math inline">\(F(a)\)</span>。</li><li>然后生成线条画<span class="math inline">\(G_A(a)\)</span>并提取它的ImageNet特征<spanclass="math inline">\(I(G_A(a)\)</span>。</li><li>然后将这些特征传递给预先训练的深度网络<spanclass="math inline">\(G_{Geom}\)</span> , 生成深度图预测<spanclass="math inline">\(G_{Geom}(I(G_A(a))\)</span></li><li>然后将该深度预测与伪地面真实深度图F(a)进行比较</li></ul><p>进一步,细节和深度重建见补充说明</p><figure><img src="Lgeom.png" alt="截屏2022-04-11 22.43.35" /><figcaption aria-hidden="true">截屏2022-04-11 22.43.35</figcaption></figure><h2 id="语义损失the-semantics-loss">语义损失The semantics loss</h2><p>​ 通过最小化 <strong>输入照片图的CLIP嵌入</strong>和<strong>生成的线条图</strong>之间的距离来实现。这一目标的目标是将原始照片中的语义信息传达到相应的合成线条图中。</p><p>​在计算机视觉中，<strong>语义通常以标签和分割图的形式学习</strong>。然而，这些<strong>表达仅限于特定领域或对象</strong>。为了对整个场景中的语义信息进行编码，我们使用了(shared visual-text embeddingCLIP)<strong>共享的视觉文本嵌入剪辑</strong>[68]，它在照片和艺术[17,24]中捕获了丰富的语义信息。然后，我们对生成的线条图和原始照片图之间的距离进行惩罚。目标如下。</p><figure><img src="LCLIP.png" alt="截屏2022-04-11 22.44.24" /><figcaption aria-hidden="true">截屏2022-04-11 22.44.24</figcaption></figure><h2id="外观损失the-appearance-loss-循环一致性损失cycle-consistency">外观损失Theappearance loss (循环一致性损失cycle consistency)</h2><p>已被用于通过图像转换对输入外观进行编码[47,89]。贴图每个方向的外观损失如下所示</p><figure><img src="Lcycle.png" alt="截屏2022-04-11 22.47.14" /><figcaption aria-hidden="true">截屏2022-04-11 22.47.14</figcaption></figure><p><br /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1
id=&quot;learning-to-generate-line-drawings-that-convey-geometry-and-semantics&quot;&gt;Learning
to generate line drawings that convey geometry and s</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GanGANv2</title>
    <link href="http://www.larryai.com/2022/05/04/GanGANv2/"/>
    <id>http://www.larryai.com/2022/05/04/GanGANv2/</id>
    <published>2022-05-04T07:31:06.000Z</published>
    <updated>2022-05-06T12:22:28.993Z</updated>
    
    <content type="html"><![CDATA[<h1 id="product-of-experts-gans">Product-of-experts GANs</h1><p><strong>输入：</strong></p><ul><li>图像数据集x</li><li>M个输入方式（多模态）</li></ul><p><strong>目标：</strong></p><p>训练一个单一的操作模型，该模型可以学习在可能模式的任意子集<spanclass="math inline">\(\mathcal{Y}\)</span>上捕获图像分布</p><p><strong>在本文中，我们考虑了四种不同的模式:</strong></p><ul><li><p>文本text</p></li><li><p>语义分割segmentation</p></li><li><p>草图sketch</p></li><li><p>风格参照style reference</p></li></ul><p>学习基于任何子数据的图像分布<spanclass="math inline">\(p(x\mid\mathcal{Y})\)</span>是一项挑战</p><p>因为它需要一个生成器同时去建模<spanclass="math inline">\(2^M\)</span>个分布</p><p>特别值得注意的是</p><p>当<spanclass="math inline">\(\mathcal{Y}\)</span>是空集合时，生成器需要捕获无条件的图像分布</p><p>其他几种模态下的单独的条件分布</p><p><span class="math inline">\(p(x|y_i) , \forall i \in\left\{1,2...M\right\}\)</span></p><p>例如，图像分布仅以文本为条件。这些设置在隔离单独中很受欢迎并得到了广泛的研究，我们的目标是将它们置于一个统一的框架下。</p><h2 id="product-of-experts-modeling">Product-of-experts modeling</h2><p>直观地说，每个输入模态都增加了合成图像必须满足的约束。</p><p>满足所有约束的图像集是满足单个约束的图像集的交点</p><p><strong>分布的乘积类似于集合的交集</strong></p><figure><img src="1.png"alt="The product of distributions is analogous to the intersection of sets" /><figcaption aria-hidden="true">The product of distributions is analogousto the intersection of sets</figcaption></figure><p>如上图，我们通过一个强有力的假设对此进行建模</p><p>即<strong>联合条件概率分布</strong><spanclass="math inline">\(p(x\mid y_i,y_j)\)</span>与单条件概率分布<spanclass="math inline">\(p(x \mid y_i)\)</span>和<spanclass="math inline">\(p(x \mid y_j)\)</span>的乘积成正比</p><p>在此设置下，为了使product分布在某个区域具有高密度，每个单独的分布需要在该区域具有高密度，从而满足每个约束</p><p>通过相乘的方式组合多个分布（“experts”）以前被称为<strong>Product-of-experts</strong></p><p>我们的生成器被训练用来将一个latent code z 映射到一个image x</p><p>因为输出的图像被latent code z 唯一的确定</p><p>所以评估<spanclass="math inline">\(p(x\mid\mathcal{Y})\)</span>的问题可以等价为评估<spanclass="math inline">\(p(z\mid\mathcal{Y})\)</span>的问题</p><p>我们使用Product-of-experts来对潜在(latent)条件分布进行建模 <spanclass="math display">\[p(z\mid\mathcal{Y}) \propto p&#39;(z)\prod_{y_i\in \mathcal{Y}}q(z\midy_i)\]</span></p><ul><li><span class="math inline">\(p&#39;(z)\)</span>是先验概率分布</li><li>每个expert <span class="math inline">\(q(z\midy_i)\)</span>是编码器预测的单一模态的分布</li></ul><blockquote><p>当没有给出模式时，潜在分布只是先验分布。</p><p>随着提供更多的模式，约束的数量增加，可能输出图像的空间缩小，潜在分布变窄。</p></blockquote><h2id="multiscale-and-hierarchical-latent-space多尺度层次潜在空间">Multiscaleand hierarchical latent space（多尺度层次潜在空间）</h2><p>有一些我们考虑的模态是二维的，自然包含多个尺度的信息。</p><p>因此，我们设计了一个带有不同分辨率下的潜变量的hierarchical latentspace（潜在层次空间）</p><p>这允许我们直接将信息从空间编码器的每个分辨率传递到潜在空间的相应分辨率，以便更好地保存高分辨率的控制信号</p><p>从数学上讲，我们的潜在代码z被分成若干组 <span class="math inline">\(z= (z^0 , z^1 ,z^2 ... z^N)\)</span></p><ul><li><p><span class="math inline">\(z^0 \in \mathbb{R}^{c_0}\)</span>是一个特征向量</p></li><li><p><span class="math inline">\(z^k \in \mathbb{R}^{c_k\times r_k\times r_k}\)</span> 是分辨率不断提高的特征图</p></li><li><p>$r_{k+1}= 2r_k , r_1 =4 $ <spanclass="math inline">\(r_N\)</span>是图像分辨率</p></li></ul><p>因此我们可以分解</p><ul><li>先验概率分布<spanclass="math inline">\(p&#39;(z)\)</span>分解到<spanclass="math inline">\(\prod_{k=0}^N p&#39;(z^k |z^{&lt;k})\)</span></li><li>expert <span class="math inline">\(q(z\mid y_i)\)</span> 分解到<spanclass="math inline">\(\prod_{k=0}^N q(z^k | z^{&lt;k} ,y_i)\)</span></li></ul><p>这种设计类似于等级VAE中的前后网络。不同之处在于，我们的编码器在输入模式中编码条件信息，而不是图像本身。根据潜在条件分布公式，我们假设每个分辨率的潜在条件分布是expert的产物<span class="math display">\[p(z^k\mid z^{&lt;k},\mathcal{Y}) \propto p&#39;(z^k |z^{&lt;k})\prod_{y_i\in \mathcal{Y}}q(z^k | z^{&lt;k},y_i)\]</span></p><blockquote><p><span class="math inline">\(p&#39;(z^k | z^{&lt;k}) =\mathcal{N}(\mu_0^k,\sigma_0^k)\)</span> 和 $q(z^k | z^{&lt;k},y_i)=(_i^k,_i^k) $用神经网络参数化均值和标准差的独立高斯分布</p></blockquote><blockquote><p>可以证明 Product of gaussion experts 同样也是高斯分布 <spanclass="math inline">\(p(z^k\mid z^{&lt;k},\mathcal{Y}) =\mathcal{N}(\mu^k,\sigma^k)\)</span></p></blockquote><p><img src="2.png" /></p><h2 id="generator-architecture">Generator architecture</h2><h3 id="encoder">1.Encoder</h3><p><img src="3.png" /></p><p>上图为生成器体系结构的概述。我们将每个模态编码成一个特征向量，然后在<strong>GlobalPoE-Net</strong>中进行聚合。</p><ul><li><p>使用卷积网络(with input skip connections)对分割图seg和草图sketch进行编码</p></li><li><p>使用residual network对风格图style进行编码</p></li><li><p>使用CLIP去对文本text进行编码</p></li></ul><blockquote><p>附录B中给出了所有模态编码器的详细信息。</p><p>解码器使用GlobalPoE-Net的输出以及<strong>分割编码器</strong>和<strong>草图编码器</strong>的跳过连接生成图像</p></blockquote><h3 id="global-poe-net">2.Global PoE-Net</h3><blockquote><p><span class="math inline">\(\mu和\sigma\)</span>下标有0，1，2，3，4</p><p>0表示先验分布 1表示分割编码器 2表示文本编码器 3表示style编码器4表示草图编码器</p></blockquote><p><img src="4.png" /></p><p>在GlobalPoE-Net中，我们使用MLP从每个模态中提取特征向量来预测一个高斯分布<spanclass="math inline">\(q(z^0|y_i)=\mathcal{N}(\mu_i^0,\sigma_i^0)\)</span></p><p>然后我们计算product of Gaussion包括$p'(z^0) = (_0^0,_0^0)=(0,I) $</p><p>并从product分布中采样得到<span class="math inline">\(z^0\)</span></p><p>然后用MLP将<spanclass="math inline">\(z^0\)</span>转化为另一个特征向量<spanclass="math inline">\(w\)</span></p><h3 id="decoder">3.Decoder</h3><p><img src="5.png" /></p><p>解码器主要有一堆残差块组成</p><p><strong>Local-PoE-Net</strong>在当前分辨率下对潜在feature map <spanclass="math inline">\(z^k\)</span>进行采样</p><p>当前分辨率下的product</p><p><span class="math inline">\(p&#39;(z^k | z^{&lt;k}) =\mathcal{N}(\mu_0^k,\sigma_0^k)\)</span> 特征图zk下的先验分布</p><p><span class="math inline">\(q(z^k | z^{&lt;k},y_i) =\mathcal{N}(\mu_i^k,\sigma_i^k)\)</span> 特征图zk下不同模态的分布</p><blockquote><p><spanclass="math inline">\(\mu_0^k,\sigma_0^k\)</span>根据最后一层的输出计算</p><p><spanclass="math inline">\(\mu_i^k,\sigma_i^k\)</span>通过连接最后一层的输出和相应模态的跳过连接来计算</p></blockquote><blockquote><p>请注意，只有具有跳过连接的模式(分割图和草图)对计算作出贡献</p><p>其他模式（文本和style参考）仅提供global信息，而不提供local细节。</p></blockquote><p>local-PoE-net生成的潜在特征映射<spanclass="math inline">\(z^k\)</span>和global-PoE-net生成的特征向量<spanclass="math inline">\(w\)</span>被送到LG AdaIN层</p><p>本地全局自适应实例规范化(LG AdaIN)</p><p><img src="6.png" /></p><p><img src="7.png" /></p><ul><li><p><spanclass="math inline">\(h^k\)</span>是在残差分支中通过一个卷积层得到的一个featuremap</p></li><li><p>μ(hk) and σ(hk) 是通道平均值和标准差</p></li><li><p><span class="math inline">\(\beta_{z^k} , \gamma_{z^k}\)</span>由<span class="math inline">\(z^k\)</span>通过一个1x1卷积得到</p></li><li><p><span class="math inline">\(\beta_{w} ,\gamma_{w}\)</span>由w计算得到</p></li></ul><blockquote><p>LG-AdaIN可以被看作AdaIN和SPADE的组合采用<strong>全局特征向量</strong>和<strong>空间变化特征映射</strong>来调节激活。</p></blockquote><h2id="multiscale-multimodal-projection-discriminator多尺度多模投影鉴别器">Multiscalemultimodal projection discriminator（多尺度多模投影鉴别器）</h2><blockquote><p>输入：</p><ul><li>image x</li><li>一系列的条件 <span class="math inline">\(\mathcal{Y}\)</span></li></ul><p>输出：</p><p>一个分数<span class="math inline">\(D(x,\mathcal{Y}) =sigmoid(f(x,\mathcal{Y}))\)</span>表明真实性</p><p>f的最优解是</p><p><img src="8.png" /></p></blockquote><p>我们假设给出x的不同模态下的条件独立,投影鉴别器（PD）（估计是一篇论文里的方法）建议使用<strong>内积</strong>来估计条件变量,他的实施将有条件的期限限制为,要相对简单，这会产生一个很好的归纳偏差，从而产生很强的实证结果。<img src="9.png" /></p><p>原始的PD</p><ul><li>首先将图像和条件一起输入到一个<strong>shared latentspace</strong>（潜在共享空间）</li><li>然后使用一个<strong>线性层</strong>来估计图像嵌入的<strong>无条件项（unconditionalterm）</strong></li><li>并使用图像嵌入和条件嵌入之间的<strong>内积</strong>来估计<strong>条件项(conditionalterm)</strong>。</li><li>将<strong>无条件项</strong>和<strong>条件项</strong>相加，以获得最终的鉴别器的结果</li></ul><blockquote><p>相当于PD的推广</p><p>我们提出了一种多模态投影判别器，它将投影判别器推广到处理多个条件输入。与计算图像嵌入和条件嵌入之间的单个内积的标准投影判别器不同，我们为每个输入模态计算一个内积并将它们加在一起以获得最终损失。</p></blockquote><p><img src="10.png" /></p><p><span class="math display">\[f(x,\mathcal{Y}) = Linear(D_x(x)) = \sum_{y_i \in \mathcal{Y}}D_{y_i}^T(y_i)D_x(x)\]</span></p><p>对于分割和草图等空间模式，在多个尺度下加强它们与图像的对齐更有效。</p><p>如图所示，我们将图像和空间模式编码为不同解决方案的特征图，并计算每个分辨率下的MPD损失。</p><p>我们计算每个位置和分辨率的损失值，然后通过先对位置进行平均，然后再对分辨率进行平均，得到最终损失。</p><p>我们将产生的鉴别器命名为多尺度多模投影鉴别器（MMPD），并在附录B中描述其细节</p><figure><img src="11.png" alt="截屏2022-03-12 19.59.36" /><figcaption aria-hidden="true">截屏2022-03-12 19.59.36</figcaption></figure><h2 id="losses-and-training-procedure">Losses and trainingprocedure</h2><h3 id="section"></h3><h3 id="latent-regularization">Latent regularization</h3><p>在product-of-experts的假设下他认为<strong>有条件的潜在分布</strong>应该与<strong>无条件的先验分布</strong>相匹配</p><figure><img src="12.png" alt="截屏2022-03-15 10.34.41" /><figcaption aria-hidden="true">截屏2022-03-15 10.34.41</figcaption></figure><p>我们<strong>最小化</strong>在每个分辨率下<strong>有条件的潜在分布</strong><spanclass="math inline">\(p(z|y_i)\)</span>和<strong>无条件的先验分布</strong><spanclass="math inline">\(p&#39;(z)\)</span>的KL散度</p><figure><img src="13.png" alt="截屏2022-03-15 10.37.50" /><figcaption aria-hidden="true">截屏2022-03-15 10.37.50</figcaption></figure><ul><li><spanclass="math inline">\(w_k\)</span>是分辨率相关的再平衡权重</li><li><span class="math inline">\(w_i\)</span>是一个特定误差权重</li></ul><blockquote><p>KL损失也减少了条件模式崩溃，因为它鼓励条件潜在分布接近先验分布，因此具有高熵。</p><p>从信息瓶颈的角度来看，KL损失鼓励每个模态只提供指定条件图像分布所需的最小信息。</p></blockquote><h3 id="contrastive-losses">Contrastive losses</h3><p>对比损失在<strong>表示学习</strong>中得到了广泛的应用，最近应用到图像合成</p><p>给定一批配对向量<span class="math inline">\((u,v) = \left\{(u_i,v_i), i =1,2,3...N\right\}\)</span></p><p>对称性交叉熵损失 最大化成对向量的相似性，同时分开 非成对向量</p><figure><img src="14.png" alt="截屏2022-03-15 10.46.34" /><figcaption aria-hidden="true">截屏2022-03-15 10.46.34</figcaption></figure><p>我们使用两种配对来构造两个对比损失项：<strong>图像对比损失</strong>和<strong>条件对比损失</strong></p><h4 id="图像对比损失">图像对比损失</h4><p>可最大化<strong>真实图像</strong>与<strong>基于条件生成的图像</strong>的相似性<span class="math display">\[\mathcal{L}_{cx} = \mathcal{L}^{ce}(E_{vgg}(x) , E_{vgg}(\bar{x}))\]</span></p><h4 id="条件对比损失">条件对比损失</h4><blockquote><p>条件对比损失可以更好地使图像与相应的条件一致。</p></blockquote><h5id="对识别器进行训练以最大化真实图像x的嵌入和条件输入之间的相似性">对识别器进行训练，以最大化真实图像X的嵌入和条件输入之间的相似性</h5><ul><li>辨别器的条件对比损失</li></ul><p><span class="math display">\[\mathcal{L}_{cy}^D = \mathcal{L}^{ce}(D_x(x) , D_{y_i}(\mathcal{Y}_i))\]</span></p><blockquote><p>其中，Dx和Dy分别是鉴别器中的两个模块，分别从x和y_i中提取特征</p></blockquote><h5id="生成器使用相同的损失进行训练但使用生成的图像barx代替真实图像x来计算鉴别器嵌入">生成器使用相同的损失进行训练，但使用生成的图像<spanclass="math inline">\(\bar{x}\)</span>代替真实图像x来计算鉴别器嵌入</h5><ul><li>生成器的条件对比损失</li></ul><p><span class="math display">\[\mathcal{L}_{cy}^G = \mathcal{L}^{ce}(D_x(\bar{x}) ,D_{y_i}(\mathcal{Y}_i))\]</span></p><blockquote><p>在实践中，我们只对<strong>文本模式</strong>使用条件对比损失，因为它消耗了太多的GPU内存，而对其他模式使用条件对比损失，尤其是在图像分辨率和批量较大的情况下。</p></blockquote><h3 id="full-training-objective">Full training objective</h3><p>总的LOSS包含生成器LOSS和辨别器LOSS</p><figure><img src="15.png" alt="image-20220315125032385" /><figcaption aria-hidden="true">image-20220315125032385</figcaption></figure><ul><li><span class="math inline">\(\mathcal{L}^G 和\mathcal{L}^D\)</span>是非饱和GAN损失</li><li><spanclass="math inline">\(\mathcal{L}_{GP}\)</span>是R1梯度惩罚损失</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;product-of-experts-gans&quot;&gt;Product-of-experts GANs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图像数据集x&lt;/li&gt;
&lt;li&gt;M个输入方式（多模态）&lt;/li&gt;
&lt;/ul&gt;
&lt;p</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ChipGAN</title>
    <link href="http://www.larryai.com/2022/05/04/ChipGAN/"/>
    <id>http://www.larryai.com/2022/05/04/ChipGAN/</id>
    <published>2022-05-04T07:21:07.000Z</published>
    <updated>2022-05-06T08:03:21.081Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概要">概要</h1><p>​风格转换已成功应用于照片，生成逼真的西方绘画。然而，由于中西绘画技法的内在差异，直接套用已有的方法，对中国水墨画风格的转换并不能产生令人满意的效果。本文提出了一种基于ChipGAN的端到端（end-to-end）生成对抗性网络体系结构，用于照片转化为中国水墨画。</p><blockquote><p>ChipGAN的核心模块实施了三个约束</p><ul><li><p>空白（voids）</p></li><li><p>笔触（ brush strokes）（画笔描边）</p></li><li><p>水墨色调和扩散（ink wash tone and diffusion）</p></li></ul><p>以解决中国水墨画中普遍采用的三个关键技术。</p></blockquote><p>​我们通过咨询专业艺术家，基于新建的中国水墨照片和图像数据集，进行风格化感知研究，以评估生成的绘画与真实绘画的相似性。与最先进的网络和高度程式化的感知研究核心相比，该方法在视觉质量方面的优势表明了该方法的有效性。</p><h1 id="介绍">介绍</h1><h1 id="section"><img src="fig1.png" /></h1><p>​任何成功的艺术家都有自己独特的绘画风格。研究绘画风格中的这种独特性对绘画技能的训练很重要。除了传统的艺术理论培训，计算机视觉和图形技术如风格迁移和非真实感渲染，旨在帮助绘画艺术家系统地理解如何通过观察真实场景或照片，运用适当的绘画技巧呈现独特风格。</p><p>​将绘画风格迁移到图像可以通过纹理合成来实现，使用低级别的图像特征，忽略图像的语义信息。利用卷积神经网络（CNN）从图像中提取高级语义信息进行风格转换，这显示了视觉逼真的结果（如上图，根据真实西方绘画的风格从照片到生成的西方绘画）。</p><p>​然而，直接将现有的风格转换技术应用到中国水墨画中会产生不切实际的结果（图中，生成的水墨画，请注意混乱的线条和颜色）。这是因为中国水墨画和西方水墨画有几个本质的区别，在图中的最后一列中，西方真实绘画与水墨画真实绘画之间的比较表明：</p><ul><li>1）就一幅画的构图而言，西方绘画在整个图像上充满了色彩，而中国水墨画则有一定的空白区域；</li><li>2）就表达技巧而言，西方绘画很少使用强线，而它指的是白皮书中的一些区域，中国水墨画家特意留下这些区域来启发观众去想象。会议：2018年10月22日至26日，韩国首尔,中国水墨画采用线条鲜明的笔触来强调轮廓中的物体；</li><li>3）在色彩丰富性方面，西方绘画倾向于使用多种颜色，而中国水墨画主要使用不同灰度的墨水，这些墨水会扩散到一张宣纸上（水墨色调和扩散）</li></ul><p>​为了实现中国水墨画的风格转换，我们提出了一种基于生成对抗网络（GAN）的中国水墨画风格转换解决方案，名为CHIPGAN。根据中国水墨画的三种技法，我们提出了三种特殊的约束：空白、笔触、水墨色调和扩散。</p><blockquote><p>对于空洞，我们的约束结合了<strong>对抗损失</strong>和<strong>周期一致性损失</strong>，因为它们的目的是通过将信息转换为不可感知的信号来生成更真实的结果，从而留下白色区域。</p></blockquote><blockquote><p>对于画笔笔划，我们嵌入了一个预先训练好的<strong>整体嵌套边缘检测器</strong>（holistically-nestededgedetector），并在照片和假画的边缘映射之间加强重新设计的交叉熵损失，以强调有力的线条。</p></blockquote><blockquote><p>对于水墨扩散和色调，我们使用<strong>腐蚀和模糊</strong>(eroded andblurred)的图像来模拟这种绘画特性，并提出了<strong>水墨鉴别器</strong>来区分处理过的真假画</p></blockquote><p>​现有的绘画数据集主要包含西方画家的作品（如梵高、莫奈等），没有包含相应中国水墨画的真实照片和图像的可用数据集。为了解决我们的问题，我们提供了一个中国水墨画数据集，其中包括从互联网和艺术工作室收集的真实场景照片和绘画图像，我们的数据集名为“ChipPhi”。</p><blockquote><p>数据集包括</p><ul><li>1630张马的照片（不同颜色和不同姿势）</li><li>912张徐悲鸿的绘画图像</li><li>1976张风景照片（世界著名风景）</li><li>1542张黄宾虹的绘画图像的景观数据集</li></ul></blockquote><p>总之，本文的贡献有三个方面</p><ul><li>我们提出了ChipGAN，这是第一个执行从照片到中国水墨画风格转换的3个弱监督的深度网络架构，特别考虑了中国水墨画的三个基本技术：空白、笔触、水墨色调和扩散</li><li>我们引入了专业艺术家参与的风格化知觉研究，以评估原画和真画之间的风格一致性，并借助深度神经网络分析中国水墨画家的技巧</li><li>我们建立了第一个包含中国水墨画真实场景和图像的数据集ChipPhi，以便于训练和测试所提出的方法，并有助于中国水墨画风格转移的后续研究</li></ul><h1 id="相关工作">相关工作</h1><p>​<strong>图像风格迁移</strong>意味着将某个示例图像的样式迁移到目标图像。以前的图像级风格转换可以分为纹理合成和基于approache的卷积神经网络</p><p>​<strong>域级风格传递</strong>是指将给定的图像（如照片）与某个域的风格（如某个画家的风格）进行转换。它是通过基于生成性对抗网络（GAN）的方法实现的。</p><p>​ 此外，我们还回顾了一些专门为中国水墨画设计的计算方法</p><ul><li><p>纹理合成</p><blockquote><p>有一些非参数算法可以通过对给定的纹理图像重新采样来合成纹理。</p><p>Efros和Freeman提出了一种对应映射，根据目标图像的图像强度约束纹理合成过程。</p><p>Ashikhmin专注于传输高频纹理，但保留目标图像的比例。</p><p>Hertzmanet al.应用图像类比将源图像的样式转换为目标图像。</p><p>然而，由于纹理合成主要依赖于面片和低层次的表现，它们无法传递艺术作品的语义风格</p></blockquote></li><li><p>CNN based approaches</p><blockquote><p>基于CNN的模型旨在通过预先训练的卷积神经网络提取语义表示。</p><p>Gatysetal.[11]首先使用CNN获取图像的表示，并在自然照片上重现著名的喘息风格。</p><p>Liet al.[30]发现线性核是极大平方的一个很好的替代品。</p><p>Yin[48]和Chen及Hsu[3]研究了内容感知神经风格转移，并改善了结果。</p><p>这些方法大多存在速度慢、计算量大的问题，可通过[21,39]中的方法进行加速。Li和Wand[29]训练了一个马尔可夫前馈网络来解决效率问题。</p><p>Dumoulinet等人[7]建议同时学习多种风格。虽然这些方法为西方绘画创造了令人印象深刻的形象，但由于中国水墨画的本质不同，它们无法传递中国水墨画的风格</p></blockquote></li><li><p>GAN based approaches</p><blockquote><p>从GAN的角度处理风格转换任务时，一些图像到图像的翻译方法是相当有效的。</p><p>CoupledGAN[34]通过实施权重共享约束来学习多域图像的关联分布。</p><p>然而，这种方法只能以一个noise vector作为输入来生成成对的图像。</p><p>因此，它不能直接用作样式转换模型。</p><p>Liuet等人[33]将CoupledGAN[34]与变分自动编码器[24]结合起来，提出了一个名为UNIT[33]的框架。</p><p>Zhuet al.引入循环一致性损失来减少映射的置换，并提出CycleGAN[50]。</p><p>基于CycleGAN[50]的体系结构，DistanceGAN[2]实施了一个约束，即在映射到另一个域时，一个域中两个样本的距离应保持不变。</p><p>我们还在模型中选择周期一致性损失来克服模式崩溃[13]，并将其与对抗性损失相结合来模拟空洞。</p><p>虽然周期一致性损失使模型保留了原始照片中的一些细节，但它同时也会错误地删除一些重要的笔画，这促使我们为中国水墨画的笔画建模设置额外的约束</p></blockquote></li><li><p>Computational methods for Chinese ink wash paintings</p><blockquote><p>中国水墨画可以使用不同的计算方法生成。</p><p>Yuetal.将真实绘画的笔触纹理与给定景观图像的颜色信息相结合，合成一幅水墨画。</p><p>Xuetal.用事先准备好的工具分解中国水墨画的笔触用于渲染动画的笔刷笔划库。</p><p>Yang和Xu通过提供自动笔刷笔划轨迹估计，进一步完善了笔刷笔划分解方法。</p><p>Wang基于Kubelka-Munk方程，提出了一种模拟水墨扩散的有效算法。</p><p>Yehet al.和Wayetal.基于3D模型的板线笔划和内部着色生成链接水墨画。</p><p>Liang和Jin通过对边缘、颜色和纸张纹理的图像处理，从给定的照片生成水墨画。</p><p>我们的方法不再像以前那样依赖现有的<strong>笔画模拟</strong>和<strong>低级图像特征</strong>，而是<strong>探索数据驱动技术来学习真实的中国水墨画特征表示</strong></p></blockquote></li></ul><h1 id="提议的方法">提议的方法</h1><p>chipGAN学习从照片领域（例如，由现实世界的马的照片定义）到绘画领域（例如，由中国水墨画的马定义）的映射。</p><ul><li><p>在<strong>空白约束</strong>中，我们结合<strong>循环一致性损失</strong>和对抗性损失作为处理空白(voids)技术的约束条件</p></li><li><p>在<strong>笔触约束</strong>中，提出了brushstrokeloss去除不必要的笔触，同时保留精华；</p></li><li><p>在<strong>水墨约束</strong>中添加了<strong>扩散效应</strong>（diffusioneffect）和<strong>水墨损失</strong>，以确保整个图像的正确色调</p></li></ul><p><img src="chipan结构图.png" /></p><h2 id="void-constraint">1.Void constraint</h2><p>直观地说，应用空白意味着在画布上的适当位置留下空白。</p><p>以马为例，适当地应用空洞需要生成的图像完全忽略天空，并且部分地忽略照片中的草，同时清晰地保持马的轮廓，如图的中间部分所示。</p><p>马的照片和一幅中国水墨画具有不同的熵，因为照片与绘画图像相比具有丰富的颜色和纹理。</p><p>在图像到图像的翻译任务中，利用源域和目标域之间的这种不同熵，通过组合对抗性损失和循环一致性损失，将源图像的信息有效地转换为几乎不可感知的信号。</p><p>因此，我们采用了类似的策略来实施空白约束</p><h3 id="adversarial-loss">Adversarial loss</h3><p>给出被认为是分别为X和Y的域的未配对的训练集合</p><p>我们的模型给出两个映射关系：<span class="math inline">\(G :X\rightarrow Y \; F:Y \rightarrow X\)</span></p><p>对于$G : XY <span class="math inline">\(以及他的辨别器\)</span>D_Y$的对抗损失是这样定义的</p><p><img src="adv%20loss.png" /></p><h3 id="cycle-consistency-loss">Cycle consistency loss</h3><p>​ 我们通过将给定的图像X从domain X totarget域Y中翻译出来，然后返回到domainX，这将产生相同的图像，从而增加循环一致性约束，公式表达为<spanclass="math inline">\(F(G(X))\approx X\)</span>。由于循环一致性约束要求在两个方向上进行恢复，因此对于每个在DomainY的图像，还存在一个循环一致性约束$G(F(Y)) Y $</p><p><img src="cycle%20loss.png" /></p><p>这种约束使得生成的图像保留了源域的一些信息，从而可以将生成的图像转换回源域。</p><h2 id="brush-stroke-constraint">Brush stroke constraint</h2><p><img src="笔触结果.png" alt="截屏2022-03-13 13.34.26" style="zoom:20%;" /></p><p>​考虑到正确生成的空白区域，我们的下一个目标是增加笔触，以清晰地描绘中国水墨画风格的物体轮廓例如马的头部和身体应该有强烈的轮廓。</p><p>​为了以统一的方式对中国水墨画中不同厚度的各种类型的笔划进行建模，我们制定了我们的<strong>笔触约束</strong>，用于加强真实照片和生成绘画的不同级别的边缘映射之间的一致性。</p><p>​我们采用<strong>整体嵌套的边缘检测器</strong>从输入图像中提取五层边缘，以模拟五种不同厚度的笔划。</p><p>​然后，我们合并从预先训练的VGG-16特征提取程序的不同阶段生成的边缘映射，以获得最终的边缘映射。与将边缘检测任务视为像素级<strong>二值分类问题</strong>不同，我们从<strong>回归</strong>的角度训练了一个多级边缘检测器，以获得<strong>不同厚度</strong>的平滑笔划。</p><p>​ training groundtruth中的每个像素都用0到1的实数标记，这表明它们可能是边缘的一部分。</p><p>​ 我们获得真实图像的edge map E(x) 以及 生成图像的edge map E(G(x))</p><p>​ 然后我们将E(x)作为ground truth然后计算平衡交叉熵损失以让G生成正确的笔触</p><p><img src="笔触.png" /></p><ul><li>N是照片或假画边缘图中的像素总数</li><li><span class="math inline">\(\mu\)</span>是一个平衡权重<ul><li><span class="math inline">\(\mu\)</span> =N_/N and 1-<spanclass="math inline">\(\mu\)</span> = N+/N</li><li>N_ 是边缘图中所有不是边缘点的可能性之和</li><li>N+是边缘图中所有是边缘点的可能性之和</li></ul></li></ul><h2 id="ink-wash-constraint">ink wash constraint</h2><p>​正确模拟了空洞和笔触，我们的最终处理是使<strong>全局色调</strong>（例如，生成的马画的整体色温应接近真实色温）和<strong>扩散效果</strong>（例如，马的腹部显示链接扩散到宣纸上的不同灰度）在真实绘制和生成绘制g（x）之间保持一致。因此，我们进一步引入了<strong>水墨约束</strong>。</p><p>​水墨在宣纸上的扩散是近似各向同性的，所以我们用<strong>侵蚀操作</strong>和<strong>高斯模糊操作</strong>来模拟它。</p><p>​当突出的物体被模糊时，这个操作会抑制纹理和内容信息的显式比较，因此，该模型更倾向于关注tone（风格？）的一致性，如图所示</p><p><img src="ink%20water.png" /></p><p>因此，我们添加了一个对抗性鉴别器<spanclass="math inline">\(D_I\)</span>，用于区分<spanclass="math inline">\(y_{eb}\)</span>和<spanclass="math inline">\(G(x)_eb\)</span></p><p><img src="ink%20water%20yeb.png" /></p><ul><li>y_eb是经过侵蚀和模糊处理的真实绘画</li><li>G(x)_eb是经过侵蚀和模糊处理的生成绘画</li><li>⊖是侵蚀操作</li><li>B是一个侵蚀核</li><li>高斯模糊核 <span class="math inline">\(G_{k,l} =\frac{1}{2\pi\sigma^2}exp(-\frac{k^2+l^2}{2\sigma^2})\)</span></li></ul><p>最后我们定一个水墨画损失:</p><p><img src="ink%20water%20loss.png" /></p><h2 id="full-objective">Full objective</h2><p>我们的全部目标是上述四种损失的线性组</p><p><img src="funll%20loss.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概要&quot;&gt;概要&lt;/h1&gt;
&lt;p&gt;​
风格转换已成功应用于照片，生成逼真的西方绘画。然而，由于中西绘画技法的内在差异，直接套用已有的方法，对中国水墨画风格的转换并不能产生令人满意的效果。本文提出了一种基于ChipGAN的端到端（
end-to-end）生成对抗性网络体</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
  </entry>
  
  <entry>
    <title>MGUIT</title>
    <link href="http://www.larryai.com/2022/05/04/MGUIT/"/>
    <id>http://www.larryai.com/2022/05/04/MGUIT/</id>
    <published>2022-05-04T07:19:48.000Z</published>
    <updated>2022-05-04T07:39:26.647Z</updated>
    
    <content type="html"><![CDATA[<p><img src="1.png" /></p><h1 id="class-aware-memory-network">Class-aware Memory Network</h1><p><img src="2.png" /></p><h2 id="结构理解">结构理解</h2><blockquote><p>每个class下有<spanclass="math inline">\(N_k\)</span>个item,所有class的item数之和为N，即一共有N个item。</p><p>满足 <span class="math inline">\(\sum_{k=1}^K N_k = N\)</span></p></blockquote><h3 id="对于每个class要用n_k个item的理解">对于每个class要用<spanclass="math inline">\(N_k\)</span>个item的理解：</h3><blockquote><p>比如一个class为人脸，那么人脸上的眼睛、嘴唇、皮肤都是不一样的特征，因此需要用到多个item表示一个class</p></blockquote><h3 id="item的组成">item的组成</h3><blockquote><p>每个item由( k , <span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span>)组成</p><ul><li>k , <span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span> 的大小都是 1x1xC 其中C是通道数</li><li>k用来检索item 大小1x1xC 方便与每个像素进行求余弦相似度用来表示content feature</li><li><span class="math inline">\(v^x\)</span>表示在 x domin 上的stylefeature</li><li><span class="math inline">\(v^y\)</span>表示在 y domin 上的stylefeature</li></ul></blockquote><h3 id="特征聚类">特征聚类</h3><blockquote><p>训练的时候，会用到数据集中的objectannotations（对象注释比如物体的种类，物体的框的位置）来辅助。</p><p>每张图的特征(c,s)会被聚类为K类：（K就是这张图像的所有class）</p><p>(𝑐1,𝑠1),···,(𝑐𝐾,𝑠𝐾)</p><ul><li><p>c1中有<span class="math inline">\(N_1^c\)</span>个items1中有<span class="math inline">\(N_1^s\)</span>个item</p></li><li><p><span class="math inline">\(c_k\)</span>中 有<spanclass="math inline">\(N_k^c\)</span>个item <spanclass="math inline">\(s_k\)</span>中 有<spanclass="math inline">\(N_k^s\)</span>个item</p></li></ul><p>这些聚类后的信息read / update memory</p></blockquote><h2 id="read">Read</h2><h3 id="前置知识">前置知识</h3><ul><li><p>假设每张图像大小为CxHxW ， 那么一个通道上面就有P=HxW个像素点每个像素点的content feature 为 1x1xC</p></li><li><p>memory中有K个class，N个item 每个item有一个k其中k的大小为1x1xC</p></li><li><p>余弦相似度的公式定义 <span class="math display">\[d(c_p , k_n) = \frac{c_pk_n^T}{|\left |  c_p|\right|_2 |\left|  k_n|\right|_2}\]</span></p></li></ul><h3id="计算第p个像素点与第n个item的相似度权重">计算第p个像素点与第n个item的相似度权重</h3><blockquote><p>一共有P个像素点，第p个像素点的大小为1x1xC，记为<spanclass="math inline">\(c_p\)</span></p><p>一共有N个item，第n个item的k的大小为1x1xC，记为<spanclass="math inline">\(k_n\)</span></p></blockquote><p>计算<span class="math inline">\(c_p\)</span>与<spanclass="math inline">\(k_n\)</span>的相似度，并在N的维度上求softmax作为每个kn的权重</p><p><img src="3.png" /></p><ul><li><span class="math inline">\(\alpha_{p,n}^x\)</span>表示对于xdomin上第p个像素点与memory中第n个item的k的相似度权重（属于第n类的概率）</li><li><span class="math inline">\(\alpha_{p,n}^y\)</span>表示对于ydomin上第p个像素点与memory中第n个item的k的相似度权重（属于第n类的概率）</li></ul><h3 id="计算风格特征hats">计算风格特征<spanclass="math inline">\(\hat{s}\)</span></h3><blockquote><p>第p个像素点的风格特征就是 对每个item的style feature 进行加权求和 ，其中权重是<spanclass="math inline">\(c_p\)</span>与item的k的相似度权重</p></blockquote><p><img src="4.png" /></p><ul><li><span class="math inline">\(v_{n}^x\)</span>表示xdomin上第n个item的style feature</li><li><span class="math inline">\(v_{n}^y\)</span>表示ydomin上第n个item的style feature</li></ul><blockquote><p>最终所有的p聚合成最后的特征图</p></blockquote><h2 id="update">update</h2><p>update是求权重然后按权重更新（<span class="math inline">\(k\)</span>, <span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span>）</p><h3id="计算第n个item的k与第p个像素点的相似度权重">计算第n个item的k与第p个像素点的相似度权重</h3><p><img src="5.png" /></p><blockquote><p>与read的计算权重不同，update是在P的维度上求softmax作为每个p的权重（第n个类与哪些p比较接近）</p></blockquote><h3 id="更新k-vx-vy">更新（<span class="math inline">\(k\)</span> ,<span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span>）</h3><blockquote><p>对第n个item的content feature进行更新 加上每个像素点的contentfeature的加权和</p><p>对第n个item的style feature进行更新 加上每个像素点对应的stylefeature的加权和</p></blockquote><p><img src="6.png" /></p><blockquote><p>k保存的content feature是两个domain共享的，所以一起更新</p><p>而v保存的style feature是两个domain单独的，所以分开更新。</p></blockquote><p>更新步骤如下:</p><p><img src="7.png" /></p><h1 id="loss-function">Loss Function</h1><h2 id="image-to-image-translation-network">image-to-image translationnetwork</h2><h3 id="reconstruction-loss-重建损失">1.Reconstruction loss(重建损失)</h3><p><img src="8.png" /></p><ul><li><span class="math inline">\(L^{self}\)</span>是x的原始contentfeature与构建的x style生成的图像与原图越逼近 ， 使得xstyle更加精确表达x的风格</li><li><spanclass="math inline">\(L^{cyc}\)</span>是将x内容特征与y风格特征产生的图像再用x风格特征产生的图像与 原图逼近</li></ul><h3 id="adversarial-loss">2.Adversarial loss</h3><blockquote><p>目的是为了最小化两个不同功能之间的分布差异</p><p>我们采用了两种对抗性损失函数：</p><ul><li><strong>contentdiscriminato</strong>：Cx和Cy之间的内容对抗性损失函数<ul><li>使得x的内容在y风格下仍旧保持原本的内容</li></ul></li><li><strong>domain discriminator</strong>：X和Y领域对抗性损失函数</li></ul></blockquote><h3 id="kl-loss">3.KL loss</h3><blockquote><p>使style的分布更接近先前的高斯分布</p></blockquote><h3 id="latent-regression-loss">4.Latent regression loss</h3><blockquote><p>使得style和image之间的映射是可逆的</p></blockquote><h2 id="class-aware-memory-network-1">Class-aware memory network</h2><h3 id="feature-contrastive-loss特征对比损失">Feature contrastiveloss（特征对比损失）</h3><blockquote><p>对于每一个特征<span class="math inline">\(c_p\)</span>(或<spanclass="math inline">\(s_p\)</span>)，我们将它最近的item <spanclass="math inline">\(k_p\)</span>（或<spanclass="math inline">\(v_p\)</span>）定义为正样本，其他样本为负样本。</p><p>到正/负样本的距离如如下方式惩罚</p><p>τ是控制浓度分布水平的温度参数</p></blockquote><p><img src="9.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;1.png&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;class-aware-memory-network&quot;&gt;Class-aware Memory Network&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;2.png&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;结构理解&quot;&gt;结构理解</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
