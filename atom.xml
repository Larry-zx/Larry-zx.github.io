<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扁同学不发言的个人博客</title>
  
  
  <link href="http://www.larryai.com/atom.xml" rel="self"/>
  
  <link href="http://www.larryai.com/"/>
  <updated>2022-05-14T07:50:47.470Z</updated>
  <id>http://www.larryai.com/</id>
  
  <author>
    <name>扁同学不发言</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux基本指令</title>
    <link href="http://www.larryai.com/2022/05/14/Linux%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/"/>
    <id>http://www.larryai.com/2022/05/14/Linux%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/</id>
    <published>2022-05-14T07:32:55.000Z</published>
    <updated>2022-05-14T07:50:47.470Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本概念">基本概念</h1><h2 id="标准文件">标准文件</h2><ul><li>stdin 标准输入</li><li>stdout 标准输出</li><li>stderror 标准错误</li></ul><h1 id="基本命令">基本命令</h1><h2 id="cat命令">Cat命令</h2><h3 id="功能">功能：</h3><p>拼接文件</p><blockquote><p>cat命令的功能是拼接文件 ， cat并不适合查看文件</p><p>因为当cat命令默认重定向到标准输出stdout ，所以能看到显示屏上看到文件</p></blockquote><p>当文件内容较多时，cat命令显示文件就会快速滚屏，且没有滚动条</p><h3 id="使用">使用</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> 1.txt 2.txt &gt; 3.txt</span><br></pre></td></tr></table></figure><h2 id="more命令">more命令</h2><h3 id="功能-1">功能:</h3><p>查看文件</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基本概念&quot;&gt;基本概念&lt;/h1&gt;
&lt;h2 id=&quot;标准文件&quot;&gt;标准文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;stdin 标准输入&lt;/li&gt;
&lt;li&gt;stdout 标准输出&lt;/li&gt;
&lt;li&gt;stderror 标准错误&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;基本命令&quot;&gt;基本命</summary>
      
    
    
    
    
    <category term="Linux" scheme="http://www.larryai.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>StyTR2</title>
    <link href="http://www.larryai.com/2022/05/06/StyTR2/"/>
    <id>http://www.larryai.com/2022/05/06/StyTR2/</id>
    <published>2022-05-06T12:18:40.000Z</published>
    <updated>2022-05-07T00:11:38.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p><strong><em>风格转换的目标是在保持原有内容的同时，在风格参照的指导下呈现出具有艺术特征的图像</em></strong></p><p>由于卷积神经网络（CNN）的局部性，很难提取和维护输入图像的全局信息。</p><p>因此，<strong>传统的神经风格转换方法面临着有偏见的内容表示</strong></p><p>为了解决这个关键问题，我们提出了一种基于转换器的方法，称为StyTr2 &gt;将输入图像的长期依赖性考虑到图像样式传输中</p><p>与其他视觉任务的视觉转换器不同<strong>StyTr2包含两个不同的转换器编码器，分别为内容和样式生成特定于域的序列</strong>在编码器之后<strong>采用多层转换器解码器根据样式序列对内容序列进行样式化</strong></p><p>我们还分析了现有位置编码方法的不足，提出了<strong>内容感知位置编码（CAPE）</strong>&gt; 它具有尺度不变性，更适合于图像样式传输任务</p><p>定性和定量实验表明，与最先进的基于CNN和基于流的方法相比，所提出的StyTr2是有效的。</p><p>代码和模型可在https://github.com/diyiiyiii/StyTR-2</p><h1 id="introduce">1.Introduce</h1><p>此处省略一些内容 详细可查看原论文</p><p>总之，我们的主要贡献包括 -一个名为StyTr2的基于转换器的风格转换框架，以生成风格化结果，并保留输入内容图像的结构和细节- 一种基于内容的位置编码方案，具有尺度不变性，适用于样式转换任务 -综合实验表明，StyTr2形成了基线方法，并以理想的内容结构和风格模式取得了显著的效果</p><h1 id="relate-work">2.Relate Work</h1><ul><li>图像风格迁移</li><li>视觉任务的transformer &gt;在本文中，我们介绍了用于样式转换任务的基于变换器的结构，可以将其视为图像块的序列到序列生成</li><li>位置编码 &gt;在本文中，我们提出了一种基于内容的位置编码机制，该机制具有尺度不变性，更适合于图像生成任务</li></ul><h1 id="method">3.Method</h1><p>为了利用transformers的功能捕获图像特征的长距离依赖性以进行样式转换</p><p>我们将该问题描述为一个连续的补丁生成任务</p><p>给定一个内容图像image (H,W,3) 并显示一个样式图像style (H,W,3)</p><p>我们将两幅图像分割成块patch（类似于NLP任务中的标记）</p><p>使用线性投影层将输入块投影到型如L×dim中嵌入 <spanclass="math inline">\(\varepsilon\)</span> 的序列特征中</p><p><span class="math display">\[L = \frac{H\times W}{m\times m}\]</span></p><blockquote><p>L是特征序列的长度</p><p>m=8是patches的size</p><p>dim是特征序列的维度</p></blockquote><h2 id="content-aware-positional-encodingcape">3.1Content-AwarePositional Encoding（CAPE）</h2><p>当使用transformer-based的模型时，位置编码（PE）应包含在输入序列中，以获取结构信息</p><p>第i个patch和第j个patch的注意力得分计算如下：</p><p><img src="公式1.png" /> &gt; Wq 用于查询的参数矩阵 &gt;Wk用于密钥计算的参数矩阵 &gt; Pi 第i个一维的PE</p><blockquote><p>在二维情况下两个像素点(xi,yi) (xj,yj)之间的位置相对关系:</p></blockquote><p><img src="公式2.png" /></p><ul><li><span class="math inline">\(w_{k} =\frac{1}{1000^{(\frac{2k}{128})}}\)</span></li><li>d = 512</li></ul><p><strong>两个patch之间的位置相对关系仅取决于它们的空间距离</strong></p><p>因此，我们提出两个问题:</p><p><strong>第一</strong></p><p>对于图像生成任务，在计算PE时是否应该考虑图像语义？</p><p>传统的PE是为按逻辑排列的句子设计的，但图像补丁是根据内容组织的。</p><p>我们将两个patch之间的距离表示为d( · , · )</p><p><img src="fig3.png" /></p><p>在图3(a)的右侧</p><p><strong>d((x0,y3),(x1,y3))</strong>(红色和绿色补丁) 和<strong>d((x0,y3),(x3,y3))</strong>(红色和青色补丁)之间的差异应该很小</p><blockquote><p>因为我们预计类似的内容patch会有类似的样式化结果</p></blockquote><p><strong>第二</strong></p><p>当输入图像的大小呈指数增长时传统的正弦位置编码是否仍然适用于视觉任务？</p><p>如图3(a)所示</p><p>调整图像大小时相同位置的面片（用蓝色小矩形表示）之间的相对距离可能会发生显著变化</p><p>这可能不适用于视觉任务中的多尺度方法</p><p>为此，我们提出了<strong>内容感知位置编码（CAPE）</strong></p><blockquote><p>它是尺度不变的，更适合风格迁移任务</p></blockquote><p>与仅考虑补丁相对距离的正弦 PE 不同，CAPE 以图像内容的语义为条件</p><p>我们假设使用 n × n 位置编码足以表示图像的语义</p><p>对于图像 <span class="math inline">\(I \in \mathbb{R}^{H \times W\times 3}\)</span> , 我们将固定的 n × n 位置编码重新缩放为<spanclass="math inline">\(\frac{H}{m} \times \frac{H}{m}\)</span> ，如图3(b) 所示</p><p>这样，<strong>各种图像尺度就不会影响两个补丁之间的空间关系</strong></p><p>补丁 (x, y) 的 CAPE 即<strong>PCA(x, y)</strong>被表述为</p><p><img src="公式3.png" /></p><ul><li><span class="math inline">\(AvgPool_{n\times n}\)</span>是平均池化函数</li><li><span class="math inline">\(\mathcal{F}_{pos}\)</span> 是 1 × 1卷积运算，用作可学习的位置编码函数</li><li><span class="math inline">\(\mathcal{P}_{\mathcal{L}}\)</span>是遵循序列<spanclass="math inline">\(\varepsilon\)</span>的可学习PE</li><li>在我们的实验中n 设置为 18</li><li>$a_{kl} $是插值权重，s 是相邻块的数量</li><li>最后，我们将 <span class="math inline">\(P_{CA_{i}}\)</span> 添加到<span class="math inline">\(\varepsilon_{i}\)</span>，作为第 i个补丁在像素位置 (x, y) 的最终特征嵌入</li></ul><h2 id="style-transfer-transformer">3.2 Style Transfer Transformer</h2><h4 id="transformer-编码器">3.2.1 Transformer 编码器</h4><p>我们通过使用基于 Transformer 的结构来学习<strong>顺序视觉表示</strong> 来<strong>捕获图像块的长期依赖关系</strong></p><p>与其他视觉任务不同，tjr风格迁移任务的输入来自两个不同的领域，分别对应于自然图像和艺术绘画</p><p>因此，StyTr2有两个转换器编码器来<strong>编码特定领域的特征</strong>，用于在下一阶段将序列从一个域转换到另一个域</p><p>给定输入内容序列 <span class="math inline">\(Z_c = \{\varepsilon_{ci} + \mathcal{P_{CA}}_i\}\)</span> 的嵌入</p><p>我们首先将其输入到转换器编码器中</p><p>编码器的每一层都由一个多头自注意力模块（MSA）和一个前馈网络（FFN）组成</p><p>输入序列被编码为查询（Q）、键（K）和值（V）： <spanclass="math display">\[Q =Z_cW_q , K=Z_cW_k , V=Z_cW_v\]</span></p><ul><li><span class="math inline">\(W_q , W_k , W_v \in \mathbb{R}^{C \timesd_{head}}\)</span></li></ul><p>multi-head Attention的计算方式</p><p><img src="4.png" /></p><ul><li><span class="math inline">\(W_0 \in \mathbb{R}^{C \timesC}\)</span>是可学习的参数</li><li>N 是注意力头的数量，并且 <span class="math inline">\(d_{head} =\frac{C}{N}\)</span></li></ul><p>应用残差连接来获得编码的内容序列 Yc</p><p><img src="5.png" /></p><ul><li>FFN是激活函数为relu的MLP</li><li>LN被应用到每一个块的末尾</li></ul><p>类似地，输入样式序列 Zs = {Es1, Es2, ..., EsL}的嵌入按照相同的计算过程编码为序列 Ys</p><p>只是不考虑位置编码，因为我们不需要维护 最终输出中的输入样式</p><h4 id="transformer-解码器">3.2.2 Transformer 解码器</h4><p>我们的转换器解码器用于根据编码样式序列 Ys 以回归方式翻译编码内容序列Yc</p><p>与 NLP任务中的自回归过程不同，我们一次将所有顺序补丁作为输入来预测输出</p><p>如图 3(a) 所示，每个 Transformer 解码器层包含两个 MSA 层和一个FFN</p><p>我们的 Transformer 解码器的输入包括编码后的内容序列</p><p>即 <span class="math inline">\(\bar{Y_c}\)</span> = {Yc1 + PCA1, Yc2+ PCA2, ..., YcL + PCAl} 以及样式序列 Ys = {Ys1, Ys2, ..., YSL}</p><blockquote><p>我们使用<strong>内容序列生成查询 Q，并使用样式序列生成键 K 和值V</strong></p></blockquote><p><span class="math display">\[Q =\bar{Y_c}W_q , K=Y_sW_k , V=Y_sW_v\]</span></p><p>然后，transformer解码器的输出序列X可以计算为</p><p><img src="6.png" /></p><h4 id="cnn解码器">3.2.3 CNN解码器</h4><p>Transformer 的输出序列 X 的形状为 【HW/64 , C】</p><p>我们没有直接对输出序列进行上采样来构造最终结果</p><p>而是使用三层 CNN 解码器来细化 Transformer 解码器的输出</p><p>对于每一层，我们通过采用包括 3 × 3 Conv + ReLU + 2 × Upsample在内的一系列操作来扩大规模</p><p>最后，我们可以得到分辨率为 H × W × 3 的最终结果</p><h2 id="network-optimization">3.3 Network Optimization</h2><p>生成的结果应保持原始内容结构和参考样式模式</p><p>因此，我们构造了两个不同的感知损失项来衡量</p><ul><li><p>输出图像 Io 和输入内容图像 Ic之间的<strong>内容差异</strong></p></li><li><p>Io 和输入风格参考 Is 之间的<strong>风格差异</strong></p></li></ul><p>我们使用由预训练的 VGG模型提取的特征图来构建之后的内容损失和样式损失</p><p>内容感知损失 Lc 定义为</p><p><img src="7.png" /></p><ul><li>其中<span class="math inline">\(\phi_i()\)</span> 表示从预训练 VGG19中的第 i 层提取的特征，<span class="math inline">\(N_l\)</span>是层数。</li></ul><p>风格感知损失 Ls 定义为</p><p><img src="8.png" /></p><ul><li>其中 μ(·) 和 σ(·) 分别表示提取特征的均值和方差。</li></ul><p>我们还采用<strong>身份损失</strong>来学习更丰富、更准确的内容和风格表示</p><p>具体来说，我们将两个相同的内容（风格）图像放入 StyTr2，生成的输出Icc(Iss) 应该与输入 Ic(Is) 相同</p><p>因此，我们计算两个身份损失项来衡量 Ic(Is) 和 Icc(Iss)之间的差异：</p><p><img src="9.png" /></p><p>通过最小化以下函数来优化整个网络：</p><p><img src="10.png" /></p><p>我们将 λc、λs、λid1 和 λid2 设置为 10、7、50 和1，以减轻幅度差异的影响</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;风格转换的目标是在保持原有内容的同时，在风格参照的指导下呈现出具有艺术特征的图像&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;由于卷积神经网络（CNN）的局部性，很难提取和维护输入图像的全</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://www.larryai.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Pycharm快捷键</title>
    <link href="http://www.larryai.com/2022/05/06/Pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://www.larryai.com/2022/05/06/Pycharm%E5%BF%AB%E6%8D%B7%E9%94%AE/</id>
    <published>2022-05-06T10:54:14.000Z</published>
    <updated>2022-05-06T11:32:16.665Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pycharm-for-mac">Pycharm for Mac</h1><h4 id="l-使代码遵守pep8规范自动整齐代码">⌘ + ⌥ + L使代码遵守pep8规范（自动整齐代码）</h4><h4 id="b-查看申明-查看源代码">⌘ + B 查看申明 / 查看源代码</h4><h4 id="w-关闭当前文件标签">⌘ + W 关闭当前文件标签</h4><h4 id="f-查找">⌘ + F 查找</h4><h4 id="r-替换">⌘ + R 替换</h4><h4 id="k-更新到vcs">⌘ + K 更新到VCS</h4><h4 id="d-复制当前一整行">⌘ + D 复制当前一整行</h4><h4 id="x-剪贴当前一整行">⌘ + X 剪贴当前一整行</h4><h4 id="v-粘贴缓存历史粘贴板全部内容">⌘ + ⇧ + V粘贴缓存（历史粘贴板全部内容）</h4><h4 id="o-查找类名">⌘ + O 查找类名</h4><h4 id="a-查找动作">⌘ + ⇧ + A 查找动作</h4><h4 id="p-参数提示">⌘ + P 参数提示</h4><h4 id="折叠代码">⌘ + - 折叠代码</h4><h4 id="展开代码">⌘ + = 展开代码</h4><h4 id="tab-切换文件窗口">⌃ + Tab 切换文件窗口</h4><h4 id="r-运行">⌃ + R 运行</h4><h4 id="r-换文件运行">⌃ + ⌥ + R 换文件运行</h4><h4 id="d-启动调试">⌃ + D 启动调试</h4><h4 id="h-调用层次结构">⌃ + ⌥ + H 调用层次结构</h4><h4 id="f6-重命名">⇧ + F6 重命名</h4><h4 id="回车-下一行">⇧ + 回车 下一行</h4><h4 id="选择代码开始处-选择代码结束处">⌘ + ⇧ + ⌥ + [ 选择代码开始处]选择代码结束处</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pycharm-for-mac&quot;&gt;Pycharm for Mac&lt;/h1&gt;
&lt;h4 id=&quot;l-使代码遵守pep8规范自动整齐代码&quot;&gt;⌘ + ⌥ + L
使代码遵守pep8规范（自动整齐代码）&lt;/h4&gt;
&lt;h4 id=&quot;b-查看申明-查看源代码&quot;&gt;⌘ + B 查看</summary>
      
    
    
    
    <category term="高效工具" scheme="http://www.larryai.com/categories/%E9%AB%98%E6%95%88%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="快捷键" scheme="http://www.larryai.com/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    
    <category term="Pycharm" scheme="http://www.larryai.com/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>ViT代码复现</title>
    <link href="http://www.larryai.com/2022/05/06/ViT/"/>
    <id>http://www.larryai.com/2022/05/06/ViT/</id>
    <published>2022-05-06T07:58:39.000Z</published>
    <updated>2022-05-06T12:22:57.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="patch-embedding模块">Patch-Embedding模块</h2><h3 id="section"><img src="2.png" /></h3><h3 id="转换流程">转换流程</h3><p>在<strong>Patch Embedding</strong>中</p><p>例如输入图片大小为256x256，将图片分为多个patch，每个patch大小为16x16</p><p>则每张图像会生成256x256/16x16=256个patches</p><ul><li>即输入序列长度L为patch_num = 256</li><li>每个patch维度dim = patchSize x patchSize x chanels=16x16x3 =768</li></ul><blockquote><p>输入的图像原始格式 [B , C , H ,W]</p><p>经过Patch-Embeding之后格式变为 [B , L , dim]</p><p>用上面的例子来讲便是[B , 3 , 256 ,256] -&gt; [B , 256 , 768]</p></blockquote><p><strong>线性投射层</strong>的维度为768xN(N=768)，因此输入通过线性投射层之后的维度依然为256x768</p><p>即一共有256个token，每个token的维度是768</p><p>代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">patch_embedding = nn.Sequential(</span><br><span class="line">  </span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2 = patch_width),</span><br><span class="line">  </span><br><span class="line">            nn.Linear(patch_dim, dim), <span class="comment">#线性投影层</span></span><br><span class="line">  </span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>最后还需要加上一个特殊字符cls，因此最终的维度是<strong>256x768</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = atch_embedding(img) <span class="comment">#对输入图像进行patch-embedding</span></span><br><span class="line">b, n, _ = x.shape <span class="comment">#获取Batch以及token数量</span></span><br><span class="line">cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim)) <span class="comment">#特殊字符cls</span></span><br><span class="line">cls_tokens = repeat(cls_token, <span class="string">&#x27;1 n d -&gt; b n d&#x27;</span>, b = b) <span class="comment">#将cls展到相等Batch</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>) <span class="comment">#进行拼接 token数量+1</span></span><br></pre></td></tr></table></figure><h2 id="positional-encoding模块">Positional encoding模块</h2><p><strong>位置编码</strong>可以理解为一个Nxdim的矩阵</p><ul><li>N就是token的数量</li><li>dim就是一个token的维度</li></ul><p>然后将位置编码进行与patch-embedding后的数据进行相加</p><blockquote><p>⚠️注意，这里是sum而非concat</p></blockquote><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">x += pos_embedding[:, :(n + <span class="number">1</span>)] <span class="comment">#n+1 是patch-embedding后加上cls之后总共的tokens</span></span><br></pre></td></tr></table></figure><h2 id="layernorm模块">LayerNorm模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LayerNorm</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br></pre></td></tr></table></figure><h2 id="attention模块">Attention模块</h2><p><img src="1.png" /></p><p>经过patch embedding 、pos_embedding 、LayerNorm之后</p><p>我们的数据要被分为VKQ那么会产生3个分支</p><p>研究发现将查询、键和值分别线性投影到 dk、dk 和 dv维度上的不同学习线性投影是有益的（投影到低维度）</p><blockquote><p>相当于给h次机会 希望能够学到不一样的投影的方式</p><p>使得在投影进去的度量空间里面 能够去匹配不同模式的相似函数</p><p>类似卷积神经网络中有多个输出通道的感觉</p></blockquote><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dk=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dk * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dk == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dk ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># [B , L ,dim] -&gt; [B , L , dk * heads * 3]</span></span><br><span class="line">        <span class="comment"># dk 主要是为了将dim变成dk 投影到低维度</span></span><br><span class="line">        <span class="comment"># heads  产生多个qkv 有多少头 产生多少个qkv</span></span><br><span class="line">        <span class="comment"># 3 数字3是输入复制成3分 给kqv</span></span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),  <span class="comment"># [B,L,dk*heads] - &gt; [B,L,dim]回到跟输入一样对格式</span></span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = [B , L , dim]</span></span><br><span class="line"></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [B , L , dk*heads ]  [B , L , dk*heads ]  [B , L , dk*heads ]</span></span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h=self.heads), qkv)</span><br><span class="line">        <span class="comment"># [B , L ,dk*heads] - &gt; [B , heads , L , dk]</span></span><br><span class="line"></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># 对最后两个维度进行转置 实现QK^T/sqrt(dk)</span></span><br><span class="line">        attn = self.softmax(dots)  <span class="comment"># softmax[QK^T/sqrt(dk)]</span></span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># softmax[QK^T/sqrt(dk)]V</span></span><br><span class="line"></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line">        <span class="comment"># [B , heads , L , dk] -&gt; [B , L , dk * heads]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure><h2 id="算法结构代码汇总">算法结构代码汇总</h2><p><ahref="https://github.com/lucidrains/vit-pytorch/blob/b3e90a265284ba4df00e19fe7a1fd97ba3e3c113/vit_pytorch/vit.py#L47">详细链接访问Github获取</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># tools</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># LayerNorm</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># MLP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dk=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dk * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dk == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dk ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># [B , L ,dim] -&gt; [B , L , dk * heads * 3]</span></span><br><span class="line">        <span class="comment"># dk 主要是为了将dim变成dk 投影到低维度</span></span><br><span class="line">        <span class="comment"># heads  产生多个qkv 有多少头 产生多少个qkv</span></span><br><span class="line">        <span class="comment"># 3 数字3是输入复制成3分 给kqv</span></span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),  <span class="comment"># [B,L,dk*heads] - &gt; [B,L,dim]回到跟输入一样对格式</span></span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = [B , L , dim]</span></span><br><span class="line"></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [B , L , dk*heads ]  [B , L , dk*heads ]  [B , L , dk*heads ]</span></span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h=self.heads), qkv)</span><br><span class="line">        <span class="comment"># [B , L ,dk*heads] - &gt; [B , heads , L , dk]</span></span><br><span class="line"></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># 对最后两个维度进行转置 实现QK^T/sqrt(dk)</span></span><br><span class="line">        attn = self.softmax(dots)  <span class="comment"># softmax[QK^T/sqrt(dk)]</span></span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># softmax[QK^T/sqrt(dk)]V</span></span><br><span class="line"></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line">        <span class="comment"># [B , heads , L , dk] -&gt; [B , L , dk * heads]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),</span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool=<span class="string">&#x27;cls&#x27;</span>, channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 dim_head=<span class="number">64</span>, dropout=<span class="number">0.</span>, emb_dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width // patch_width)</span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1=patch_height, p2=patch_width),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">            nn.Linear(dim, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;1 n d -&gt; b n d&#x27;</span>, b=b)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        x = x.mean(dim=<span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;patch-embedding模块&quot;&gt;Patch-Embedding模块&lt;/h2&gt;
&lt;h3 id=&quot;section&quot;&gt;&lt;img src=&quot;2.png&quot; /&gt;&lt;/h3&gt;
&lt;h3 id=&quot;转换流程&quot;&gt;转换流程&lt;/h3&gt;
&lt;p&gt;在&lt;strong&gt;Patch Embedd</summary>
      
    
    
    
    <category term="代码复现" scheme="http://www.larryai.com/categories/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python笔记</title>
    <link href="http://www.larryai.com/2022/05/06/python%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.larryai.com/2022/05/06/python%E7%AC%94%E8%AE%B0/</id>
    <published>2022-05-06T07:54:55.000Z</published>
    <updated>2022-05-06T11:29:25.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rearrange函数">Rearrange函数</h1><h2 id="作用">作用</h2><p><img src="1.png" /></p><p>在<strong>Patch Embedding</strong>中使用这个函数进行处理</p><p>例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16</p><p>则每张图像会生成224x224/16x16=196个patches</p><ul><li>即输入序列长度为patch_num = 196</li><li>每个patch维度 = patchSize x patchSize x chanels=16x16x3 = 768</li></ul><blockquote><p>相当于将[B , C , H , W ]的图片变成 [ B , PatchNum , dimension ]</p><p>其中 PathNum = <span class="math inline">\(\frac{H \times W}{p \timesp}\)</span> , dimension = <span class="math inline">\(p \times p \timesC\)</span></p></blockquote><h2 id="具体实现">具体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">patch_size = <span class="number">16</span></span><br><span class="line">img = torch.ones(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">fun = Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1=patch_size, p2=patch_size)</span><br><span class="line"></span><br><span class="line">out = fun(img)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img.shape)  <span class="comment"># torch.Size([1, 3, 224, 224])</span></span><br><span class="line"><span class="built_in">print</span>(out.shape)  <span class="comment"># torch.Size([1, 196, 768])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="chunks函数">chunks函数</h1><p>该函数在Transformer的Attention模块将输入投影后给QKV时用到</p><h2 id="作用-1">作用</h2><blockquote><p>若对最后一个维度进行chunks操作</p><p>[B , C , N , D*3] -&gt;[B ,C , N ,D] , [B ,C , N ,D] ,[B ,C , N,D]</p></blockquote><h2 id="具体实现-1">具体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.ones(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span> * <span class="number">3</span>)</span><br><span class="line">output = <span class="built_in">input</span>.chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)  <span class="comment"># torch.Size([1, 3, 672])</span></span><br><span class="line"><span class="built_in">print</span>(output[<span class="number">0</span>].shape)  <span class="comment"># torch.Size([1, 3, 224])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="adaptiveavgpool2d函数">AdaptiveAvgPool2d()函数</h1><h2 id="作用-2">作用</h2><blockquote><p>AdaptivePooling，自适应池化层</p><p>函数通过输入原始尺寸和目标尺寸，自适应地计算核的大小和每次移动的步长。</p><p>如告诉函数原来的矩阵是32x32的尺寸，我要得到18x18的尺寸，函数就会自己计算出核多大、该怎么运动。</p></blockquote><h2 id="具体实现-2">具体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.ones(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">autoPool = nn.AdaptiveAvgPool2d(output_size=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">output = autoPool(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input.shape&#x27;</span>, <span class="built_in">input</span>.shape) <span class="comment"># [1, 3, 256, 256]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output.shape&#x27;</span>, output.shape) <span class="comment"># [1, 3, 18, 18]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;rearrange函数&quot;&gt;Rearrange函数&lt;/h1&gt;
&lt;h2 id=&quot;作用&quot;&gt;作用&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;1.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;Patch Embedding&lt;/strong&gt;中使用这个函数进行处理&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="函数笔记" scheme="http://www.larryai.com/categories/%E5%87%BD%E6%95%B0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Python" scheme="http://www.larryai.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>MASA-SR</title>
    <link href="http://www.larryai.com/2022/05/04/MASA-SR/"/>
    <id>http://www.larryai.com/2022/05/04/MASA-SR/</id>
    <published>2022-05-04T07:58:52.000Z</published>
    <updated>2022-05-04T08:03:00.875Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" /></p><h2 id="主要组成">主要组成</h2><ul><li><p>编码器 Encoder</p></li><li><p>匹配与提取模块（MEM） Math &amp; Extraction Modules</p></li><li><p>空间自适应模块（SAM） Spatial Adaptation Modules</p><blockquote><p>将Ref特征的分布映射到LR特征的分布</p></blockquote></li><li><p>双残差聚合模块（DRAM）Dual Residual Aggregation Modules</p><blockquote><p>进行有效的特征融合</p></blockquote></li></ul><h3 id="名词解释">名词解释</h3><blockquote><ul><li>LR 低分辨率图像</li><li>Ref↓ 表示 x4双三次下采样参考图</li><li>Ref 参考图</li></ul></blockquote><h2 id="编码器">编码器</h2><ul><li><p>与之前使用预先训练过的VGG作为自然提取器的方法不同</p></li><li><p>这里的编码器与网络的其他部分一起从头开始训练的</p></li><li><p>编码器含有三个构造块</p><ul><li>第二个和第三个 利用stride=2的方法将featue map大小折半</li></ul></li><li><p>将Ref参考图传入编码器分别经过三个构造块得到三个不同比例的特征</p><ul><li>生成<span class="math inline">\(F_{Ref}^s\)</span> 其中s =1,2,4</li></ul></li><li><p>LR图像和Ref↓ 图像只经管编码器的第一个构造块</p><ul><li>生成<span class="math inline">\(F_{LR}\)</span> 和 <spanclass="math inline">\(F_{Ref↓ }\)</span></li></ul></li></ul><h2 id="matching-extraction-module-mem">Matching &amp; Extraction Module(MEM)</h2><h3 id="概述">概述</h3><p>​众所周知，在自然图像的局部区域中，相邻像素可能来自公共对象共享相似的颜色统计数据。以往对自然图像先验的研究也表明，一幅图像中的相邻斑块很可能会发现它们之间的对应关系在空间上是一致的。</p><p>​这促使我们提出了一种从粗到精的匹配方案，即粗块匹配和精块匹配。请注意，在我们的方法中，block和patch是两个不同的概念，block的大小大于patch（在我们的实验中patch为3×3的大小）。</p><p>​ 如图3所示，我们首先只在featurespace中找到block的对应关系。具体来说，我们将LR特征(<spanclass="math inline">\(F_{LR}\)</span>)展开为不重叠的block块。每个LRblock将找到其最相关的Ref↓ block。</p><p>​ 与以前的方法相比，通过这样做，匹配的计算成本显著降低。</p><p>​ 为了达到足够的精度，我们进一步对每一个&lt;LR block，Ref↓block&gt;对进行密集patch匹配</p><p>​ 在最后一个阶段，我们根据获得的对应信息提取有用的Ref feature</p><p><imgsrc="https://pic3.zhimg.com/80/v2-e551112dcfc05e03aecc23d0b457189e_1440w.jpg" /></p><h3 id="stage-1-coarse-matching粗匹配">Stage 1: Coarsematching（粗匹配）</h3><p>将<span class="math inline">\(F_{LR}\)</span>展开成K个不重复的blocks<span class="math display">\[\left\{  B_{LR}^0,B_{LR}^1,B_{LR}^2 ,...B_{LR}^{K-1} \right\}\]</span></p><blockquote><p>一个Block中有很多patch 对每个<spanclass="math inline">\(B_{LR}^k\)</span>块找到与它<strong>最相关</strong>的<spanclass="math inline">\(F_{Ref↓}\)</span> block 记作<spanclass="math inline">\(B_{Ref↓}^k\)</span></p></blockquote><p>首先将<span class="math inline">\(B_{LR}^k\)</span>的<strong>centerpatch</strong>与<spanclass="math inline">\(F_{Ref↓}\)</span>中的每一个patch进行计算<strong>余弦相似度</strong>(cosinesimilarity) <span class="math display">\[r_{c,j}^k = \left \langle \frac{p_c^k}{\left \| p_c^k \right \|} ,\frac{q_j}{\left \| q_j \right \|} \right \rangle\]</span></p><ul><li><span class="math inline">\(p_c^k\)</span>是<spanclass="math inline">\(B_{LR}^k\)</span>块的center patch</li><li><span class="math inline">\(q_j\)</span>是<spanclass="math inline">\(F_{Ref↓}\)</span>块的第j个patch</li><li><spanclass="math inline">\(r_{c,j}^k\)</span>表示相似性大小（similarityscores）</li></ul><p>通过<strong>相似性大小</strong>(similarity score)我们可以找到<spanclass="math inline">\(F_{Ref↓}\)</span> 中与<spanclass="math inline">\(p_c^k\)</span>最相似的patch</p><p>然后crop一个围绕着这个最相似patchb并且大小为<spanclass="math inline">\([dx,dy]\)</span>的block 记作<spanclass="math inline">\(B_{Ref↓}^k\)</span></p><blockquote><p>根据局部相干特性,每个<spanclass="math inline">\(B_{LR}^k\)</span>块的center patch都能找到与在<spanclass="math inline">\(F_{Ref↓}\)</span>其中最相似的patch后找到对应的<spanclass="math inline">\(B_{Ref↓}^k\)</span></p><p>另一方面，我们可以在<spanclass="math inline">\(F_{Ref}^s\)</span>中剪切相应的大小为<spanclass="math inline">\([s*dx,s*dy]\)</span>的block，记作<spanclass="math inline">\(B_{Ref}^{s,k}\)</span></p><p>这将用于<strong>特征提取阶段</strong></p></blockquote><p>注意问题⚠️</p><ul><li>如果<spanclass="math inline">\(B_{LR}^k\)</span>的大小远大于其centerpatch的大小，则center patch可能无法代表<spanclass="math inline">\(B_{LR}^k\)</span>的全部内容</li><li>这可能会误导我们找到不相关的<spanclass="math inline">\(B_{Ref↓}^k\)</span></li></ul><p>解决方法：</p><ul><li>使用具有<strong>不同膨胀率</strong>的中心块来计算相似度</li><li>细节如图3的第1阶段所示，<ul><li>其中蓝色虚线表示<em>dilation</em>=1的情况</li><li>橙色虚线表示<em>dilation</em>=2的情况</li><li>然后将相似性得分计算为<strong>不同扩张的结果之和</strong></li></ul></li></ul><blockquote><p>这个阶段我们得到了（<spanclass="math inline">\(B_{LR}^k\)</span>，<spanclass="math inline">\(B_{Ref↓}^k\)</span>，<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>）</p><p>在下面的精细匹配阶段阶段我们将限制<spanclass="math inline">\(B_{LR}^k\)</span> to <spanclass="math inline">\(B_{Ref↓}^k\)</span> 的搜索空间</p></blockquote><h3 id="stage-2-fine-matching精细匹配阶段阶段">Stage 2: Finematching（精细匹配阶段阶段）</h3><blockquote><p>对<span class="math inline">\(B_{LR}^k\)</span> to <spanclass="math inline">\(B_{Ref↓}^k\)</span>进行dense pathmatching（密集补丁匹配）</p><p>得到index maps集合 <span class="math inline">\(\left\{ D^0 , D^1 ,... D^{K-1} \right\}\)</span></p><p>similarity maps 集合<span class="math inline">\(\left\{ R^0 , R^1 ,... R^{K-1} \right\}\)</span></p></blockquote><p>拿第k对(<span class="math inline">\(B_{LR}^k\)</span> , <spanclass="math inline">\(B_{Ref↓}^k\)</span>)为例 我们计算每个patch in<span class="math inline">\(B_{LR}^k\)</span> 与 每个patch in <spanclass="math inline">\(B_{Ref↓}^k\)</span>之间的相似性分数 <spanclass="math display">\[r_{i,j}^k = \left \langle \frac{p_i^k}{\left \| p_i^k \right \|} ,\frac{q_j^k}{\left \| q_j^k \right \|} \right \rangle\]</span></p><ul><li><span class="math inline">\(p_i^k\)</span>是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch</li><li><span class="math inline">\(q_j^k\)</span>是<spanclass="math inline">\(B_{Ref↓}^k\)</span>的第j个patch</li><li>k表示第k对(<span class="math inline">\(B_{LR}^k\)</span> , <spanclass="math inline">\(B_{Ref↓}^k\)</span>)</li><li>$r_{i,j}^k $表示它们的相似性大小</li></ul><blockquote><p><span class="math inline">\(D^k\)</span>的第i个元素的值j表示的是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch与<spanclass="math inline">\(B_{Ref↓}^k\)</span>中第j个ptach最相似</p></blockquote><p><span class="math display">\[D_i^k = \mathop{\arg \max}_j \ r_{i,j}^k\]</span></p><blockquote><p><span class="math inline">\(R^k\)</span>的第i个元素表示的是<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch相对应的highestsimilarity score(最高相似分数)</p></blockquote><p><span class="math display">\[R_i^k = \mathop{\max}_j \ r_{i,j}^k\]</span></p><h3 id="stage-3-feature-extraction特征提取">Stage 3: Featureextraction(特征提取)</h3><p>根据index map <span class="math inline">\(D^k\)</span> 从<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>中提取patches，生成新的featuremap <span class="math inline">\(B_M^{s,k}\)</span></p><p>更精准的说</p><p><span class="math inline">\(D_i^k\)</span>表示<spanclass="math inline">\(B_{Ref↓}^k\)</span>中与<spanclass="math inline">\(B_{LR}^k\)</span>的第i个patch最相似的patch</p><p>我们将<span class="math inline">\(D_i^k\)</span>个<spanclass="math inline">\(B_{Ref}^{s,k}\)</span>中的patch作为<spanclass="math inline">\(B_M^{s,k}\)</span>的第i个patch</p><p>此外，由于相似性<strong>分数较高的Ref特征更有用</strong></p><p>我们将<span class="math inline">\(B_M^{s,k}\)</span>与相应的<spanclass="math inline">\(R^k\)</span>相乘获得<strong>加权特征块</strong><span class="math display">\[B_M^{s,k} := B_M^{s,k}\odot(R^k)\uparrow\]</span></p><ul><li><span class="math inline">\(()\uparrow\)</span>表示双线性插值</li><li><span class="math inline">\(\odot\)</span> 表示element-wise mul</li></ul><p>MEM的最终结果就是将 <span class="math inline">\(\left\{ B_M^{s,0} ,B_M^{s,1} , B_M^{s,2},...B_M^{s,K-1}\right\}\)</span>进行折叠在一起获得，折叠操作是步骤一的逆向操作</p><h3 id="分析">分析</h3><h4 id="以往的配对方法">以往的配对方法</h4><ul><li><p>图像LR的像素 为 m pixels ; 图像<spanclass="math inline">\(Ref\downarrow\)</span>的像素 为 n pixels</p></li><li><p>计算复杂度为O(mn)</p></li></ul><h4 id="mem方法">MEM方法</h4><ul><li>假设<span class="math inline">\(Ref\downarrow\)</span> block 有 n’pixels</li><li>计算复杂度将被降为 O(Kn+mn’)</li><li>K远小于m</li><li>n‘ 远小于n</li><li>通过这种从粗到精的匹配方案，计算量显著降低</li></ul><h2 id="spatial-adaptation-modulesam空间自适应模块">Spatial AdaptationModule（SAM）空间自适应模块</h2><p><imgsrc="https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b" /></p><p>在许多情况下，LR和Ref图像可能具有相似的内容和纹理，颜色和亮度不一样</p><p>因此，提取的REF特征的分布可能与LR特征的分布不一致。因此，简单地将Ref和LR特性连接到一起，并将它们输入到下面的卷积层中并不是最佳选择。我们建议使用空间自适应模块（SAM）将提取的Ref特征的分布重新映射到LR特征的分布</p><p>首先将LR特征和提取的Ref特征连接(Cat)起来，然后预先送入卷积层，以产生两个参数β和γ，这两个参数的大小与LR特征相同。</p><p>我们用特征的平均值和标准偏差更新β和γ <span class="math display">\[\beta \leftarrow \beta + \mu_{LR}\\\gamma \leftarrow \gamma +\sigma_{LR}\]</span></p><blockquote><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span>的产生方式跟<spanclass="math inline">\(\mu_{Ref}\)</span> <spanclass="math inline">\(\sigma_{Ref}\)</span> 一样</p><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span> 的大小是 C x 1 x 1</p><p>C表示一共有C个通道</p></blockquote><p>然后将实例规范化(Instance Norm)应用于Ref特征，如下所示： <spanclass="math display">\[F_{Ref}^c \leftarrow \frac{F_{Ref}^c - \mu_{Ref}^c}{\sigma_{Ref}^c}\]</span> <span class="math inline">\(\mu_{Ref}^c\)</span> <spanclass="math inline">\(\sigma_{Ref}^c\)</span>分别表示Ref特征图在通道c的均值和方差 <span class="math display">\[\mu_{Ref}^c = \frac{1}{HW} \sum_{y,x}F_{Ref}^{c,y,x}\]</span></p><p><span class="math display">\[\sigma_{Ref}^c  = \sqrt{\frac{1}{HW}\sum_{y,x}(F_{Ref}^{c,y,x} -\mu_{ref}^c)^2}\]</span></p><p>最后，将γ和β添加到归一化Ref特征中，如下所示： <spanclass="math display">\[F_{Ref} \leftarrow F_{Ref}·\gamma + \beta\]</span> 由于Ref特征和LR特征之间的差异随空间位置而变化</p><p><span class="math inline">\(\mu_{LR}\)</span> <spanclass="math inline">\(\sigma_{LR}\)</span> <spanclass="math inline">\(\mu_{Ref}\)</span> <spanclass="math inline">\(\sigma_{Ref}\)</span> 的大小为 C x 1 x 1</p><p>我们使用可学习卷积来预测两个空间参数β和γ</p><p>不同于只使用分割图(segmentation)去生成两个参数，SAM中的卷积将Ref和LR特征作为输入，以了解它们的差异。此外，在从卷积中获得β和γ后，我们将它们与LR特征的均值和标准偏差相加</p><h2 id="dual-residual-aggregation-module-dram-双残差聚合模块">DualResidual Aggregation Module (DRAM) 双残差聚合模块</h2><p>在空间自适应后，使用我们提出的双剩余聚合模块（DRAM），将传输的Ref特征与LR特征融合</p><p>DRAM由两个分支组成，即 <strong>LR分支</strong> 和<strong>Ref分支</strong></p><p><strong>Ref分支</strong></p><blockquote><p>旨在细化Ref功能的高频细节</p></blockquote><ul><li>首先使用stride=2的卷积对Ref Feature 进行下采样</li><li>将下采样后的Ref Feature 减去 LR Feature 得到Res_Ref 残余特征</li><li>将Res_Ref使用转置卷积（逆卷积）上采样后加上原始的RefFeatu得到新的Ref ‘ 特征图</li></ul><p><span class="math display">\[\begin{cases}    Res_{Ref} = Conv(F_{Ref}) - F_{LR}\\    F&#39;_{Ref}= F_{Ref} + DeConv(Res_{Ref})\end{cases}\]</span></p><p><strong>LR分支</strong></p><blockquote><p>LR功能的高频细节细化</p></blockquote><ul><li>首先使用stride=2的卷积对Ref Feature 进行下采样</li><li>将 <strong>LR Feature</strong> 减去<strong>下采样后的RefFeature</strong> 得到Res_LR 残余特征</li><li>将Res_LR 加上 LR Feature 相加后 进行上采样的懂新的LR’ 特征图</li></ul><p><span class="math display">\[\begin{cases}    Res_{LR} =  F_{LR}-Conv(F_{Ref})\\    F&#39;_{LR}= DeConv(F_{LR}+Res_{LR})\end{cases}\]</span></p><p>最后，将两个分支的输出串联起来，并以步长1通过另一个卷积层。</p><p>通过这种方式，LR和Ref功能中的细节得到了增强和聚合，从而产生了更具代表性的功能。</p><h2 id="loss-functions">Loss Functions</h2><h3 id="reconstruction-loss重建损失">1.Reconstructionloss(重建损失)</h3><p>采用L1损失作为重建损失</p><p><span class="math display">\[\mathcal{L}_{rec} = \left \|  I_{HR} - I_{SR}\right\|_1\]</span></p><h3 id="perceptual-loss知觉损失">2.Perceptual loss(知觉损失)</h3><p>知觉损失的表达为 <span class="math display">\[\mathcal{L}_{per} = \left \|  \phi_i(I_{HR}) - \phi_i(I_{SR}) \right\|_2\]</span></p><blockquote><p><span class="math inline">\(\phi_i\)</span>表示VGG19的第i层这里是用conv5_4</p></blockquote><h3 id="adversarial-loss对抗性损失">3.Adversarial loss(对抗性损失)</h3><p>它可以有效地生成具有自然细节的视觉愉悦图像 <spanclass="math display">\[\mathcal{L}_D =-\mathbb{E}_{I_{HR}}[\log(D(I_{HR},I_{SR}))]-\mathbb{E}_{I_{SR}}[\log(1-D(I_{HR},I_{SR}))],\\\mathcal{L}_G=-\mathbb{E}_{I_{HR}}[\log(1-D(I_{HR},I_{SR}))]-\mathbb{E}_{I_{SR}}[\log(D(I_{SR},I_{HR}))]\]</span></p><h3 id="full-objective">4.Full objective</h3><p><span class="math display">\[\mathcal{L} =\lambda_{rec}\mathcal{L}_{rec}+\lambda_{per}\mathcal{L}_{per}+\lambda_{adv}\mathcal{L_{adv}}\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://pic1.zhimg.com/v2-3486f59df8faa40673ddcbe4f5212844_1440w.jpg?source=172ae18b&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;主要组成&quot;&gt;主要组成&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="超分重建" scheme="http://www.larryai.com/tags/%E8%B6%85%E5%88%86%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>informative-drawing</title>
    <link href="http://www.larryai.com/2022/05/04/informative-drawing/"/>
    <id>http://www.larryai.com/2022/05/04/informative-drawing/</id>
    <published>2022-05-04T07:56:42.000Z</published>
    <updated>2022-05-04T07:59:24.408Z</updated>
    
    <content type="html"><![CDATA[<h1id="learning-to-generate-line-drawings-that-convey-geometry-and-semantics">Learningto generate line drawings that convey geometry and semantics</h1><h2 id="摘要">摘要</h2><p>本文提出了一种从照片中生成线条图的非配对方法。目前的方法通常依赖高质量的成对数据集来生成线条图。然而，由于图纸主题属于特定do-main，或收集的数据量有限，这些数据集通常具有局限性。尽管近年来在无监督图像到图像的翻译方面取得了很大进展，但最新的方法仍然难以生成令人信服的线条图。我们观察到，线条图是场景信息的编码，并试图传达三维形状和语义。我们将这些观察构建成一组目标，并训练图像翻译，将照片映射成线条图。我们引入了一种几何损失，<strong>它从线条图的图像特征中预测深度信息</strong>，<strong>以及一种语义损失</strong>，<strong>它将线条图的剪辑特征与其相应的照片相匹配</strong>。我们的方法优于最先进的未成对图像翻译和线条绘制生成方法，可以从ar位图照片中创建线条图。</p><h2 id="方法">方法</h2><p>我们的目标是训练一个模型，在给定照片数据集和未配对的线图数据集的情况下，自动生成任意照片的线图。我们将这个问题描述为包含照片的域A和代表特定风格线条图的do-mainB之间的未配对图像转换。大多数以前的方法仅仅考虑在循环图中保持PHO图样的外观。相反，我们的方法通过评估几何和语义的目标进一步指导翻译通过线条图传达的信息。此设置如图2所示。我们在第4节中说明，这些新损失对于创建有意义的图形至关重要。</p><figure><img src="1.png" alt="截屏2022-04-11 22.10.53" /><figcaption aria-hidden="true">截屏2022-04-11 22.10.53</figcaption></figure><blockquote><p>给出一张照片a，我们的模型训练网络GA，通过四个主要损失来合成线图GA（a）。带有鉴别器DB的对抗式损失鼓励生成的线条绘制与训练集的风格相匹配。</p><p>线条、外观和几何图形的LOSS强制要求线条分别传达有效的语义、外观和几何图形</p></blockquote><p>我们对域A和域B分别使用带有生成器网络GA、GB和鉴别器DA、DB的对抗性训练设置。</p><ul><li>几何目标通过预先训练的深度网络来实现，该网络根据<strong>线图预测深度图</strong>，并对深度输出施加监督损失。这种损失促使我们的模型在几何上重要的位置（例如遮挡轮廓）绘制线。</li><li>其次，我们引入了一个<strong>CLIPloss</strong>，将语义添加到生成的线条图中。由于任意的照片往往显示复杂的场景，我们使用<strong>视觉CLIP嵌入</strong>，它可以很好地捕捉语义细节。然后，我们规定线条图的CLIP嵌入与原始照片的CLIP嵌入相似。</li><li>我们还使用弱加权<strong>循环一致性损失</strong>来保留外观信息。</li></ul><h1 id="loss">LOSS</h1><h2 id="对抗性损失">对抗性损失</h2><p>鼓励生成的图像长到各自的域[27]。使用LSGAN设置[61]的每个域的损耗公式如下</p><figure><img src="2.png" alt="截屏2022-04-11 22.14.36" /><figcaption aria-hidden="true">截屏2022-04-11 22.14.36</figcaption></figure><h2 id="几何目标损失the-geometry-objective">几何目标损失The geometryobjective</h2><p>​在训练期间，最大限度地利用自动绘制的线条图提供深度信息。我们观察到，线条图通常是3D形状的有效载体,并在培训期间应用此属性。给定一个线图的实体数据集，模型可以在没有任何明确监督的情况下学习这一序列。然而，目前没有这种几何约束的方法无法在有意义的地方放置线（见第4节）。照片数据集和线条图之间的领域差距也是显而易见的。相反，我们<strong>提出了一个几何约束，用于监督线条图的深度预测</strong></p><p>​为了监督线条图的深度预测，有必要获得用于摄影输入的深度图。不幸的是，大多数数据集通常无法获得地面真相深度信息。然而，<strong>最近的方法在为照片绘制高分辨率深度图方面非常成功</strong>。这一进展使我们能够使用从最先进的深度预测网络F获得的伪地面真深度图；实际上，我们使用[62]中的网络，它基于MiDaS[69]。我们注意到，<strong>照片的伪地面真相图仅在培训时需要，而不是在测试时需要</strong></p><p>​<strong>监督几何预测</strong>的一个简单方法是，在培训期间引入网络<spanclass="math inline">\(G_{geom}\)</span>来根据线图预测深度图。然而，这种方法有几个问题。从合成线图中学习深度的培训可能会鼓励线图生成器以一种不需要的形式灌输深度信息</p><blockquote><p>比如一个不易察觉的信号[15]。我们希望避免意外地将不可见的信息嵌入到我们的线条图中。</p><p>由于领域差距，在线条图上使用预训练深度网络不是一个选项</p></blockquote><p>​相反，我们建议学习从通常在照片和线条图之间<strong>共享的图像特征推断深度</strong>。具体来说，我们在给定ImageNet[20]特征的情况下，预先训练一个网络<spanclass="math inline">\(G_{geom}\)</span>来预测深度。这些特征，尤其是在早期阶段，对迁移学习非常有用[49]。这个场景希望通过首先将<strong>线条图编码</strong>(intoa shared representation withphotographs)为带有照片的共享演示，然后应用一个从照片特征中学习深度的网络，来避免不可见的信号问题。</p><ul><li>将线条图编码与和图片加入共享representation</li><li>应用一个从照片特征中学习深度的网络？</li></ul><p>为了获得图像特征，我们将照片输入预先训练好的<strong>Inceptionv3[76]网络</strong>，并从<strong>混合6b节点</strong>提取特征（见补充）。</p><p>我们将input a在这层提取出来的特征定义为<spanclass="math inline">\(I(a)\)</span></p><p>经过预训练后，<spanclass="math inline">\(G_{geom}\)</span>提供线条画的深度估计图</p><p>在练习中 我们在培训线条绘制生成的同时进行微调 <spanclass="math inline">\(G_{geom}\)</span></p><p>几何损失公式如下。</p><ul><li>给定照片a，我们首先将其输入到最先进的深度网络F[62]中，并获得pseudo-ground truth depth map <spanclass="math inline">\(F(a)\)</span>。</li><li>然后生成线条画<span class="math inline">\(G_A(a)\)</span>并提取它的ImageNet特征<spanclass="math inline">\(I(G_A(a)\)</span>。</li><li>然后将这些特征传递给预先训练的深度网络<spanclass="math inline">\(G_{Geom}\)</span> , 生成深度图预测<spanclass="math inline">\(G_{Geom}(I(G_A(a))\)</span></li><li>然后将该深度预测与伪地面真实深度图F(a)进行比较</li></ul><p>进一步,细节和深度重建见补充说明</p><figure><img src="Lgeom.png" alt="截屏2022-04-11 22.43.35" /><figcaption aria-hidden="true">截屏2022-04-11 22.43.35</figcaption></figure><h2 id="语义损失the-semantics-loss">语义损失The semantics loss</h2><p>​ 通过最小化 <strong>输入照片图的CLIP嵌入</strong>和<strong>生成的线条图</strong>之间的距离来实现。这一目标的目标是将原始照片中的语义信息传达到相应的合成线条图中。</p><p>​在计算机视觉中，<strong>语义通常以标签和分割图的形式学习</strong>。然而，这些<strong>表达仅限于特定领域或对象</strong>。为了对整个场景中的语义信息进行编码，我们使用了(shared visual-text embeddingCLIP)<strong>共享的视觉文本嵌入剪辑</strong>[68]，它在照片和艺术[17,24]中捕获了丰富的语义信息。然后，我们对生成的线条图和原始照片图之间的距离进行惩罚。目标如下。</p><figure><img src="LCLIP.png" alt="截屏2022-04-11 22.44.24" /><figcaption aria-hidden="true">截屏2022-04-11 22.44.24</figcaption></figure><h2id="外观损失the-appearance-loss-循环一致性损失cycle-consistency">外观损失Theappearance loss (循环一致性损失cycle consistency)</h2><p>已被用于通过图像转换对输入外观进行编码[47,89]。贴图每个方向的外观损失如下所示</p><figure><img src="Lcycle.png" alt="截屏2022-04-11 22.47.14" /><figcaption aria-hidden="true">截屏2022-04-11 22.47.14</figcaption></figure><p><br /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1
id=&quot;learning-to-generate-line-drawings-that-convey-geometry-and-semantics&quot;&gt;Learning
to generate line drawings that convey geometry and s</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GanGANv2</title>
    <link href="http://www.larryai.com/2022/05/04/GanGANv2/"/>
    <id>http://www.larryai.com/2022/05/04/GanGANv2/</id>
    <published>2022-05-04T07:31:06.000Z</published>
    <updated>2022-05-06T12:22:28.993Z</updated>
    
    <content type="html"><![CDATA[<h1 id="product-of-experts-gans">Product-of-experts GANs</h1><p><strong>输入：</strong></p><ul><li>图像数据集x</li><li>M个输入方式（多模态）</li></ul><p><strong>目标：</strong></p><p>训练一个单一的操作模型，该模型可以学习在可能模式的任意子集<spanclass="math inline">\(\mathcal{Y}\)</span>上捕获图像分布</p><p><strong>在本文中，我们考虑了四种不同的模式:</strong></p><ul><li><p>文本text</p></li><li><p>语义分割segmentation</p></li><li><p>草图sketch</p></li><li><p>风格参照style reference</p></li></ul><p>学习基于任何子数据的图像分布<spanclass="math inline">\(p(x\mid\mathcal{Y})\)</span>是一项挑战</p><p>因为它需要一个生成器同时去建模<spanclass="math inline">\(2^M\)</span>个分布</p><p>特别值得注意的是</p><p>当<spanclass="math inline">\(\mathcal{Y}\)</span>是空集合时，生成器需要捕获无条件的图像分布</p><p>其他几种模态下的单独的条件分布</p><p><span class="math inline">\(p(x|y_i) , \forall i \in\left\{1,2...M\right\}\)</span></p><p>例如，图像分布仅以文本为条件。这些设置在隔离单独中很受欢迎并得到了广泛的研究，我们的目标是将它们置于一个统一的框架下。</p><h2 id="product-of-experts-modeling">Product-of-experts modeling</h2><p>直观地说，每个输入模态都增加了合成图像必须满足的约束。</p><p>满足所有约束的图像集是满足单个约束的图像集的交点</p><p><strong>分布的乘积类似于集合的交集</strong></p><figure><img src="1.png"alt="The product of distributions is analogous to the intersection of sets" /><figcaption aria-hidden="true">The product of distributions is analogousto the intersection of sets</figcaption></figure><p>如上图，我们通过一个强有力的假设对此进行建模</p><p>即<strong>联合条件概率分布</strong><spanclass="math inline">\(p(x\mid y_i,y_j)\)</span>与单条件概率分布<spanclass="math inline">\(p(x \mid y_i)\)</span>和<spanclass="math inline">\(p(x \mid y_j)\)</span>的乘积成正比</p><p>在此设置下，为了使product分布在某个区域具有高密度，每个单独的分布需要在该区域具有高密度，从而满足每个约束</p><p>通过相乘的方式组合多个分布（“experts”）以前被称为<strong>Product-of-experts</strong></p><p>我们的生成器被训练用来将一个latent code z 映射到一个image x</p><p>因为输出的图像被latent code z 唯一的确定</p><p>所以评估<spanclass="math inline">\(p(x\mid\mathcal{Y})\)</span>的问题可以等价为评估<spanclass="math inline">\(p(z\mid\mathcal{Y})\)</span>的问题</p><p>我们使用Product-of-experts来对潜在(latent)条件分布进行建模 <spanclass="math display">\[p(z\mid\mathcal{Y}) \propto p&#39;(z)\prod_{y_i\in \mathcal{Y}}q(z\midy_i)\]</span></p><ul><li><span class="math inline">\(p&#39;(z)\)</span>是先验概率分布</li><li>每个expert <span class="math inline">\(q(z\midy_i)\)</span>是编码器预测的单一模态的分布</li></ul><blockquote><p>当没有给出模式时，潜在分布只是先验分布。</p><p>随着提供更多的模式，约束的数量增加，可能输出图像的空间缩小，潜在分布变窄。</p></blockquote><h2id="multiscale-and-hierarchical-latent-space多尺度层次潜在空间">Multiscaleand hierarchical latent space（多尺度层次潜在空间）</h2><p>有一些我们考虑的模态是二维的，自然包含多个尺度的信息。</p><p>因此，我们设计了一个带有不同分辨率下的潜变量的hierarchical latentspace（潜在层次空间）</p><p>这允许我们直接将信息从空间编码器的每个分辨率传递到潜在空间的相应分辨率，以便更好地保存高分辨率的控制信号</p><p>从数学上讲，我们的潜在代码z被分成若干组 <span class="math inline">\(z= (z^0 , z^1 ,z^2 ... z^N)\)</span></p><ul><li><p><span class="math inline">\(z^0 \in \mathbb{R}^{c_0}\)</span>是一个特征向量</p></li><li><p><span class="math inline">\(z^k \in \mathbb{R}^{c_k\times r_k\times r_k}\)</span> 是分辨率不断提高的特征图</p></li><li><p>$r_{k+1}= 2r_k , r_1 =4 $ <spanclass="math inline">\(r_N\)</span>是图像分辨率</p></li></ul><p>因此我们可以分解</p><ul><li>先验概率分布<spanclass="math inline">\(p&#39;(z)\)</span>分解到<spanclass="math inline">\(\prod_{k=0}^N p&#39;(z^k |z^{&lt;k})\)</span></li><li>expert <span class="math inline">\(q(z\mid y_i)\)</span> 分解到<spanclass="math inline">\(\prod_{k=0}^N q(z^k | z^{&lt;k} ,y_i)\)</span></li></ul><p>这种设计类似于等级VAE中的前后网络。不同之处在于，我们的编码器在输入模式中编码条件信息，而不是图像本身。根据潜在条件分布公式，我们假设每个分辨率的潜在条件分布是expert的产物<span class="math display">\[p(z^k\mid z^{&lt;k},\mathcal{Y}) \propto p&#39;(z^k |z^{&lt;k})\prod_{y_i\in \mathcal{Y}}q(z^k | z^{&lt;k},y_i)\]</span></p><blockquote><p><span class="math inline">\(p&#39;(z^k | z^{&lt;k}) =\mathcal{N}(\mu_0^k,\sigma_0^k)\)</span> 和 $q(z^k | z^{&lt;k},y_i)=(_i^k,_i^k) $用神经网络参数化均值和标准差的独立高斯分布</p></blockquote><blockquote><p>可以证明 Product of gaussion experts 同样也是高斯分布 <spanclass="math inline">\(p(z^k\mid z^{&lt;k},\mathcal{Y}) =\mathcal{N}(\mu^k,\sigma^k)\)</span></p></blockquote><p><img src="2.png" /></p><h2 id="generator-architecture">Generator architecture</h2><h3 id="encoder">1.Encoder</h3><p><img src="3.png" /></p><p>上图为生成器体系结构的概述。我们将每个模态编码成一个特征向量，然后在<strong>GlobalPoE-Net</strong>中进行聚合。</p><ul><li><p>使用卷积网络(with input skip connections)对分割图seg和草图sketch进行编码</p></li><li><p>使用residual network对风格图style进行编码</p></li><li><p>使用CLIP去对文本text进行编码</p></li></ul><blockquote><p>附录B中给出了所有模态编码器的详细信息。</p><p>解码器使用GlobalPoE-Net的输出以及<strong>分割编码器</strong>和<strong>草图编码器</strong>的跳过连接生成图像</p></blockquote><h3 id="global-poe-net">2.Global PoE-Net</h3><blockquote><p><span class="math inline">\(\mu和\sigma\)</span>下标有0，1，2，3，4</p><p>0表示先验分布 1表示分割编码器 2表示文本编码器 3表示style编码器4表示草图编码器</p></blockquote><p><img src="4.png" /></p><p>在GlobalPoE-Net中，我们使用MLP从每个模态中提取特征向量来预测一个高斯分布<spanclass="math inline">\(q(z^0|y_i)=\mathcal{N}(\mu_i^0,\sigma_i^0)\)</span></p><p>然后我们计算product of Gaussion包括$p'(z^0) = (_0^0,_0^0)=(0,I) $</p><p>并从product分布中采样得到<span class="math inline">\(z^0\)</span></p><p>然后用MLP将<spanclass="math inline">\(z^0\)</span>转化为另一个特征向量<spanclass="math inline">\(w\)</span></p><h3 id="decoder">3.Decoder</h3><p><img src="5.png" /></p><p>解码器主要有一堆残差块组成</p><p><strong>Local-PoE-Net</strong>在当前分辨率下对潜在feature map <spanclass="math inline">\(z^k\)</span>进行采样</p><p>当前分辨率下的product</p><p><span class="math inline">\(p&#39;(z^k | z^{&lt;k}) =\mathcal{N}(\mu_0^k,\sigma_0^k)\)</span> 特征图zk下的先验分布</p><p><span class="math inline">\(q(z^k | z^{&lt;k},y_i) =\mathcal{N}(\mu_i^k,\sigma_i^k)\)</span> 特征图zk下不同模态的分布</p><blockquote><p><spanclass="math inline">\(\mu_0^k,\sigma_0^k\)</span>根据最后一层的输出计算</p><p><spanclass="math inline">\(\mu_i^k,\sigma_i^k\)</span>通过连接最后一层的输出和相应模态的跳过连接来计算</p></blockquote><blockquote><p>请注意，只有具有跳过连接的模式(分割图和草图)对计算作出贡献</p><p>其他模式（文本和style参考）仅提供global信息，而不提供local细节。</p></blockquote><p>local-PoE-net生成的潜在特征映射<spanclass="math inline">\(z^k\)</span>和global-PoE-net生成的特征向量<spanclass="math inline">\(w\)</span>被送到LG AdaIN层</p><p>本地全局自适应实例规范化(LG AdaIN)</p><p><img src="6.png" /></p><p><img src="7.png" /></p><ul><li><p><spanclass="math inline">\(h^k\)</span>是在残差分支中通过一个卷积层得到的一个featuremap</p></li><li><p>μ(hk) and σ(hk) 是通道平均值和标准差</p></li><li><p><span class="math inline">\(\beta_{z^k} , \gamma_{z^k}\)</span>由<span class="math inline">\(z^k\)</span>通过一个1x1卷积得到</p></li><li><p><span class="math inline">\(\beta_{w} ,\gamma_{w}\)</span>由w计算得到</p></li></ul><blockquote><p>LG-AdaIN可以被看作AdaIN和SPADE的组合采用<strong>全局特征向量</strong>和<strong>空间变化特征映射</strong>来调节激活。</p></blockquote><h2id="multiscale-multimodal-projection-discriminator多尺度多模投影鉴别器">Multiscalemultimodal projection discriminator（多尺度多模投影鉴别器）</h2><blockquote><p>输入：</p><ul><li>image x</li><li>一系列的条件 <span class="math inline">\(\mathcal{Y}\)</span></li></ul><p>输出：</p><p>一个分数<span class="math inline">\(D(x,\mathcal{Y}) =sigmoid(f(x,\mathcal{Y}))\)</span>表明真实性</p><p>f的最优解是</p><p><img src="8.png" /></p></blockquote><p>我们假设给出x的不同模态下的条件独立,投影鉴别器（PD）（估计是一篇论文里的方法）建议使用<strong>内积</strong>来估计条件变量,他的实施将有条件的期限限制为,要相对简单，这会产生一个很好的归纳偏差，从而产生很强的实证结果。<img src="9.png" /></p><p>原始的PD</p><ul><li>首先将图像和条件一起输入到一个<strong>shared latentspace</strong>（潜在共享空间）</li><li>然后使用一个<strong>线性层</strong>来估计图像嵌入的<strong>无条件项（unconditionalterm）</strong></li><li>并使用图像嵌入和条件嵌入之间的<strong>内积</strong>来估计<strong>条件项(conditionalterm)</strong>。</li><li>将<strong>无条件项</strong>和<strong>条件项</strong>相加，以获得最终的鉴别器的结果</li></ul><blockquote><p>相当于PD的推广</p><p>我们提出了一种多模态投影判别器，它将投影判别器推广到处理多个条件输入。与计算图像嵌入和条件嵌入之间的单个内积的标准投影判别器不同，我们为每个输入模态计算一个内积并将它们加在一起以获得最终损失。</p></blockquote><p><img src="10.png" /></p><p><span class="math display">\[f(x,\mathcal{Y}) = Linear(D_x(x)) = \sum_{y_i \in \mathcal{Y}}D_{y_i}^T(y_i)D_x(x)\]</span></p><p>对于分割和草图等空间模式，在多个尺度下加强它们与图像的对齐更有效。</p><p>如图所示，我们将图像和空间模式编码为不同解决方案的特征图，并计算每个分辨率下的MPD损失。</p><p>我们计算每个位置和分辨率的损失值，然后通过先对位置进行平均，然后再对分辨率进行平均，得到最终损失。</p><p>我们将产生的鉴别器命名为多尺度多模投影鉴别器（MMPD），并在附录B中描述其细节</p><figure><img src="11.png" alt="截屏2022-03-12 19.59.36" /><figcaption aria-hidden="true">截屏2022-03-12 19.59.36</figcaption></figure><h2 id="losses-and-training-procedure">Losses and trainingprocedure</h2><h3 id="section"></h3><h3 id="latent-regularization">Latent regularization</h3><p>在product-of-experts的假设下他认为<strong>有条件的潜在分布</strong>应该与<strong>无条件的先验分布</strong>相匹配</p><figure><img src="12.png" alt="截屏2022-03-15 10.34.41" /><figcaption aria-hidden="true">截屏2022-03-15 10.34.41</figcaption></figure><p>我们<strong>最小化</strong>在每个分辨率下<strong>有条件的潜在分布</strong><spanclass="math inline">\(p(z|y_i)\)</span>和<strong>无条件的先验分布</strong><spanclass="math inline">\(p&#39;(z)\)</span>的KL散度</p><figure><img src="13.png" alt="截屏2022-03-15 10.37.50" /><figcaption aria-hidden="true">截屏2022-03-15 10.37.50</figcaption></figure><ul><li><spanclass="math inline">\(w_k\)</span>是分辨率相关的再平衡权重</li><li><span class="math inline">\(w_i\)</span>是一个特定误差权重</li></ul><blockquote><p>KL损失也减少了条件模式崩溃，因为它鼓励条件潜在分布接近先验分布，因此具有高熵。</p><p>从信息瓶颈的角度来看，KL损失鼓励每个模态只提供指定条件图像分布所需的最小信息。</p></blockquote><h3 id="contrastive-losses">Contrastive losses</h3><p>对比损失在<strong>表示学习</strong>中得到了广泛的应用，最近应用到图像合成</p><p>给定一批配对向量<span class="math inline">\((u,v) = \left\{(u_i,v_i), i =1,2,3...N\right\}\)</span></p><p>对称性交叉熵损失 最大化成对向量的相似性，同时分开 非成对向量</p><figure><img src="14.png" alt="截屏2022-03-15 10.46.34" /><figcaption aria-hidden="true">截屏2022-03-15 10.46.34</figcaption></figure><p>我们使用两种配对来构造两个对比损失项：<strong>图像对比损失</strong>和<strong>条件对比损失</strong></p><h4 id="图像对比损失">图像对比损失</h4><p>可最大化<strong>真实图像</strong>与<strong>基于条件生成的图像</strong>的相似性<span class="math display">\[\mathcal{L}_{cx} = \mathcal{L}^{ce}(E_{vgg}(x) , E_{vgg}(\bar{x}))\]</span></p><h4 id="条件对比损失">条件对比损失</h4><blockquote><p>条件对比损失可以更好地使图像与相应的条件一致。</p></blockquote><h5id="对识别器进行训练以最大化真实图像x的嵌入和条件输入之间的相似性">对识别器进行训练，以最大化真实图像X的嵌入和条件输入之间的相似性</h5><ul><li>辨别器的条件对比损失</li></ul><p><span class="math display">\[\mathcal{L}_{cy}^D = \mathcal{L}^{ce}(D_x(x) , D_{y_i}(\mathcal{Y}_i))\]</span></p><blockquote><p>其中，Dx和Dy分别是鉴别器中的两个模块，分别从x和y_i中提取特征</p></blockquote><h5id="生成器使用相同的损失进行训练但使用生成的图像barx代替真实图像x来计算鉴别器嵌入">生成器使用相同的损失进行训练，但使用生成的图像<spanclass="math inline">\(\bar{x}\)</span>代替真实图像x来计算鉴别器嵌入</h5><ul><li>生成器的条件对比损失</li></ul><p><span class="math display">\[\mathcal{L}_{cy}^G = \mathcal{L}^{ce}(D_x(\bar{x}) ,D_{y_i}(\mathcal{Y}_i))\]</span></p><blockquote><p>在实践中，我们只对<strong>文本模式</strong>使用条件对比损失，因为它消耗了太多的GPU内存，而对其他模式使用条件对比损失，尤其是在图像分辨率和批量较大的情况下。</p></blockquote><h3 id="full-training-objective">Full training objective</h3><p>总的LOSS包含生成器LOSS和辨别器LOSS</p><figure><img src="15.png" alt="image-20220315125032385" /><figcaption aria-hidden="true">image-20220315125032385</figcaption></figure><ul><li><span class="math inline">\(\mathcal{L}^G 和\mathcal{L}^D\)</span>是非饱和GAN损失</li><li><spanclass="math inline">\(\mathcal{L}_{GP}\)</span>是R1梯度惩罚损失</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;product-of-experts-gans&quot;&gt;Product-of-experts GANs&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图像数据集x&lt;/li&gt;
&lt;li&gt;M个输入方式（多模态）&lt;/li&gt;
&lt;/ul&gt;
&lt;p</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ChipGAN</title>
    <link href="http://www.larryai.com/2022/05/04/ChipGAN/"/>
    <id>http://www.larryai.com/2022/05/04/ChipGAN/</id>
    <published>2022-05-04T07:21:07.000Z</published>
    <updated>2022-05-06T08:03:21.081Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概要">概要</h1><p>​风格转换已成功应用于照片，生成逼真的西方绘画。然而，由于中西绘画技法的内在差异，直接套用已有的方法，对中国水墨画风格的转换并不能产生令人满意的效果。本文提出了一种基于ChipGAN的端到端（end-to-end）生成对抗性网络体系结构，用于照片转化为中国水墨画。</p><blockquote><p>ChipGAN的核心模块实施了三个约束</p><ul><li><p>空白（voids）</p></li><li><p>笔触（ brush strokes）（画笔描边）</p></li><li><p>水墨色调和扩散（ink wash tone and diffusion）</p></li></ul><p>以解决中国水墨画中普遍采用的三个关键技术。</p></blockquote><p>​我们通过咨询专业艺术家，基于新建的中国水墨照片和图像数据集，进行风格化感知研究，以评估生成的绘画与真实绘画的相似性。与最先进的网络和高度程式化的感知研究核心相比，该方法在视觉质量方面的优势表明了该方法的有效性。</p><h1 id="介绍">介绍</h1><h1 id="section"><img src="fig1.png" /></h1><p>​任何成功的艺术家都有自己独特的绘画风格。研究绘画风格中的这种独特性对绘画技能的训练很重要。除了传统的艺术理论培训，计算机视觉和图形技术如风格迁移和非真实感渲染，旨在帮助绘画艺术家系统地理解如何通过观察真实场景或照片，运用适当的绘画技巧呈现独特风格。</p><p>​将绘画风格迁移到图像可以通过纹理合成来实现，使用低级别的图像特征，忽略图像的语义信息。利用卷积神经网络（CNN）从图像中提取高级语义信息进行风格转换，这显示了视觉逼真的结果（如上图，根据真实西方绘画的风格从照片到生成的西方绘画）。</p><p>​然而，直接将现有的风格转换技术应用到中国水墨画中会产生不切实际的结果（图中，生成的水墨画，请注意混乱的线条和颜色）。这是因为中国水墨画和西方水墨画有几个本质的区别，在图中的最后一列中，西方真实绘画与水墨画真实绘画之间的比较表明：</p><ul><li>1）就一幅画的构图而言，西方绘画在整个图像上充满了色彩，而中国水墨画则有一定的空白区域；</li><li>2）就表达技巧而言，西方绘画很少使用强线，而它指的是白皮书中的一些区域，中国水墨画家特意留下这些区域来启发观众去想象。会议：2018年10月22日至26日，韩国首尔,中国水墨画采用线条鲜明的笔触来强调轮廓中的物体；</li><li>3）在色彩丰富性方面，西方绘画倾向于使用多种颜色，而中国水墨画主要使用不同灰度的墨水，这些墨水会扩散到一张宣纸上（水墨色调和扩散）</li></ul><p>​为了实现中国水墨画的风格转换，我们提出了一种基于生成对抗网络（GAN）的中国水墨画风格转换解决方案，名为CHIPGAN。根据中国水墨画的三种技法，我们提出了三种特殊的约束：空白、笔触、水墨色调和扩散。</p><blockquote><p>对于空洞，我们的约束结合了<strong>对抗损失</strong>和<strong>周期一致性损失</strong>，因为它们的目的是通过将信息转换为不可感知的信号来生成更真实的结果，从而留下白色区域。</p></blockquote><blockquote><p>对于画笔笔划，我们嵌入了一个预先训练好的<strong>整体嵌套边缘检测器</strong>（holistically-nestededgedetector），并在照片和假画的边缘映射之间加强重新设计的交叉熵损失，以强调有力的线条。</p></blockquote><blockquote><p>对于水墨扩散和色调，我们使用<strong>腐蚀和模糊</strong>(eroded andblurred)的图像来模拟这种绘画特性，并提出了<strong>水墨鉴别器</strong>来区分处理过的真假画</p></blockquote><p>​现有的绘画数据集主要包含西方画家的作品（如梵高、莫奈等），没有包含相应中国水墨画的真实照片和图像的可用数据集。为了解决我们的问题，我们提供了一个中国水墨画数据集，其中包括从互联网和艺术工作室收集的真实场景照片和绘画图像，我们的数据集名为“ChipPhi”。</p><blockquote><p>数据集包括</p><ul><li>1630张马的照片（不同颜色和不同姿势）</li><li>912张徐悲鸿的绘画图像</li><li>1976张风景照片（世界著名风景）</li><li>1542张黄宾虹的绘画图像的景观数据集</li></ul></blockquote><p>总之，本文的贡献有三个方面</p><ul><li>我们提出了ChipGAN，这是第一个执行从照片到中国水墨画风格转换的3个弱监督的深度网络架构，特别考虑了中国水墨画的三个基本技术：空白、笔触、水墨色调和扩散</li><li>我们引入了专业艺术家参与的风格化知觉研究，以评估原画和真画之间的风格一致性，并借助深度神经网络分析中国水墨画家的技巧</li><li>我们建立了第一个包含中国水墨画真实场景和图像的数据集ChipPhi，以便于训练和测试所提出的方法，并有助于中国水墨画风格转移的后续研究</li></ul><h1 id="相关工作">相关工作</h1><p>​<strong>图像风格迁移</strong>意味着将某个示例图像的样式迁移到目标图像。以前的图像级风格转换可以分为纹理合成和基于approache的卷积神经网络</p><p>​<strong>域级风格传递</strong>是指将给定的图像（如照片）与某个域的风格（如某个画家的风格）进行转换。它是通过基于生成性对抗网络（GAN）的方法实现的。</p><p>​ 此外，我们还回顾了一些专门为中国水墨画设计的计算方法</p><ul><li><p>纹理合成</p><blockquote><p>有一些非参数算法可以通过对给定的纹理图像重新采样来合成纹理。</p><p>Efros和Freeman提出了一种对应映射，根据目标图像的图像强度约束纹理合成过程。</p><p>Ashikhmin专注于传输高频纹理，但保留目标图像的比例。</p><p>Hertzmanet al.应用图像类比将源图像的样式转换为目标图像。</p><p>然而，由于纹理合成主要依赖于面片和低层次的表现，它们无法传递艺术作品的语义风格</p></blockquote></li><li><p>CNN based approaches</p><blockquote><p>基于CNN的模型旨在通过预先训练的卷积神经网络提取语义表示。</p><p>Gatysetal.[11]首先使用CNN获取图像的表示，并在自然照片上重现著名的喘息风格。</p><p>Liet al.[30]发现线性核是极大平方的一个很好的替代品。</p><p>Yin[48]和Chen及Hsu[3]研究了内容感知神经风格转移，并改善了结果。</p><p>这些方法大多存在速度慢、计算量大的问题，可通过[21,39]中的方法进行加速。Li和Wand[29]训练了一个马尔可夫前馈网络来解决效率问题。</p><p>Dumoulinet等人[7]建议同时学习多种风格。虽然这些方法为西方绘画创造了令人印象深刻的形象，但由于中国水墨画的本质不同，它们无法传递中国水墨画的风格</p></blockquote></li><li><p>GAN based approaches</p><blockquote><p>从GAN的角度处理风格转换任务时，一些图像到图像的翻译方法是相当有效的。</p><p>CoupledGAN[34]通过实施权重共享约束来学习多域图像的关联分布。</p><p>然而，这种方法只能以一个noise vector作为输入来生成成对的图像。</p><p>因此，它不能直接用作样式转换模型。</p><p>Liuet等人[33]将CoupledGAN[34]与变分自动编码器[24]结合起来，提出了一个名为UNIT[33]的框架。</p><p>Zhuet al.引入循环一致性损失来减少映射的置换，并提出CycleGAN[50]。</p><p>基于CycleGAN[50]的体系结构，DistanceGAN[2]实施了一个约束，即在映射到另一个域时，一个域中两个样本的距离应保持不变。</p><p>我们还在模型中选择周期一致性损失来克服模式崩溃[13]，并将其与对抗性损失相结合来模拟空洞。</p><p>虽然周期一致性损失使模型保留了原始照片中的一些细节，但它同时也会错误地删除一些重要的笔画，这促使我们为中国水墨画的笔画建模设置额外的约束</p></blockquote></li><li><p>Computational methods for Chinese ink wash paintings</p><blockquote><p>中国水墨画可以使用不同的计算方法生成。</p><p>Yuetal.将真实绘画的笔触纹理与给定景观图像的颜色信息相结合，合成一幅水墨画。</p><p>Xuetal.用事先准备好的工具分解中国水墨画的笔触用于渲染动画的笔刷笔划库。</p><p>Yang和Xu通过提供自动笔刷笔划轨迹估计，进一步完善了笔刷笔划分解方法。</p><p>Wang基于Kubelka-Munk方程，提出了一种模拟水墨扩散的有效算法。</p><p>Yehet al.和Wayetal.基于3D模型的板线笔划和内部着色生成链接水墨画。</p><p>Liang和Jin通过对边缘、颜色和纸张纹理的图像处理，从给定的照片生成水墨画。</p><p>我们的方法不再像以前那样依赖现有的<strong>笔画模拟</strong>和<strong>低级图像特征</strong>，而是<strong>探索数据驱动技术来学习真实的中国水墨画特征表示</strong></p></blockquote></li></ul><h1 id="提议的方法">提议的方法</h1><p>chipGAN学习从照片领域（例如，由现实世界的马的照片定义）到绘画领域（例如，由中国水墨画的马定义）的映射。</p><ul><li><p>在<strong>空白约束</strong>中，我们结合<strong>循环一致性损失</strong>和对抗性损失作为处理空白(voids)技术的约束条件</p></li><li><p>在<strong>笔触约束</strong>中，提出了brushstrokeloss去除不必要的笔触，同时保留精华；</p></li><li><p>在<strong>水墨约束</strong>中添加了<strong>扩散效应</strong>（diffusioneffect）和<strong>水墨损失</strong>，以确保整个图像的正确色调</p></li></ul><p><img src="chipan结构图.png" /></p><h2 id="void-constraint">1.Void constraint</h2><p>直观地说，应用空白意味着在画布上的适当位置留下空白。</p><p>以马为例，适当地应用空洞需要生成的图像完全忽略天空，并且部分地忽略照片中的草，同时清晰地保持马的轮廓，如图的中间部分所示。</p><p>马的照片和一幅中国水墨画具有不同的熵，因为照片与绘画图像相比具有丰富的颜色和纹理。</p><p>在图像到图像的翻译任务中，利用源域和目标域之间的这种不同熵，通过组合对抗性损失和循环一致性损失，将源图像的信息有效地转换为几乎不可感知的信号。</p><p>因此，我们采用了类似的策略来实施空白约束</p><h3 id="adversarial-loss">Adversarial loss</h3><p>给出被认为是分别为X和Y的域的未配对的训练集合</p><p>我们的模型给出两个映射关系：<span class="math inline">\(G :X\rightarrow Y \; F:Y \rightarrow X\)</span></p><p>对于$G : XY <span class="math inline">\(以及他的辨别器\)</span>D_Y$的对抗损失是这样定义的</p><p><img src="adv%20loss.png" /></p><h3 id="cycle-consistency-loss">Cycle consistency loss</h3><p>​ 我们通过将给定的图像X从domain X totarget域Y中翻译出来，然后返回到domainX，这将产生相同的图像，从而增加循环一致性约束，公式表达为<spanclass="math inline">\(F(G(X))\approx X\)</span>。由于循环一致性约束要求在两个方向上进行恢复，因此对于每个在DomainY的图像，还存在一个循环一致性约束$G(F(Y)) Y $</p><p><img src="cycle%20loss.png" /></p><p>这种约束使得生成的图像保留了源域的一些信息，从而可以将生成的图像转换回源域。</p><h2 id="brush-stroke-constraint">Brush stroke constraint</h2><p><img src="笔触结果.png" alt="截屏2022-03-13 13.34.26" style="zoom:20%;" /></p><p>​考虑到正确生成的空白区域，我们的下一个目标是增加笔触，以清晰地描绘中国水墨画风格的物体轮廓例如马的头部和身体应该有强烈的轮廓。</p><p>​为了以统一的方式对中国水墨画中不同厚度的各种类型的笔划进行建模，我们制定了我们的<strong>笔触约束</strong>，用于加强真实照片和生成绘画的不同级别的边缘映射之间的一致性。</p><p>​我们采用<strong>整体嵌套的边缘检测器</strong>从输入图像中提取五层边缘，以模拟五种不同厚度的笔划。</p><p>​然后，我们合并从预先训练的VGG-16特征提取程序的不同阶段生成的边缘映射，以获得最终的边缘映射。与将边缘检测任务视为像素级<strong>二值分类问题</strong>不同，我们从<strong>回归</strong>的角度训练了一个多级边缘检测器，以获得<strong>不同厚度</strong>的平滑笔划。</p><p>​ training groundtruth中的每个像素都用0到1的实数标记，这表明它们可能是边缘的一部分。</p><p>​ 我们获得真实图像的edge map E(x) 以及 生成图像的edge map E(G(x))</p><p>​ 然后我们将E(x)作为ground truth然后计算平衡交叉熵损失以让G生成正确的笔触</p><p><img src="笔触.png" /></p><ul><li>N是照片或假画边缘图中的像素总数</li><li><span class="math inline">\(\mu\)</span>是一个平衡权重<ul><li><span class="math inline">\(\mu\)</span> =N_/N and 1-<spanclass="math inline">\(\mu\)</span> = N+/N</li><li>N_ 是边缘图中所有不是边缘点的可能性之和</li><li>N+是边缘图中所有是边缘点的可能性之和</li></ul></li></ul><h2 id="ink-wash-constraint">ink wash constraint</h2><p>​正确模拟了空洞和笔触，我们的最终处理是使<strong>全局色调</strong>（例如，生成的马画的整体色温应接近真实色温）和<strong>扩散效果</strong>（例如，马的腹部显示链接扩散到宣纸上的不同灰度）在真实绘制和生成绘制g（x）之间保持一致。因此，我们进一步引入了<strong>水墨约束</strong>。</p><p>​水墨在宣纸上的扩散是近似各向同性的，所以我们用<strong>侵蚀操作</strong>和<strong>高斯模糊操作</strong>来模拟它。</p><p>​当突出的物体被模糊时，这个操作会抑制纹理和内容信息的显式比较，因此，该模型更倾向于关注tone（风格？）的一致性，如图所示</p><p><img src="ink%20water.png" /></p><p>因此，我们添加了一个对抗性鉴别器<spanclass="math inline">\(D_I\)</span>，用于区分<spanclass="math inline">\(y_{eb}\)</span>和<spanclass="math inline">\(G(x)_eb\)</span></p><p><img src="ink%20water%20yeb.png" /></p><ul><li>y_eb是经过侵蚀和模糊处理的真实绘画</li><li>G(x)_eb是经过侵蚀和模糊处理的生成绘画</li><li>⊖是侵蚀操作</li><li>B是一个侵蚀核</li><li>高斯模糊核 <span class="math inline">\(G_{k,l} =\frac{1}{2\pi\sigma^2}exp(-\frac{k^2+l^2}{2\sigma^2})\)</span></li></ul><p>最后我们定一个水墨画损失:</p><p><img src="ink%20water%20loss.png" /></p><h2 id="full-objective">Full objective</h2><p>我们的全部目标是上述四种损失的线性组</p><p><img src="funll%20loss.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概要&quot;&gt;概要&lt;/h1&gt;
&lt;p&gt;​
风格转换已成功应用于照片，生成逼真的西方绘画。然而，由于中西绘画技法的内在差异，直接套用已有的方法，对中国水墨画风格的转换并不能产生令人满意的效果。本文提出了一种基于ChipGAN的端到端（
end-to-end）生成对抗性网络体</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="图像生成" scheme="http://www.larryai.com/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
  </entry>
  
  <entry>
    <title>MGUIT</title>
    <link href="http://www.larryai.com/2022/05/04/MGUIT/"/>
    <id>http://www.larryai.com/2022/05/04/MGUIT/</id>
    <published>2022-05-04T07:19:48.000Z</published>
    <updated>2022-05-04T07:39:26.647Z</updated>
    
    <content type="html"><![CDATA[<p><img src="1.png" /></p><h1 id="class-aware-memory-network">Class-aware Memory Network</h1><p><img src="2.png" /></p><h2 id="结构理解">结构理解</h2><blockquote><p>每个class下有<spanclass="math inline">\(N_k\)</span>个item,所有class的item数之和为N，即一共有N个item。</p><p>满足 <span class="math inline">\(\sum_{k=1}^K N_k = N\)</span></p></blockquote><h3 id="对于每个class要用n_k个item的理解">对于每个class要用<spanclass="math inline">\(N_k\)</span>个item的理解：</h3><blockquote><p>比如一个class为人脸，那么人脸上的眼睛、嘴唇、皮肤都是不一样的特征，因此需要用到多个item表示一个class</p></blockquote><h3 id="item的组成">item的组成</h3><blockquote><p>每个item由( k , <span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span>)组成</p><ul><li>k , <span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span> 的大小都是 1x1xC 其中C是通道数</li><li>k用来检索item 大小1x1xC 方便与每个像素进行求余弦相似度用来表示content feature</li><li><span class="math inline">\(v^x\)</span>表示在 x domin 上的stylefeature</li><li><span class="math inline">\(v^y\)</span>表示在 y domin 上的stylefeature</li></ul></blockquote><h3 id="特征聚类">特征聚类</h3><blockquote><p>训练的时候，会用到数据集中的objectannotations（对象注释比如物体的种类，物体的框的位置）来辅助。</p><p>每张图的特征(c,s)会被聚类为K类：（K就是这张图像的所有class）</p><p>(𝑐1,𝑠1),···,(𝑐𝐾,𝑠𝐾)</p><ul><li><p>c1中有<span class="math inline">\(N_1^c\)</span>个items1中有<span class="math inline">\(N_1^s\)</span>个item</p></li><li><p><span class="math inline">\(c_k\)</span>中 有<spanclass="math inline">\(N_k^c\)</span>个item <spanclass="math inline">\(s_k\)</span>中 有<spanclass="math inline">\(N_k^s\)</span>个item</p></li></ul><p>这些聚类后的信息read / update memory</p></blockquote><h2 id="read">Read</h2><h3 id="前置知识">前置知识</h3><ul><li><p>假设每张图像大小为CxHxW ， 那么一个通道上面就有P=HxW个像素点每个像素点的content feature 为 1x1xC</p></li><li><p>memory中有K个class，N个item 每个item有一个k其中k的大小为1x1xC</p></li><li><p>余弦相似度的公式定义 <span class="math display">\[d(c_p , k_n) = \frac{c_pk_n^T}{|\left |  c_p|\right|_2 |\left|  k_n|\right|_2}\]</span></p></li></ul><h3id="计算第p个像素点与第n个item的相似度权重">计算第p个像素点与第n个item的相似度权重</h3><blockquote><p>一共有P个像素点，第p个像素点的大小为1x1xC，记为<spanclass="math inline">\(c_p\)</span></p><p>一共有N个item，第n个item的k的大小为1x1xC，记为<spanclass="math inline">\(k_n\)</span></p></blockquote><p>计算<span class="math inline">\(c_p\)</span>与<spanclass="math inline">\(k_n\)</span>的相似度，并在N的维度上求softmax作为每个kn的权重</p><p><img src="3.png" /></p><ul><li><span class="math inline">\(\alpha_{p,n}^x\)</span>表示对于xdomin上第p个像素点与memory中第n个item的k的相似度权重（属于第n类的概率）</li><li><span class="math inline">\(\alpha_{p,n}^y\)</span>表示对于ydomin上第p个像素点与memory中第n个item的k的相似度权重（属于第n类的概率）</li></ul><h3 id="计算风格特征hats">计算风格特征<spanclass="math inline">\(\hat{s}\)</span></h3><blockquote><p>第p个像素点的风格特征就是 对每个item的style feature 进行加权求和 ，其中权重是<spanclass="math inline">\(c_p\)</span>与item的k的相似度权重</p></blockquote><p><img src="4.png" /></p><ul><li><span class="math inline">\(v_{n}^x\)</span>表示xdomin上第n个item的style feature</li><li><span class="math inline">\(v_{n}^y\)</span>表示ydomin上第n个item的style feature</li></ul><blockquote><p>最终所有的p聚合成最后的特征图</p></blockquote><h2 id="update">update</h2><p>update是求权重然后按权重更新（<span class="math inline">\(k\)</span>, <span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span>）</p><h3id="计算第n个item的k与第p个像素点的相似度权重">计算第n个item的k与第p个像素点的相似度权重</h3><p><img src="5.png" /></p><blockquote><p>与read的计算权重不同，update是在P的维度上求softmax作为每个p的权重（第n个类与哪些p比较接近）</p></blockquote><h3 id="更新k-vx-vy">更新（<span class="math inline">\(k\)</span> ,<span class="math inline">\(v^x\)</span> , <spanclass="math inline">\(v^y\)</span>）</h3><blockquote><p>对第n个item的content feature进行更新 加上每个像素点的contentfeature的加权和</p><p>对第n个item的style feature进行更新 加上每个像素点对应的stylefeature的加权和</p></blockquote><p><img src="6.png" /></p><blockquote><p>k保存的content feature是两个domain共享的，所以一起更新</p><p>而v保存的style feature是两个domain单独的，所以分开更新。</p></blockquote><p>更新步骤如下:</p><p><img src="7.png" /></p><h1 id="loss-function">Loss Function</h1><h2 id="image-to-image-translation-network">image-to-image translationnetwork</h2><h3 id="reconstruction-loss-重建损失">1.Reconstruction loss(重建损失)</h3><p><img src="8.png" /></p><ul><li><span class="math inline">\(L^{self}\)</span>是x的原始contentfeature与构建的x style生成的图像与原图越逼近 ， 使得xstyle更加精确表达x的风格</li><li><spanclass="math inline">\(L^{cyc}\)</span>是将x内容特征与y风格特征产生的图像再用x风格特征产生的图像与 原图逼近</li></ul><h3 id="adversarial-loss">2.Adversarial loss</h3><blockquote><p>目的是为了最小化两个不同功能之间的分布差异</p><p>我们采用了两种对抗性损失函数：</p><ul><li><strong>contentdiscriminato</strong>：Cx和Cy之间的内容对抗性损失函数<ul><li>使得x的内容在y风格下仍旧保持原本的内容</li></ul></li><li><strong>domain discriminator</strong>：X和Y领域对抗性损失函数</li></ul></blockquote><h3 id="kl-loss">3.KL loss</h3><blockquote><p>使style的分布更接近先前的高斯分布</p></blockquote><h3 id="latent-regression-loss">4.Latent regression loss</h3><blockquote><p>使得style和image之间的映射是可逆的</p></blockquote><h2 id="class-aware-memory-network-1">Class-aware memory network</h2><h3 id="feature-contrastive-loss特征对比损失">Feature contrastiveloss（特征对比损失）</h3><blockquote><p>对于每一个特征<span class="math inline">\(c_p\)</span>(或<spanclass="math inline">\(s_p\)</span>)，我们将它最近的item <spanclass="math inline">\(k_p\)</span>（或<spanclass="math inline">\(v_p\)</span>）定义为正样本，其他样本为负样本。</p><p>到正/负样本的距离如如下方式惩罚</p><p>τ是控制浓度分布水平的温度参数</p></blockquote><p><img src="9.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;1.png&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;class-aware-memory-network&quot;&gt;Class-aware Memory Network&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;2.png&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;结构理解&quot;&gt;结构理解</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="GAN" scheme="http://www.larryai.com/tags/GAN/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性链表</title>
    <link href="http://www.larryai.com/2022/05/04/%E7%BA%BF%E6%80%A7%E9%93%BE%E8%A1%A8/"/>
    <id>http://www.larryai.com/2022/05/04/%E7%BA%BF%E6%80%A7%E9%93%BE%E8%A1%A8/</id>
    <published>2022-05-04T07:15:02.000Z</published>
    <updated>2022-05-04T07:15:30.981Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性链表">线性链表</h1><h3 id="源代码清单">1.1源代码清单:</h3><ul><li><h5 id="linklist.cpp">LinkList.cpp</h5></li><li><h5 id="linklist.h">LinkList.h</h5></li></ul><h3 id="线性链表数据结构">1.2线性链表数据结构:</h3><blockquote><p>本程序元素类型为int,读者可根据需求自行更改数据类型。<br /></p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单链表数据结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">LNode</span>&#123;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">   <span class="keyword">struct</span> <span class="title class_">LNode</span> *next;</span><br><span class="line">&#125;LNode , *LinkList;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="线性链表的基本操作">1.3线性链表的基本操作:</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">InitList</span><span class="params">(LinkList &amp;L)</span></span>;</span><br><span class="line"><span class="comment">// 销毁 释放链表所占内存，头结点也会被清理。</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">DestroyList</span><span class="params">(LinkList* L)</span></span>;</span><br><span class="line"><span class="comment">// 置空 这里需要释放链表中非头结点处的空间。</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ClearList</span><span class="params">(LinkList L)</span></span>;</span><br><span class="line"><span class="comment">// 判空 判断链表中是否包含有效数据。</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ListEmpty</span><span class="params">(LinkList L)</span></span>;</span><br><span class="line"><span class="comment">// 计数</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListLength</span><span class="params">(LinkList L)</span></span>;</span><br><span class="line"><span class="comment">// 取值</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">GetElem</span><span class="params">(LinkList L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>;</span><br><span class="line"><span class="comment">// 查找 返回链表中首个与e满足Compare关系的元素位序。 如果不存在这样的元素，则返回0。</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">LocateElem</span><span class="params">(LinkList L, <span class="type">int</span> e, <span class="type">bool</span>(Compare)(<span class="type">int</span>, <span class="type">int</span>))</span></span>;</span><br><span class="line"><span class="comment">// 前驱</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">PriorElem</span><span class="params">(LinkList L, <span class="type">int</span> cur_e,<span class="type">int</span>* pre_e)</span></span>;</span><br><span class="line"><span class="comment">// 后继</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">NextElem</span><span class="params">(LinkList L, <span class="type">int</span> cur_e, <span class="type">int</span>* next_e)</span></span>;</span><br><span class="line"><span class="comment">// 插入</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListInsert</span><span class="params">(LinkList L, <span class="type">int</span> i, <span class="type">int</span> e)</span></span>;</span><br><span class="line"><span class="comment">// 删除</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListDelete</span><span class="params">(LinkList L, <span class="type">int</span> i, <span class="type">int</span>* e)</span></span>;</span><br><span class="line"><span class="comment">// 遍历</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">ListTraverse</span><span class="params">(LinkList L, <span class="type">void</span>(Visit)(<span class="type">int</span>))</span></span>;</span><br></pre></td></tr></table></figure><h3 id="linklist.h-1">1.4 LinkList.h</h3><ul><li><h4 id="宏定义">宏定义</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">/* 状态码 */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> TRUE        1   <span class="comment">// 真/是</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FALSE       0   <span class="comment">// 假/否</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OK          1   <span class="comment">// 通过/成功</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ERROR       -1   <span class="comment">// 错误/失败</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//系统中已有此状态码定义，要防止冲突</span></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> OVERFLOW</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OVERFLOW    -2  <span class="comment">//堆栈上溢</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure><ul><li><h4 id="初始化表">初始化表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">InitList</span><span class="params">(LinkList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="comment">//创建头节点</span></span><br><span class="line">    L = (LinkList) <span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">    <span class="comment">//创建是否成功</span></span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="built_in">exit</span>(OVERFLOW);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//头节点指向空</span></span><br><span class="line">    L-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="销毁线性表">销毁线性表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">DestroyList</span><span class="params">(LinkList&amp; L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    &#125;</span><br><span class="line">    LinkList p =L;</span><br><span class="line">    <span class="keyword">while</span>(p-&gt;next!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">        p=L-&gt;next;</span><br><span class="line">        <span class="built_in">free</span>(L);</span><br><span class="line">        L=p;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="置空线性表">置空线性表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ClearList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    L.length=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="判断是否为空">判断是否为空</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ListEmpty</span><span class="params">(LinkList L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">return</span> L-&gt;next==<span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="线性表长度">线性表长度</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListLength</span><span class="params">(LinkList L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    LinkList p=L-&gt;next;</span><br><span class="line">    <span class="type">int</span> count=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="获取值">获取值</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">GetElem</span><span class="params">(LinkList L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> index=<span class="number">0</span>;</span><br><span class="line">    LinkList p=L-&gt;next;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span> &amp;&amp; index&lt;i)&#123;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">        index++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        e = p-&gt;data;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="比较">比较</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Compare</span><span class="params">(<span class="type">int</span> a , <span class="type">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a==b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="定位">定位</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">LocateElem</span><span class="params">(LinkList L, <span class="type">int</span> e, <span class="type">bool</span>(Compare)(<span class="type">int</span>, <span class="type">int</span>))</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    LinkList p = L-&gt;next;</span><br><span class="line">    <span class="type">int</span> index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span> &amp;&amp; !<span class="built_in">Compare</span>(p-&gt;data,e))&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">        index++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="前驱">前驱</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">PriorElem</span><span class="params">(LinkList L, <span class="type">int</span> cur_e,<span class="type">int</span>&amp; pre_e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)<span class="keyword">return</span> ERROR;</span><br><span class="line">    LinkList p = L-&gt;next;</span><br><span class="line">    <span class="comment">//第一个节点没有前驱</span></span><br><span class="line">    <span class="keyword">if</span>(cur_e==p-&gt;data)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">while</span>(p-&gt;next!=<span class="literal">NULL</span> &amp;&amp; p-&gt;next-&gt;data!=cur_e)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p-&gt;next==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">       pre_e =  p-&gt;data;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="后继">后继</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">NextElem</span><span class="params">(LinkList L, <span class="type">int</span> cur_e, <span class="type">int</span>&amp; next_e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)<span class="keyword">return</span> ERROR;</span><br><span class="line">    LinkList p = L-&gt;next;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span> &amp;&amp; p-&gt;data!=cur_e)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(p-&gt;next==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        next_e =  p-&gt;next-&gt;data;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="插入">插入</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListInsert</span><span class="params">(LinkList L, <span class="type">int</span> i, <span class="type">int</span> e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)<span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//移动到第i-1个节点 因为要到i-1个节点 当i=0是就要到头节点 所以p一开始只指向头节点</span></span><br><span class="line">    <span class="type">int</span> index =<span class="number">0</span>;</span><br><span class="line">    LinkList p =L;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span> &amp;&amp; index&lt;i)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">        index++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//新建一个节点</span></span><br><span class="line">    LNode* newNode = (LNode*) <span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(LNode));</span><br><span class="line">    newNode-&gt;data = e;</span><br><span class="line">    <span class="comment">//进行插入</span></span><br><span class="line">    newNode-&gt;next = p-&gt;next;</span><br><span class="line">    p-&gt;next = newNode;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="删除">删除</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListDelete</span><span class="params">(LinkList L, <span class="type">int</span> i, <span class="type">int</span>&amp; e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)<span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//移动到第i-1个节点 因为要到i-1个节点 当i=0是就要到头节点 所以p一开始只指向头节点</span></span><br><span class="line">    <span class="type">int</span> index =<span class="number">0</span>;</span><br><span class="line">    LinkList p =L;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span> &amp;&amp; index&lt;i)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">        index++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//删除节点</span></span><br><span class="line">    LinkList q;</span><br><span class="line">    q = p-&gt;next;</span><br><span class="line">    p-&gt;next = q-&gt;next;</span><br><span class="line">    e = q-&gt;data;</span><br><span class="line">    <span class="built_in">free</span>(q);</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="访问元素">访问元素</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Visit</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    cout&lt;&lt;x&lt;&lt;<span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="输出顺序表">输出顺序表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">ListTraverse</span><span class="params">(LinkList L, <span class="type">void</span>(Visit)(<span class="type">int</span>))</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L==<span class="literal">NULL</span>)</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;数据为空&quot;</span>&lt;&lt;endl;</span><br><span class="line">    LinkList p =L-&gt;next;</span><br><span class="line">    <span class="keyword">while</span>(p!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="built_in">Visit</span>(p-&gt;data);</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;<span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="sqlist.cpp">1.5 SqList.cpp</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;LinkList.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">const</span> <span class="type">char</span> * argv[])</span> </span>&#123;</span><br><span class="line">    LinkList myList;</span><br><span class="line">    <span class="built_in">InitList</span>(myList);</span><br><span class="line">    <span class="type">int</span> num;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入数据数量:&quot;</span>&lt;&lt;endl;</span><br><span class="line">    cin&gt;&gt;num;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入数据:&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> elem;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span> ; i&lt;num;i++)&#123;</span><br><span class="line">        cin&gt;&gt;elem;</span><br><span class="line">        <span class="built_in">ListInsert</span>(myList, i, elem);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">ListTraverse</span>(myList, Visit);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;长度是:&quot;</span>&lt;&lt;<span class="built_in">ListLength</span>(myList);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;是否为空:&quot;</span>&lt;&lt;<span class="built_in">ListEmpty</span>(myList);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入一个想定位的数据:&quot;</span>;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    cin&gt;&gt;data;</span><br><span class="line">    cout&lt;&lt;data&lt;&lt;<span class="string">&quot;定位是:&quot;</span>&lt;&lt;<span class="built_in">LocateElem</span>(myList, data, Compare)&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> pre_data;</span><br><span class="line">    <span class="built_in">PriorElem</span>(myList, data, pre_data);</span><br><span class="line">    <span class="type">int</span> after_data;</span><br><span class="line">    <span class="built_in">NextElem</span>(myList, data, after_data);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;前驱是:&quot;</span>&lt;&lt;pre_data&lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt;<span class="string">&quot;后继是:&quot;</span>&lt;&lt;after_data&lt;&lt;<span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;删除元素&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> deldata;</span><br><span class="line">    <span class="built_in">ListDelete</span>(myList, <span class="number">3</span>, deldata);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;删除的元素是:&quot;</span>&lt;&lt;deldata&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">ListTraverse</span>(myList, Visit);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;线性链表&quot;&gt;线性链表&lt;/h1&gt;
&lt;h3 id=&quot;源代码清单&quot;&gt;1.1源代码清单:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h5 id=&quot;linklist.cpp&quot;&gt;LinkList.cpp&lt;/h5&gt;&lt;/li&gt;
&lt;li&gt;&lt;h5 id=&quot;linklist.h&quot;&gt;LinkList</summary>
      
    
    
    
    <category term="数据结构" scheme="http://www.larryai.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="C++" scheme="http://www.larryai.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>顺序表</title>
    <link href="http://www.larryai.com/2022/05/04/%E9%A1%BA%E5%BA%8F%E8%A1%A8/"/>
    <id>http://www.larryai.com/2022/05/04/%E9%A1%BA%E5%BA%8F%E8%A1%A8/</id>
    <published>2022-05-04T03:37:36.000Z</published>
    <updated>2022-05-04T07:14:49.099Z</updated>
    
    <content type="html"><![CDATA[<h1 id="顺序表">顺序表</h1><h3 id="源代码清单">1.1源代码清单:</h3><ul><li><h5 id="sqlist.cpp">SqList.cpp</h5></li><li><h5 id="sqlist.h">SqList.h</h5></li></ul><h3 id="顺序表数据结构">1.2顺序表数据结构:</h3><blockquote><p>本程序元素类型为int,读者可根据需求自行更改数据类型。</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//线性表结构体</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">int</span>* elem; <span class="comment">//线性表数据</span></span><br><span class="line">    <span class="type">int</span> length; <span class="comment">//线性表长度</span></span><br><span class="line">    <span class="type">int</span> listSize; <span class="comment">//线性表大小</span></span><br><span class="line">&#125;SqList;</span><br></pre></td></tr></table></figure><h3 id="该顺序表设计到的方法">1.3该顺序表设计到的方法:</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span>  <span class="title">InitList</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//初始化线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">DestroyList</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//销毁线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">ClearList</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//置空线性表</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ListEmpty</span><span class="params">(SqList L)</span></span>; <span class="comment">//判断是否为空</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">ListLength</span><span class="params">(SqList &amp;L)</span></span>; <span class="comment">//线性表长度</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">GetElem</span><span class="params">(SqList L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>; <span class="comment">//获取值</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Compare</span><span class="params">(<span class="type">int</span> a , <span class="type">int</span> b)</span></span>; <span class="comment">//比较</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">LocateElem</span><span class="params">(SqList L, <span class="type">int</span> e, <span class="type">bool</span>(Compare)(<span class="type">int</span>, <span class="type">int</span>))</span></span>; <span class="comment">//定位</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">PriorElem</span><span class="params">(SqList L, <span class="type">int</span> cur_e , <span class="type">int</span> &amp;pre_e)</span></span>; <span class="comment">//前驱</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">NestElem</span><span class="params">(SqList L , <span class="type">int</span> cur_e , <span class="type">int</span> &amp;next_e)</span></span>; <span class="comment">//后继</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">InsertList</span><span class="params">(SqList &amp;L,<span class="type">int</span> i ,<span class="type">int</span> elem)</span></span>; <span class="comment">//插入</span></span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">ListDelete</span><span class="params">(SqList &amp;L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>; <span class="comment">//删除</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Visit</span><span class="params">(<span class="type">int</span> x)</span></span>; <span class="comment">//访问</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">visitList</span><span class="params">(SqList L,<span class="type">void</span>(Visit)(<span class="type">int</span>))</span></span>; <span class="comment">//输出顺序表</span></span><br></pre></td></tr></table></figure><h3 id="sqlist.h-1">1.4 SqList.h</h3><ul><li><h4 id="宏定义">宏定义</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">/* 状态码 */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> TRUE        1   <span class="comment">// 真/是</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FALSE       0   <span class="comment">// 假/否</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OK          1   <span class="comment">// 通过/成功</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ERROR       0   <span class="comment">// 错误/失败</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//系统中已有此状态码定义，要防止冲突</span></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> OVERFLOW</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OVERFLOW    -2  <span class="comment">//堆栈上溢</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//线性表的宏定义</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> listInitSize 20</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> listSizeStride 10 <span class="comment">//空间增加的步长</span></span></span><br></pre></td></tr></table></figure><ul><li><h4 id="初始化表">初始化表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">InitList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="comment">//给线性表数据部分分配空间</span></span><br><span class="line">    L.elem = (<span class="type">int</span>*)<span class="built_in">malloc</span>(listInitSize * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    <span class="comment">//分配空间失败</span></span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="comment">// 存储内存失败</span></span><br><span class="line">        <span class="built_in">exit</span>(OVERFLOW);</span><br><span class="line">    <span class="comment">//线性表长度</span></span><br><span class="line">    L.length=<span class="number">0</span>;</span><br><span class="line">    <span class="comment">//线性表大小</span></span><br><span class="line">    L.listSize=listInitSize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="销毁线性表">销毁线性表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//销毁线性表</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">DestroyList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="comment">//确保线性表存在</span></span><br><span class="line">    <span class="keyword">if</span>( L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放内存</span></span><br><span class="line">    <span class="built_in">free</span>(L.elem);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//置空指针</span></span><br><span class="line">    L.elem = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//长度内存的改变</span></span><br><span class="line">    L.length =<span class="number">0</span>;</span><br><span class="line">    L.listSize=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="置空线性表">置空线性表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ClearList</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    L.length=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="判断是否为空">判断是否为空</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span>  <span class="title">ListEmpty</span><span class="params">(SqList L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> L.length==<span class="number">0</span>?TRUE:ERROR;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="线性表长度">线性表长度</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListLength</span><span class="params">(SqList &amp;L)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> L.length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="获取值">获取值</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">GetElem</span><span class="params">(SqList L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    e = L.elem[i];</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="比较">比较</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Compare</span><span class="params">(<span class="type">int</span> a , <span class="type">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a==b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="定位">定位</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">LocateElem</span><span class="params">(SqList L, <span class="type">int</span> e, <span class="type">bool</span>(Compare)(<span class="type">int</span>, <span class="type">int</span>))</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//下标</span></span><br><span class="line">    <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="comment">//新建指针指向数据域</span></span><br><span class="line">    <span class="type">int</span>* p = L.elem;</span><br><span class="line">    <span class="comment">//遍历</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;L.length &amp;&amp; !<span class="built_in">Compare</span>(*p++,e))&#123;</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;L.length)</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;<span class="comment">//没找到</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="前驱">前驱</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span>  <span class="title">PriorElem</span><span class="params">(SqList L, <span class="type">int</span> cur_e , <span class="type">int</span> &amp;pre_e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> *p = L.elem;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;L.length &amp;&amp; !<span class="built_in">Compare</span>(*p++, cur_e))&#123;</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>||i&gt;=L.length)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        pre_e = L.elem[i<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="后继">后继</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">NestElem</span><span class="params">(SqList L , <span class="type">int</span> cur_e , <span class="type">int</span> &amp;next_e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> *p = L.elem;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;L.length &amp;&amp; !<span class="built_in">Compare</span>(*p++, cur_e))&#123;</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(i&gt;=L.length<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        next_e = L.elem[i+<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="插入">插入</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">InsertList</span><span class="params">(SqList &amp;L,<span class="type">int</span> i ,<span class="type">int</span> elem)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//插入位置是否正确</span></span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">0</span> || i&gt;L.length)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* newbase;<span class="comment">//预计新建空间</span></span><br><span class="line">    <span class="comment">//是否空间不足</span></span><br><span class="line">    <span class="keyword">if</span>(L.length&gt;=L.listSize)&#123;</span><br><span class="line">        newbase = (<span class="type">int</span>*) <span class="built_in">realloc</span>(L.elem, (L.listSize+listSizeStride)*<span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">        <span class="comment">//创建是否成功</span></span><br><span class="line">        <span class="keyword">if</span>(newbase==<span class="literal">NULL</span>)</span><br><span class="line">            <span class="built_in">exit</span>(OVERFLOW);</span><br><span class="line">        L.elem = newbase;</span><br><span class="line">        L.listSize += listSizeStride;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>*q = &amp;L.elem[i]; <span class="comment">//q指向要插入的位置</span></span><br><span class="line">    <span class="type">int</span>*p = &amp;L.elem[L.length<span class="number">-1</span>]; <span class="comment">//p指向数据尾部位</span></span><br><span class="line">    <span class="comment">//右移元素</span></span><br><span class="line">    <span class="keyword">while</span>(p&gt;=q)&#123;</span><br><span class="line">        *(p+<span class="number">1</span>) = *p;</span><br><span class="line">        p--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//插入元素</span></span><br><span class="line">    *q = elem;</span><br><span class="line">    <span class="comment">//表长加1</span></span><br><span class="line">    L.length++;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="删除">删除</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">ListDelete</span><span class="params">(SqList &amp;L, <span class="type">int</span> i, <span class="type">int</span> &amp;e)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="comment">//删除位置是否正确</span></span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">0</span> || i&gt;L.length)</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    <span class="type">int</span> *q = &amp;L.elem[i] ;<span class="comment">//指向要被删除的位置</span></span><br><span class="line">    <span class="type">int</span> *p = &amp;L.elem[L.length<span class="number">-1</span>]; <span class="comment">// 指向表尾元素</span></span><br><span class="line">    <span class="comment">//要删除的元素赋值给e</span></span><br><span class="line">    e = *q;</span><br><span class="line">    <span class="keyword">while</span>(q&lt;p)&#123;</span><br><span class="line">        *q = *(q+<span class="number">1</span>);</span><br><span class="line">        q++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//表长减1</span></span><br><span class="line">    L.length--;</span><br><span class="line">    <span class="keyword">return</span> OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="访问元素">访问元素</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Visit</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    cout&lt;&lt;x&lt;&lt;<span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><h4 id="输出顺序表">输出顺序表</h4></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">visitList</span><span class="params">(SqList L,<span class="type">void</span>(Visit)(<span class="type">int</span>))</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(L.elem==<span class="literal">NULL</span>)</span><br><span class="line">        cout&lt;&lt;<span class="string">&quot;数据为空&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;L.length;i++)</span><br><span class="line">        <span class="built_in">Visit</span>(L.elem[i]);</span><br><span class="line">    cout&lt;&lt;<span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="sqlist.cpp-1">1.5 SqList.cpp</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;SqList.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">const</span> <span class="type">char</span> * argv[])</span> </span>&#123;</span><br><span class="line">    SqList myList;</span><br><span class="line">    <span class="built_in">InitList</span>(myList);</span><br><span class="line">    <span class="type">int</span> num;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入数据数量:&quot;</span>&lt;&lt;endl;</span><br><span class="line">    cin&gt;&gt;num;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入数据:&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> elem;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span> ; i&lt;num;i++)&#123;</span><br><span class="line">        cin&gt;&gt;elem;</span><br><span class="line">        <span class="built_in">InsertList</span>(myList, i, elem);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">visitList</span>(myList, Visit);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;长度是:&quot;</span>&lt;&lt;<span class="built_in">ListLength</span>(myList);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;是否为空:&quot;</span>&lt;&lt;<span class="built_in">ListEmpty</span>(myList);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;输入一个想定位的数据:&quot;</span>;</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">    cin&gt;&gt;data;</span><br><span class="line">    cout&lt;&lt;data&lt;&lt;<span class="string">&quot;定位是:&quot;</span>&lt;&lt;<span class="built_in">LocateElem</span>(myList, data, Compare)&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> pre_data;</span><br><span class="line">    <span class="built_in">PriorElem</span>(myList, data, pre_data);</span><br><span class="line">    <span class="type">int</span> after_data;</span><br><span class="line">    <span class="built_in">NestElem</span>(myList, data, after_data);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;前驱是:&quot;</span>&lt;&lt;pre_data&lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt;<span class="string">&quot;后继是:&quot;</span>&lt;&lt;after_data&lt;&lt;<span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;删除元素&quot;</span>&lt;&lt;endl;</span><br><span class="line">    <span class="type">int</span> deldata;</span><br><span class="line">    <span class="built_in">ListDelete</span>(myList, <span class="number">3</span>, deldata);</span><br><span class="line">    cout&lt;&lt;<span class="string">&quot;删除的元素是:&quot;</span>&lt;&lt;deldata&lt;&lt;endl;</span><br><span class="line">    <span class="built_in">visitList</span>(myList, Visit);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;顺序表&quot;&gt;顺序表&lt;/h1&gt;
&lt;h3 id=&quot;源代码清单&quot;&gt;1.1源代码清单:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h5 id=&quot;sqlist.cpp&quot;&gt;SqList.cpp&lt;/h5&gt;&lt;/li&gt;
&lt;li&gt;&lt;h5 id=&quot;sqlist.h&quot;&gt;SqList.h&lt;/h5&gt;&lt;/l</summary>
      
    
    
    
    <category term="数据结构" scheme="http://www.larryai.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="C++" scheme="http://www.larryai.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
    <link href="http://www.larryai.com/2022/05/04/PLST-SR/"/>
    <id>http://www.larryai.com/2022/05/04/PLST-SR/</id>
    <published>2022-05-03T18:56:42.000Z</published>
    <updated>2022-05-04T07:37:36.967Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络模型">1.网络模型</h2><p><imgsrc="https://pic4.zhimg.com/v2-8e9936fdcfd4e8371720b9834f8f97d7_r.jpg" /></p><h3 id="组成部分">1.1 组成部分</h3><blockquote><p>网络模型总体分为两部分:Image Transform Net和VGG-16</p></blockquote><ul><li><strong>Image Transform Net</strong>是参数W待训练的网络</li><li><strong>VGG-16</strong>是已经预训练好参数的网络</li></ul><h3 id="工作原理">1.2 工作原理</h3><p><strong>(1) 输入</strong>为 :</p><ul><li>原始图像<span class="math inline">\(x\)</span></li><li>风格目标图<span class="math inline">\(y_s\)</span></li><li>内容目标图<span class="math inline">\(y_c\)</span></li></ul><p><strong>(2) Image Transform Net</strong>作用：</p><ul><li>将原始图像<span class="math inline">\(x\)</span>经过<strong>ImageTransform Net</strong>得到输出图像<spanclass="math inline">\(\hat{y}\)</span></li><li>映射关系为: <span class="math inline">\(\hat{y} =f_W(x)\)</span></li><li>其中W是Images Transform Net的参数 ， x是网络输入，y是网络输出。</li></ul><p><strong>(3) VGG-16</strong>作用：</p><ul><li><p>内容层面</p><blockquote><p>将<span class="math inline">\(\hat{y}\)</span> 与<spanclass="math inline">\(y_c\)</span>在VGG中间层的欧式距离作为Loss训练<strong>图像转换网络</strong></p><p>使得Image Transform Net输出的<spanclass="math inline">\(\hat{y}\)</span>与目标内容图<spanclass="math inline">\(y_c\)</span>越来越接近</p></blockquote></li><li><p>风格层面</p><blockquote><p>将<span class="math inline">\(\hat{y}\)</span> 与<spanclass="math inline">\(y_s\)</span>在VGG多个中间层得到的featuremap生成的Gram矩阵的欧式距离加权和作为Loss训练<strong>图像转换网络</strong></p><p>使得Image Transform Net输出的<spanclass="math inline">\(\hat{y}\)</span>与目标风格图<spanclass="math inline">\(y_s\)</span>越来越接近</p></blockquote></li></ul><h2 id="损失函数">2.损失函数</h2><h3 id="特征内容损失feature-reconstruction-loss">2.1特征内容损失(FeatureReconstruction Loss)</h3><p><span class="math display">\[\ell_{feat}^{\phi , j}(\hat{y},y) = \frac{1}{C_jH_jW_j}\Vert\phi_j(\hat{y})-\phi_j(y)\Vert_2\]</span></p><ul><li>j 表示VGG-16中间层代号</li><li>y表示特征目标图像</li><li><span class="math inline">\(\hat{y}\)</span>表示image transform net输出的图像</li><li><span class="math inline">\(\phi_{j}(y)\)</span>表示图像y在VGG-16中间层j时的输出</li><li><span class="math inline">\(\phi_{j}(\hat{y})\)</span> 表示图像<spanclass="math inline">\(\hat{y}\)</span>在VGG-16中间层j时的输出</li><li><spanclass="math inline">\(C_jH_jW_j\)</span>分别表示在VGG-16中间层j时的通道数、高度、宽度</li></ul><blockquote><p><strong>Feature Reconstruction Loss</strong>这数学公式就可以理解为两个图像在VGG-16中间层j的欧氏距离</p><p>越小说明VGG-16网络认为这两张图越接近</p></blockquote><h3 id="风格损失style-reconstruction-loss">2.2风格损失(<strong>StyleReconstruction Loss</strong>)</h3><ul><li><strong>Gram特征矩阵中的元素</strong> <span class="math display">\[G_{j}^{\phi}(x)_{c,c^{&#39;}} = \frac{1}{C_jH_jW_j} \sum_{h=1}^{H_j}\sum_{w=1}^{W_j}\phi_j(x)_{h,w,c}\phi_{j}(x)_{h,w,c^{&#39;}}\]</span></li></ul><blockquote><p>VGG中间层j的<strong>feature map</strong>大小为[<spanclass="math inline">\(C_j\)</span>,<spanclass="math inline">\(H_j\)</span>,<spanclass="math inline">\(W_j\)</span>]</p><p>我们经过<strong>flatten</strong>和<strong>矩阵转置</strong>操作可以变形为[<spanclass="math inline">\(C_j\)</span> , <spanclass="math inline">\(H_j*W_j\)</span>]和的[<spanclass="math inline">\(H_j*W_j\)</span> , <spanclass="math inline">\(C_j\)</span>]矩阵</p><p>再对两个作<strong>内积</strong>得到Gram Matrices大小为[<spanclass="math inline">\(C_j,C_j\)</span>]</p></blockquote><ul><li><strong>中间层j的风格损失</strong></li></ul><p><span class="math display">\[\ell_{style}^{\phi,j}(\hat{y},y) =\Vert G_j^{\phi}(\hat{y}) -G_j^{\phi}(y)\Vert_{F}^{2}\]</span></p><blockquote><p>计算图像<span class="math inline">\(y\)</span>和图像<spanclass="math inline">\(\hat{y}\)</span>两者VGG-16中间层j中gram矩阵距离的平方和</p></blockquote><h3 id="简单损失函数">2.3简单损失函数</h3><ul><li><p><strong>像素损失</strong></p><p>像素损失是输出图和目标图之间标准化的差距 <spanclass="math display">\[\ell_{pixel}({\hat{y},y}) = \frac{1}{CHW}\Vert \hat{y}-y\Vert_2^2\]</span></p></li><li><p><strong>全变差正则化</strong></p></li></ul><p>​为使得输出图像比较平滑，遵循了前人在特征反演上的研究，在超分辨率重建上使用了全变差正则化<spanclass="math inline">\(\ell_{TV}(\hat{y})\)</span></p><h2 id="image-transform-net细节">3.Image Transform Net细节</h2><h3 id="风格迁移">3.1 风格迁移</h3><table><thead><tr class="header"><th style="text-align: center;">Layer</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">9x9 conv , stride=2</td></tr><tr class="even"><td style="text-align: center;">3x3 conv , stride=2</td></tr><tr class="odd"><td style="text-align: center;">Residual blocks</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">...</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">3x3 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr><tr class="even"><td style="text-align: center;">9x9 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr></tbody></table><p><strong>具体解释</strong></p><blockquote><p>1.输入<span class="math inline">\(x\)</span> 大小为3x256x256</p><p>2.使用2层 stride=2 的卷积层进行<strong>下采样</strong></p><p>3.使用5个残差模块</p><p>4.使用2层stride=<spanclass="math inline">\(\frac{1}{2}\)</span>的卷积层进行<strong>上采样</strong></p><p>5.输出<span class="math inline">\(\hat{y}\)</span>大小为3x256x256</p></blockquote><p><strong>输入图像与输出图像大小相同先下采样再上采样的好处</strong></p><ul><li><strong><em>可计算复杂性</em></strong><ul><li>比较</li><li>3x3的C个卷积核 在CxHxW的图像上 需要 <spanclass="math inline">\(9C^2HW\)</span></li><li>3x3的DC个卷积核 在DC x <spanclass="math inline">\(\frac{H}{D}\)</span>x<spanclass="math inline">\(\frac{W}{D}\)</span> 的图像上 也需要<spanclass="math inline">\(9C^2HW\)</span></li><li>在下采样之后，我们可以使用一个<strong>更大的网络来获得相同的计算成本</strong></li></ul></li><li><strong><em>有效的感受野大小</em></strong><ul><li>优势就在于在输出中的每个像素都有输入中的<strong>大面积有效的感受野</strong></li><li>一个附加的3x3卷积层都能把感受野的大小<strong>增加2倍</strong></li><li>在用因子D进行下采样后，每个3x3的卷积增加<strong>感受野的大小到2D</strong></li><li>下采样使得相同数量的层<strong>给出了更大的感受野大小</strong></li></ul></li></ul><h3 id="超分辨率">3.2超分辨率</h3><p>假设<strong>上采样因子</strong>为<spanclass="math inline">\(f\)</span></p><table><thead><tr class="header"><th style="text-align: center;">Layer</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Residual blocks</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">...</td></tr><tr class="even"><td style="text-align: center;">Residual blocks</td></tr><tr class="odd"><td style="text-align: center;">3x3 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr><tr class="even"><td style="text-align: center;">3x3 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr><tr class="odd"><td style="text-align: center;">(<spanclass="math inline">\(一共使用\log_2{f}个conv\)</span>)</td></tr><tr class="even"><td style="text-align: center;">9x9 conv , stride=<spanclass="math inline">\(\frac{1}{2}\)</span></td></tr></tbody></table><p><strong>具体解释</strong></p><blockquote><p>1.输入<span class="math inline">\(x\)</span> 大小为3 x <spanclass="math inline">\(\frac{288}{f}\)</span> x <spanclass="math inline">\(\frac{288}{f}\)</span></p><p>2.使用 <strong>5</strong>个残差模块</p><p>3.使用_2{f}个stride=<spanclass="math inline">\(\frac{1}{2}\)</span>的卷积层进行<strong>上采样</strong></p><p>5.输出<span class="math inline">\(\hat{y}\)</span>大小为3x288x288</p></blockquote><h3 id="残差连接">3.3 残差连接</h3><p><imgsrc="https://pic2.zhimg.com/80/v2-c7677c713fa2df00682e864f41d581e9_720w.png" /></p><h3 id="其他细节">3.4其他细节</h3><ul><li>除开<strong>第一个和最后一个</strong>层用<strong>9x9</strong>的kernel其他所有卷积层都用<strong>3x3</strong>的kernels</li><li>优化方法选的是SGD（随机梯度下降法）</li><li>除去最后一层卷积层后连接Tanh激活层，其他非残差卷积层都连接BatchNorm归一层和ReLu激活层</li><li>上面的做法可以使得输出图像的像素值在 [0<em>,</em> 255]这个范围</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;网络模型&quot;&gt;1.网络模型&lt;/h2&gt;
&lt;p&gt;&lt;img
src=&quot;https://pic4.zhimg.com/v2-8e9936fdcfd4e8371720b9834f8f97d7_r.jpg&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;组成部分&quot;&gt;1.1 组成部分&lt;/h3&gt;
</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="超分重建" scheme="http://www.larryai.com/tags/%E8%B6%85%E5%88%86%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Image Style Transfer</title>
    <link href="http://www.larryai.com/2022/05/04/Image%20Style%20Transfer/"/>
    <id>http://www.larryai.com/2022/05/04/Image%20Style%20Transfer/</id>
    <published>2022-05-03T18:54:56.000Z</published>
    <updated>2022-05-06T08:01:58.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像风格迁移">图像风格迁移</h1><p><img src="fig1.png" /></p><h2 id="损失函数组成">损失函数组成</h2><blockquote><h4 id="loss-w1-lc-w2-ls">Loss = w1 * Lc + w2 * Ls</h4></blockquote><ul><li><h3 id="loss-of-contentlc"><strong>Loss ofcontent(Lc)</strong></h3></li><li><h3 id="loss-of-stylels"><strong>Loss ofstyle(Ls)</strong></h3></li></ul><h2 id="loss-of-content">Loss of content</h2><blockquote><p>内容图和随机噪声图经过多次卷积滤波后，conten和noise在第4层的featuremap的距离的平方和</p></blockquote><p><img src="fig2.png" /></p><h2 id="loss-of-style">Loss of style</h2><blockquote><p>先对风格图和噪声图的每一层卷积得到feature map</p><p>对feature map求gram矩阵</p><p>计算两者gram距离的平方和</p><p>将5层的结果加权求和</p></blockquote><p><img src="fig5.png" /></p><h2 id="实验图">实验图</h2><p><img src="fig4.png" /></p><blockquote><p>随着卷积网络层数增加，获得的特征映射更加抽象。</p><p>上图可以看出，层数增高的时候：</p><ul><li><p>内容<strong>重构图可变化性</strong>增加，具有更大的风格变化能力。</p></li><li><p>风格随着使用的层数越多，<strong>风格迁移的稳定性越强</strong>。</p></li></ul></blockquote><h2 id="gram矩阵">Gram矩阵</h2><h3 id="定义">定义</h3><blockquote><p>n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的<strong>格拉姆矩阵<em>(Grammatrix)</em></strong>，很明显，这是一个对称矩阵。</p></blockquote><p><img src="fig6.png" /></p><p><img src="fig7.png" /></p><h3 id="计算">计算</h3><blockquote><p>输入图像的feature map为<strong>[ ch, h, w]</strong>。</p><p>我们经过<strong>flatten</strong>和<strong>矩阵转置</strong>操作</p><p>可以变形为<strong>[ ch, h*w]</strong>和<strong>[ h*w,ch]</strong>的矩阵</p><p>再对两个作<strong>内积</strong>得到Gram Matrices</p></blockquote><h3 id="理解">理解</h3><blockquote><p>格拉姆矩阵可以看做feature之间的偏心协方差矩阵（即没有减去均值的协方差矩阵）</p><p>在featuremap中，每个数字都来自于一个特定滤波器在特定位置的卷积，因此<strong>每个数字代表一个特征的强度</strong></p><p>Gram计算的实际上是<strong>两两特征之间的相关性</strong>，哪两个特征是同时出现的，哪两个是此消彼长的等等。</p><p>因为为乘法操作 两两特征同时为高 结果才高</p></blockquote><blockquote><p>格拉姆矩阵用于度量<strong>各个维度自己的特性</strong>以及<strong>各个维度之间的关系</strong></p><p>内积之后得到的多尺度矩阵中:</p><ul><li><p>对角线元素提供了<strong>不同特征图各自的信息</strong></p></li><li><p>其余元素提供了<strong>不同特征图之间的相关信息</strong>。这样一个矩阵，既能体现出有哪些特征，又能体现出不同特征间的紧密程度</p></li></ul></blockquote><blockquote><p>gram矩阵是计算每个通道 i 的feature map与每个通道 j 的featuremap的内积</p><p>gram matrix的每个值可以说是代表 <strong>I 通道的feature map与 j通道的feature map的互相关程度</strong></p></blockquote><h2 id="参考链接">参考链接</h2><ul><li>https://www.cnblogs.com/yifanrensheng/p/12862174.html</li><li>https://blog.csdn.net/weixin_40759186/article/details/87804316</li><li>https://www.cnblogs.com/subic/p/8110478.html</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图像风格迁移&quot;&gt;图像风格迁移&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;fig1.png&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;损失函数组成&quot;&gt;损失函数组成&lt;/h2&gt;
&lt;blockquote&gt;
&lt;h4 id=&quot;loss-w1-lc-w2-ls&quot;&gt;Loss = w1 * Lc </summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="风格迁移" scheme="http://www.larryai.com/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
    <category term="cs.CV" scheme="http://www.larryai.com/tags/cs-CV/"/>
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://www.larryai.com/2022/05/04/transformer/"/>
    <id>http://www.larryai.com/2022/05/04/transformer/</id>
    <published>2022-05-03T18:42:20.000Z</published>
    <updated>2022-05-06T08:01:19.065Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。</p><p>性能最好的模型还通过注意力机制连接编码器和解码器。</p><p>我们提出了一种新的简单网络架构 <strong>Transformer</strong></p><p><strong>它完全基于注意力机制，完全摒弃了递归和卷积</strong></p><p>对两个机器翻译任务的实验表明，这些模型在质量上更优越，同时更可并行化，并且需要的训练时间显着减少。</p><ul><li>我们的模型在 WMT 2014 英德翻译任务上达到了 28.4BLEU，比现有的最佳结果（包括合奏）提高了 2 BLEU 以上。</li><li>在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5天后，建立了一个新的单模型 state-of-the-art BLEU 得分41.0，这是最好的训练成本的一小部分 文献中的模型。</li></ul><h1 id="introduction">1.Introduction</h1><p>循环神经网络(RNN)、长短期记忆(LSTM)和门控循环神经网络，尤其是在语言建模和机器翻译等序列建模和转导问题已被牢固确立为最先进的方法此后，许多努力继续推动循环语言模型和编码器-解码器架构的界限</p><p>循环模型通常沿输入和输出序列的符号位置考虑计算。</p><p>将位置与计算时间的步骤对齐，它们生成一系列隐藏状态ht，作为先前隐藏状态 ht-1 和位置 t 的输入的函数。</p><p>这种固有的顺序性质排除了训练示例中的并行化，这在更长的序列长度下变得至关重要，因为内存限制限制了示例之间的批处理。</p><p>最近的工作通过因式分解技巧 和条件计算显着提高了计算效率，同时在后者的情况下也提高了模型性能。</p><p>然而，顺序计算的基本约束仍然存在。</p><p>注意机制已成为各种任务中引人注目的序列建模和转导模型的组成部分，允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离</p><p>然而，除了少数情况，这种注意力机制与循环网络结合使用。</p><p>在这项工作中，我们提出了<strong>Transformer，这是一种避免重复的模型架构，而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系</strong></p><p>在八个 P100 GPU 上经过短短 12 小时的训练后，Transformer可以实现更多的并行化，并且可以在翻译质量方面达到新的水平</p><h1 id="background">2.Background</h1><p>减少顺序计算的目标也构成了扩展神经 GPU 、ByteNet 和 ConvS2S的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入的隐藏表示和输出位置。</p><p>在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增长</p><p>对于 ConvS2S 呈线性增长，而对于 ByteNet 则呈对数增长。</p><p>这使得学习远距离位置之间的依赖关系变得更加困难</p><p>在 Transformer中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而降低了有效分辨率，我们使用<strong>多头注意力(Multi-HeadAttention)</strong>来抵消这种影响</p><p><strong>自注意力(Self-attention)</strong>，有时称为内部注意力，是一种将单个序列的不同位置关联起来以计算序列表示的注意力机制</p><p>自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴涵和学习任务无关的句子表示</p><p><strong>端到端记忆网络(End-to-end memorynetwork)</strong>基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务中表现良好</p><p>然而，据我们所知</p><p><strong>Transformer是第一个完全依赖自注意力来计算其输入和输出表示而不使用序列对齐 RNN或卷积的转换模型</strong></p><p>在接下来的部分中，我们将描述Transformer，激发自注意力并讨论其相对于其他模型的优势</p><h1 id="model-architecture">3.Model Architecture</h1><p>大多数竞争性神经序列转导模型具有编码器-解码器结构</p><p>编码器将符号表示的输入序列 (x1, ..., xn) 映射到连续表示的序列 z =(z1, ..., zn)</p><blockquote><p>其中z1是一个向量 用一个向量来表示x1</p></blockquote><p>给定 z，解码器然后一次生成一个元素的符号输出序列 (y1, ..., ym)</p><p>在每个步骤中，模型都是自回归(auto-regressive)的,<strong><em>在生成下一个时将先前生成的符号用作附加输入</em></strong></p><p>Transformer遵循这种整体架构，对编码器和解码器使用堆叠的自注意力(self-attention)和point-wise</p><p>编码器和解码器的全连接层，分别如图 1 的左半部分和右半部分所示</p><p><img src="fig1.png" alt="结构图" style="zoom:100%;" /></p><h2 id="encoder-and-decoder-stacks">3.1Encoder and Decoder Stacks</h2><h4 id="encoder">3.1.1Encoder</h4><p>编码器由 N = 6 个相同的层组成 , 每层有两个子层</p><ul><li><p>第一个子层是 <strong>多头自注意力机制(multi-head self-attentionmechanism)</strong></p></li><li><p>第二个子层是simple, position-wise fully connected feed-forwardnetwork.（说简单点就是<strong>MLP</strong>）</p></li></ul><p>我们在两个子层中的每一个周围使用残差连接，然后进行层归一化</p><p>即每个子层的输出为<strong>LayerNorm(x + Sublayer(x))</strong></p><p>其中Sublayer(x)是子层自己实现的函数</p><p>为了促进这些残差连接，模型中的所有子层以及嵌入层都会产生维度 dmodel =512 的输出</p><blockquote><p>LayerNorm的细节可以参考下面链接</p><p>https://blog.csdn.net/jump882/article/details/119795466</p></blockquote><h4 id="decoder">3.1.1Decoder</h4><p>解码器也由一堆 N = 6 个相同的层组成。</p><p>除了每个编码器层中的两个子层之外，解码器还插入了第三个子层</p><p>该子层对编码器堆栈的输出执行多头注意力（multi-head attention）</p><p>与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化</p><p>我们还修改了解码器堆栈中的自注意力子层，以<strong>防止位置关注后续位置</strong></p><p><strong>这种掩蔽与输出嵌入偏移一个位置的事实相结合，确保对位置 i的预测只能依赖于位置小于 i 的已知输出</strong></p><h2 id="attention">3.2Attention</h2><p>注意力函数可以描述为将aquery(查询)和一组key-value键值对映射到输出</p><p>其中<strong>查询query</strong>、<strong>键key</strong>、<strong>值value</strong>和<strong>输出</strong>都是向量</p><p><strong>输出可以理解为计算值value的加权和所得</strong></p><p>其中<strong>分配给每个value的权重weight由查询query与相应键key的相似度函数计算</strong></p><p>下面给了一张参考图</p><p><img src="attention1.png" /></p><h4 id="scaled-dot-product-attention">3.2.1 Scaled Dot-ProductAttention</h4><p><img src="fig2_left.png" /></p><p>我们将我们的particular attention称为“Scaled Dot-ProductAttention”</p><p>输入由维度 dk 的query和key以及维度 dv 的value组成</p><p><strong>我们计算的query和所有keys的点积，将每个key除以 <spanclass="math inline">\(\sqrt{d_k}\)</span>，然后应用 softmax函数来获得value的权重</strong></p><p>在实践中，我们同时计算一组querys的注意力函数，并打包到矩阵 Q 中</p><p>key和value也打包到矩阵 K 和 V 中</p><p>我们将输出矩阵计算为： <span class="math display">\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span> 下面给了一张参考图 当m=1时就跟单独运算一样</p><p><img src="attention2.png" /></p><p>两个最常用的注意功能是</p><ul><li><p>加性注意 （additive attention ）</p></li><li><p>点积（乘法）注意（dot-product (multiplicative)attention）</p></li></ul><p>点积注意力与我们的算法相同，除了 <spanclass="math inline">\(\sqrt{d_k}\)</span> 的比例因子</p><p>Additive attention使用具有单个隐藏层的前馈网络计算兼容性函数</p><p>虽然两者在理论上的复杂性相似，但<strong>点积注意力在实践中更快且更节省空</strong>间，因为它可以使用高度优化的矩阵乘法代码来实现</p><p>虽然对于较小的 dk值，这两种机制的性能相似，但加法注意力优于点积注意力，而无需对较大的 dk值进行缩放</p><p>我们怀疑对于较大的 dk 值，点积的幅度会变大，从而将 softmax函数推入具有极小梯度的区域</p><p>为了抵消这种影响，我们将点积缩放<spanclass="math inline">\(\sqrt{d_k}\)</span></p><blockquote><p>注意Mask部分具体操作就是将qt之后的值给换成一个非常大的负数，在后续的softmax时候就会变成0</p><p>使得计算结果只用到了v1到vt-1的结果</p></blockquote><h4 id="multi-head-attention">3.2.2 Multi-Head Attention</h4><p><img src="fig2_right.png" /></p><p>与使用 dmodel维度的key、value和query执行单个注意函数不同</p><p>我们发现将查询、键和值分别线性投影到 dk、dk 和 dv维度上的不同学习线性投影是有益的（投影到低维度）</p><blockquote><p>相当于给h次机会 希望能够学到不一样的投影的方式</p><p>使得在投影进去的度量空间里面 能够去匹配不同模式的相似函数</p><p>类似卷积神经网络中有多个输出通道的感觉</p></blockquote><p>然后，在每个查询、键和值的投影版本上，我们并行执行 Scaled Dot-ProductAttention，产生 dv 维输出值。</p><p>这些被连接cat起来并再次投影，产生最终值</p><p>Multi-HeadAttention允许模型共同关注来自不同位置的不同表示子空间的信息</p><p>对于单个注意力头，平均化会抑制这一点</p><p><img src="multi-head.png" /></p><ul><li><p>Q 矩阵从[m,dmodel] 降维到[m , dk] 那么<spanclass="math inline">\(W_i^Q \in \mathbb{R}^{d_{model}\timesd_k}\)</span></p></li><li><p>K 矩阵从[n,dmodel] 降维到[n , dk] 那么<spanclass="math inline">\(W_i^K \in \mathbb{R}^{d_{model}\timesd_k}\)</span></p></li><li><p>V 矩阵从[n,dmodel] 降维到[n , dv] 那么<spanclass="math inline">\(W_i^V \in \mathbb{R}^{d_{model}\timesd_v}\)</span></p></li></ul><p>在这项工作中，我们使用 h = 8 个并行注意力层或头</p><p>对于其中的每一个，我们使用 dk = dv = dmodel/h = 64</p><p>由于每个头的维度减少，总计算成本类似于具有全维度的单头注意力</p><h4 id="applications-of-attention-in-our-model">3.2.3 Applications ofAttention in our Model</h4><p>Transformer 以三种不同的方式使用多头注意力：</p><ul><li>在“编码器-解码器注意力”层中，query来自前一个解码器层，记忆key和value来自编码器的输出。这允许解码器中的每个位置参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制</li><li>编码器包含自注意力层在自注意力层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器中前一层的输出。编码器中的每个位置都可以关注编码器上一层中的所有位置。</li><li>类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的信息向左流动，以保持自回归特性。我们通过屏蔽掉（设置为 -∞）softmax输入中与非法连接相对应的所有值来实现缩放点积注意力的内部</li></ul><h2 id="position-wise-feed-forward-networks">3.3Position-wiseFeed-Forward Networks</h2><p>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性变换，中间有一个 ReLU 激活。 <span class="math display">\[FFN(x) = max(0,xW_1 + b_1 )W_2 + b_2\]</span></p><blockquote><p>输入层 - 隐藏层 - 输出层</p><p>输入( n , dmodel = 512 )</p><p>隐藏层( n , dmodel*4 = 2048)</p><p>输出层（n ， dmodel = 512）</p></blockquote><p>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另</p><p>一种描述方式是内核大小为 1 的两个卷积</p><p>输入和输出的维度为 dmodel = 512，内层的维度为 dff = 2048</p><h2 id="embeddings-and-softmax">3.4 Embeddings and Softmax</h2><p>与其他序列转导模型类似，我们<strong>使用learnedembedding将输入标记和输出标记转换为维度 dmodel 的向量</strong></p><p>我们还使用通常的学习线性变换和 softmax函数将解码器输出转换为预测的下一个token概率</p><p>在我们的模型中，我们在两个embedding和 pre-softmax线性变换之间共享相同的权重矩阵</p><p>在embedding中，我们将这些权重乘以 <spanclass="math inline">\(\sqrt{d_{model}}\)</span></p><h2 id="positional-encoding">3.5 Positional Encoding</h2><p>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于标记在序列中的相对或绝对位置的信息</p><p>为此，我们在输入嵌入编码器和解码器堆栈的底部中添加“位置编码”</p><p>位置编码与嵌入具有相同的维度 dmodel，因此可以将两者相加</p><p>位置编码有很多选择，学习的和固定的</p><p>在这项工作中，我们使用不同频率的正弦和余弦函数： <spanclass="math display">\[PE(pos,2i) = sin(pos/1000^{2i/d_{model}})\]</span></p><p><span class="math display">\[PE(pos,2i+1) = cos(pos/1000^{2i/d_{model}})\]</span></p><ul><li>pos 是位置</li><li>i 是维度</li></ul><p>也就是说，位置编码的每个维度对应一个正弦曲线。</p><p>波长形成从 2π 到 10000 · 2π 的几何级数</p><p>我们选择这个函数是因为我们假设它可以让模型轻松学习通过相对位置来参与，因为对于任何固定的偏移量k</p><p><span class="math inline">\(PE_{pos+k}\)</span>可以表示为 <spanclass="math inline">\(PE_{pos}\)</span> 的线性函数</p><p>我们还尝试使用学习的位置嵌入 , 发现这两个版本产生了几乎相同的结果</p><p>我们选择了正弦版本，因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;主要的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。&lt;/p&gt;
&lt;p&gt;性能最好的模型还通过注意力机制连接编码器和解码器。&lt;/p&gt;
&lt;p&gt;我们提出了一种新的简单网络架构 &lt;strong&gt;Transf</summary>
      
    
    
    
    <category term="论文笔记" scheme="http://www.larryai.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="http://www.larryai.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://www.larryai.com/tags/Transformer/"/>
    
  </entry>
  
</feed>
